{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1151,
   "id": "fc8b6c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.6.0'"
      ]
     },
     "execution_count": 1151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install -qq implicit==0.6.0\n",
    "\n",
    "import implicit\n",
    "implicit.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1152,
   "id": "ec2c02a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial.distance import cdist\n",
    "import tqdm\n",
    "from itertools import combinations\n",
    "\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from implicit.nearest_neighbours import bm25_weight\n",
    "from implicit.nearest_neighbours import ItemItemRecommender\n",
    "from implicit.bpr import BayesianPersonalizedRanking\n",
    "\n",
    "from sklearn.decomposition import PCA, LatentDirichletAllocation\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from lightgbm import LGBMClassifier, LGBMRanker\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1153,
   "id": "343fd7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(recommended_list, bought_list, k=5):\n",
    "    \n",
    "    bought_list = np.array(bought_list)\n",
    "    recommended_list = np.array(recommended_list)[:k]\n",
    "    \n",
    "    flags = np.isin(bought_list, recommended_list)\n",
    "    precision = flags.sum() / len(recommended_list)\n",
    "    \n",
    "    return precision\n",
    "\n",
    "def ap_k(recommended_list, bought_list, k=5):\n",
    "    \n",
    "    bought_list = np.array(bought_list)\n",
    "    recommended_list = np.array(recommended_list)\n",
    "    \n",
    "    flags = np.isin(recommended_list, bought_list)\n",
    "    \n",
    "    if sum(flags) == 0:\n",
    "        return 0\n",
    "    \n",
    "    sum_ = 0\n",
    "    for i in range(k):\n",
    "        \n",
    "        if flags[i]:\n",
    "            p_k = precision_at_k(recommended_list, bought_list, k=i+1)\n",
    "            sum_ += p_k\n",
    "            \n",
    "    result = sum_ / sum(flags)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def map_k(recommend_list, bought_list, k=5):\n",
    "    return np.mean([ap_k(rec, bt, k) for rec, bt in zip(recommend_list, bought_list)])\n",
    "\n",
    "def recall_at_k(recommended_list, bought_list, k=5):\n",
    "    \n",
    "    bought_list = np.array(bought_list)\n",
    "    recommended_list = np.array(recommended_list)[:k]\n",
    "    flags = np.isin(bought_list, recommended_list)\n",
    "    \n",
    "    recall = flags.sum() / len(bought_list)\n",
    "    \n",
    "    return recall\n",
    "\n",
    "def calc_precision_at_k(df_data, top_k):\n",
    "    for col_name in df_data.columns[2:]:\n",
    "        yield col_name, df_data.apply(lambda row: precision_at_k(row[col_name], row['actual'], k=top_k), axis=1).mean()\n",
    "\n",
    "def calc_recall(df_data, top_k):\n",
    "    for col_name in df_data.columns[2:]:\n",
    "        yield col_name, df_data.apply(lambda row: recall_at_k(row[col_name], row['actual'], k=top_k), axis=1).mean()\n",
    "\n",
    "def calc_map_at_k(df_data):\n",
    "    for col_name in df_data.columns[2:]:\n",
    "        yield col_name, map_k(df_data[col_name].values.tolist(), df_data['actual'].values.tolist())\n",
    "\n",
    "def reduce_memory(df):\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object and str(col_type)[:4] != 'uint' and str(col_type) != 'category':\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        elif str(col_type)[:4] != 'uint':\n",
    "            df[col] = df[col].astype('category')\n",
    "    return df\n",
    "\n",
    "def popularity_recommendation(data, n=5):\n",
    "    \"\"\"Топ-n популярных товаров\"\"\"\n",
    "    \n",
    "    popular = data.groupby('item_id')['sales_value'].sum().reset_index()\n",
    "    popular.sort_values('sales_value', ascending=False, inplace=True)\n",
    "    \n",
    "    recs = popular.head(n).item_id\n",
    "    \n",
    "    return recs.tolist()\n",
    "\n",
    "def transform_data_for_eval(dataset, rec_col, user_col='user_id'):\n",
    "    eval_dataset = dataset[[user_col, rec_col]].copy()\n",
    "    eval_dataset[rec_col] = eval_dataset[rec_col].apply(lambda x: ' '.join([str(i) for i in x]))\n",
    "    eval_dataset.rename(columns={\n",
    "        user_col: 'UserId',\n",
    "        rec_col: 'Predicted'\n",
    "    }, inplace=True)\n",
    "    return eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1154,
   "id": "dcd0acc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('retail_train.csv')\n",
    "item_features = pd.read_csv('product.csv')\n",
    "user_features = pd.read_csv('hh_demographic.csv')\n",
    "test = pd.read_csv('test_user.csv')\n",
    "\n",
    "N_preds = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1155,
   "id": "dcff937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features.columns = [col.lower() for col in item_features.columns]\n",
    "user_features.columns = [col.lower() for col in user_features.columns]\n",
    "\n",
    "item_features.rename(columns={'product_id': 'item_id'}, inplace=True)\n",
    "user_features.rename(columns={'household_key': 'user_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1156,
   "id": "bca0cfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1 Train - users: 2498, items: 82059\n",
      "Level 1 Valid - users: 2169, items: 27912\n",
      "Level 2 Train - users: 2169, items: 27912\n",
      "Level 2 Valid - users: 2197, items: 30040\n"
     ]
    }
   ],
   "source": [
    "val_lvl_1_size_weeks = 12\n",
    "val_lvl_2_size_weeks = 6\n",
    "\n",
    "data_train_lvl_1 = data[data['week_no'] < data['week_no'].max() - val_lvl_1_size_weeks]\n",
    "data_val_lvl_1 = data[(data['week_no'] >= data['week_no'].max() - val_lvl_1_size_weeks) &\n",
    "                      (data['week_no'] < data['week_no'].max() - val_lvl_2_size_weeks)]\n",
    "\n",
    "data_train_lvl_2 = data_val_lvl_1.copy()\n",
    "data_val_lvl_2 = data[data['week_no'] >= data['week_no'].max() - val_lvl_2_size_weeks]\n",
    "\n",
    "data_train_lvl_1 = reduce_memory(data_train_lvl_1)\n",
    "data_val_lvl_1 = reduce_memory(data_val_lvl_1)\n",
    "data_train_lvl_2 = reduce_memory(data_train_lvl_2)\n",
    "data_val_lvl_2 = reduce_memory(data_val_lvl_2)\n",
    "\n",
    "print(f'Level 1 Train - users: {data_train_lvl_1.user_id.nunique()}, items: {data_train_lvl_1.item_id.nunique()}')\n",
    "print(f'Level 1 Valid - users: {data_val_lvl_1.user_id.nunique()}, items: {data_val_lvl_1.item_id.nunique()}')\n",
    "print(f'Level 2 Train - users: {data_train_lvl_2.user_id.nunique()}, items: {data_train_lvl_2.item_id.nunique()}')\n",
    "print(f'Level 2 Valid - users: {data_val_lvl_2.user_id.nunique()}, items: {data_val_lvl_2.item_id.nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1157,
   "id": "9e6f7a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>actual</th>\n",
       "      <th>popular_recommendation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[836548, 856942, 877391, 933913, 948420, 10368...</td>\n",
       "      <td>[6534178, 6533889, 1029743, 6534166, 1082185]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[831125, 838136, 852864, 899624, 908649, 91504...</td>\n",
       "      <td>[6534178, 6533889, 1029743, 6534166, 1082185]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                             actual  \\\n",
       "0        1  [836548, 856942, 877391, 933913, 948420, 10368...   \n",
       "1        2  [831125, 838136, 852864, 899624, 908649, 91504...   \n",
       "\n",
       "                          popular_recommendation  \n",
       "0  [6534178, 6533889, 1029743, 6534166, 1082185]  \n",
       "1  [6534178, 6533889, 1029743, 6534166, 1082185]  "
      ]
     },
     "execution_count": 1157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = data_val_lvl_1.groupby('user_id')['item_id'].unique().reset_index()\n",
    "result.columns=['user_id', 'actual']\n",
    "\n",
    "popular_recs = popularity_recommendation(data_train_lvl_1, n=5)\n",
    "\n",
    "result['popular_recommendation'] = result['user_id'].apply(lambda x: popular_recs)\n",
    "result.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1158,
   "id": "fd576bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefilter_items(data, take_n_popular=5000, item_features=None, n_weeks=95):\n",
    "    # Уберем самые популярные товары (их и так купят)\n",
    "    popularity = data.groupby('item_id')['user_id'].nunique().reset_index() / data['user_id'].nunique()\n",
    "    popularity.rename(columns={'user_id': 'share_unique_users'}, inplace=True)\n",
    "\n",
    "    top_popular = popularity[popularity['share_unique_users'] > 0.2].item_id.tolist()\n",
    "    data = data[~data['item_id'].isin(top_popular)]\n",
    "\n",
    "    # Уберем самые НЕ популярные товары (их и так НЕ купят)\n",
    "    top_notpopular = popularity[popularity['share_unique_users'] < 0.02].item_id.tolist()\n",
    "    data = data[~data['item_id'].isin(top_notpopular)]\n",
    "\n",
    "    # Уберем не интересные для рекоммендаций категории (department)\n",
    "    if item_features is not None:\n",
    "        department_size = pd.DataFrame(item_features.groupby('department')['item_id'].nunique().sort_values(ascending=False)).reset_index()\n",
    "        department_size.columns = ['department', 'n_items']\n",
    "        rare_departments = department_size[department_size['n_items'] < 150].department.tolist()\n",
    "        items_in_rare_departments = item_features[item_features['department'].isin(rare_departments)].item_id.unique().tolist()\n",
    "\n",
    "        data = data[~data['item_id'].isin(items_in_rare_departments)]\n",
    "\n",
    "    # Уберем слишком дешевые товары (на них не заработаем). 1 покупка из рассылок стоит 60 руб.\n",
    "    data['price'] = data['sales_value'] / (np.maximum(data['quantity'], 1))\n",
    "    data = data[data['price'] > 2]\n",
    "\n",
    "    # Уберем слишком дорогие товарыs\n",
    "    data = data[data['price'] < 50]\n",
    "\n",
    "    # уберем товары, не продававшиеся более n_week недель\n",
    "    data = data[data['week_no'] >= data['week_no'].max() - n_weeks]\n",
    "\n",
    "    # Возьмем топ по популярности\n",
    "    popularity = data.groupby('item_id')['quantity'].sum().reset_index()\n",
    "    popularity.rename(columns={'quantity': 'n_sold'}, inplace=True)\n",
    "    top = popularity.sort_values('n_sold', ascending=False).head(take_n_popular).item_id.tolist()\n",
    "    \n",
    "    # Заведем фиктивный item_id (если юзер покупал товары из топ-N, то он \"купил\" такой товар)\n",
    "    data.loc[~data['item_id'].isin(top), 'item_id'] = 999999\n",
    "    \n",
    "    return data\n",
    "\n",
    "def postfilter(recommendations, item_info, N=5):\n",
    "    \"\"\"Пост-фильтрация товаров\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    recommendations: list\n",
    "        Ранжированный список item_id для рекомендаций\n",
    "    item_info: pd.DataFrame\n",
    "        Датафрейм с информацией о товарах\n",
    "    \"\"\"\n",
    "    \n",
    "    # Уникальность\n",
    "    unique_recommendations = []\n",
    "    [unique_recommendations.append(item) for item in recommendations if item not in unique_recommendations]\n",
    "    \n",
    "    # Разные категории\n",
    "    categories_used = []\n",
    "    final_recommendations = []\n",
    "    CATEGORY_NAME = 'sub_commodity_desc'\n",
    "    for item in unique_recommendations:\n",
    "        category = item_features.loc[item_features['item_id'] == item, CATEGORY_NAME].values[0]\n",
    "        \n",
    "        if category not in categories_used:\n",
    "            final_recommendations.append(item)\n",
    "            \n",
    "        unique_recommendations.remove(item)\n",
    "        categories_used.append(category)\n",
    "    \n",
    "    n_rec = len(final_recommendations)\n",
    "    if n_rec < N:\n",
    "        final_recommendations.extend(unique_recommendations[:N - n_rec])\n",
    "    else:\n",
    "        final_recommendations = final_recommendations[:N]\n",
    "    \n",
    "    assert len(final_recommendations) == N, 'Количество рекомендаций != {}'.format(N)\n",
    "    return final_recommendations\n",
    "\n",
    "def rule(x, y, model, N=5):\n",
    "    if x in y:\n",
    "        return recommender.overall_top_purchases[:N]\n",
    "    if model == 'als':\n",
    "        return recommender.get_als_recommendations(x, N=N)\n",
    "    elif model == 'own':\n",
    "        return recommender.get_own_recommendations(x, N=N)\n",
    "    elif model == 'similar_items':\n",
    "        return recommender.get_similar_items_recommendation(x, N=N)\n",
    "    elif model == 'similar_users':\n",
    "        return recommender.get_similar_users_recommendation(x, N=N)\n",
    "    elif model == 'bayesian':\n",
    "        return recommender.get_bayesian_recommendations(x, N=N)\n",
    "    \n",
    "def get_self_top_purchases(user_id, N=5):\n",
    "    return recommender.top_purchases[recommender.top_purchases.user_id == user_id].item_id.head(N).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1159,
   "id": "0bef8831",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainRecommender:\n",
    "    \"\"\"Рекоммендации, которые можно получить из ALS\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    user_item_matrix: pd.DataFrame\n",
    "        Матрица взаимодействий user-item\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, data_type='quantity', weighting=True, normalize=False, k1=150, \n",
    "                 create_mtx=True, user_item_matrix=None):\n",
    "        self.data_type = data_type\n",
    "        self.normalize = normalize\n",
    "        self.k1 = k1\n",
    "        if user_item_matrix is not None:\n",
    "            create_mtx = False\n",
    "\n",
    "        # Топ покупок каждого юзера\n",
    "        self.top_purchases = data.groupby(['user_id', 'item_id'])['quantity'].count().reset_index()\n",
    "        self.top_purchases.sort_values('quantity', ascending=False, inplace=True)\n",
    "        self.top_purchases = self.top_purchases[self.top_purchases['item_id'] != 999999]\n",
    "\n",
    "        # Топ покупок по всему датасету\n",
    "        self.overall_top_purchases = data.groupby('item_id')['quantity'].count().reset_index()\n",
    "        self.overall_top_purchases.sort_values('quantity', ascending=False, inplace=True)\n",
    "        self.overall_top_purchases = self.overall_top_purchases[self.overall_top_purchases['item_id'] != 999999]\n",
    "        self.overall_top_purchases = self.overall_top_purchases.item_id.tolist()\n",
    "        \n",
    "        if create_mtx:\n",
    "            self.user_item_matrix = self._prepare_matrix(data, data_type=self.data_type, normalize=self.normalize)\n",
    "        elif user_item_matrix is not None:\n",
    "            self.user_item_matrix = user_item_matrix.copy()\n",
    "        else:\n",
    "            raise ValueError(f'Set create_mtx to \"True\" or pass user_item_matrix to \"user_item_matrix\" attr.')\n",
    "        \n",
    "        self.id_to_itemid, self.id_to_userid, self.itemid_to_id, self.userid_to_id = self._prepare_dicts(self.user_item_matrix)\n",
    "\n",
    "        if weighting:\n",
    "            self.user_item_matrix = bm25_weight(self.user_item_matrix, K1=self.k1, B=0.8)  # default: 100, 0.8 ver. 0.5.2\n",
    "\n",
    "        self.model = self.fit(self.user_item_matrix)\n",
    "        self.own_recommender = self.fit_own_recommender(self.user_item_matrix)\n",
    "        self.ranker = self.fit_ranker(self.user_item_matrix)\n",
    "\n",
    "    @staticmethod\n",
    "    def _prepare_matrix(data, data_type, normalize=False):\n",
    "        \"\"\"Готовит user-item матрицу\"\"\"\n",
    "        if data_type == 'quantity':\n",
    "            user_item_matrix = pd.pivot_table(data,\n",
    "                                              index='user_id', \n",
    "                                              columns='item_id',\n",
    "                                              values='quantity',\n",
    "                                              aggfunc='count',\n",
    "                                              fill_value=0\n",
    "                                              )\n",
    "        elif data_type == 'sales':\n",
    "            user_item_matrix = pd.pivot_table(data,\n",
    "                                              index='user_id', \n",
    "                                              columns='item_id',\n",
    "                                              values='sales_value',\n",
    "                                              aggfunc='sum',\n",
    "                                              fill_value=0\n",
    "                                              )\n",
    "            if normalize:\n",
    "                user_item_matrix = user_item_matrix / user_item_matrix.max()  # normalize\n",
    "        elif data_type == 'quantity_sum':\n",
    "            user_item_matrix = pd.pivot_table(data,\n",
    "                                              index='user_id',\n",
    "                                              columns='item_id',\n",
    "                                              values='quantity',\n",
    "                                              aggfunc='sum',\n",
    "                                              fill_value=0\n",
    "                                              )\n",
    "            if normalize:\n",
    "                user_item_matrix = user_item_matrix / user_item_matrix.max()\n",
    "        elif data_type == 'weighted_sum':\n",
    "            if 'weighted' not in data.columns:\n",
    "                return self._prepare_matrix(data, data_type='quantity')\n",
    "            \n",
    "            user_item_matrix = pd.pivot_table(data, \n",
    "                                              index='user_id', \n",
    "                                              columns='item_id', \n",
    "                                              values='weighted',\n",
    "                                              aggfunc='sum',\n",
    "                                              fill_value=0\n",
    "                                             )\n",
    "        else:\n",
    "            raise ValueError(f'Agg data type must be \"quantity\", \"sales\" or \"quantity_sum\", given: {data_type}')\n",
    "\n",
    "        user_item_matrix = user_item_matrix.astype(float)\n",
    "\n",
    "        return user_item_matrix\n",
    "\n",
    "    @staticmethod\n",
    "    def _prepare_dicts(user_item_matrix):\n",
    "        \"\"\"Подготавливает вспомогательные словари\"\"\"\n",
    "\n",
    "        userids = user_item_matrix.index.values\n",
    "        itemids = user_item_matrix.columns.values\n",
    "\n",
    "        matrix_userids = np.arange(len(userids))\n",
    "        matrix_itemids = np.arange(len(itemids))\n",
    "\n",
    "        id_to_itemid = dict(zip(matrix_itemids, itemids))\n",
    "        id_to_userid = dict(zip(matrix_userids, userids))\n",
    "\n",
    "        itemid_to_id = dict(zip(itemids, matrix_itemids))\n",
    "        userid_to_id = dict(zip(userids, matrix_userids))\n",
    "\n",
    "        return id_to_itemid, id_to_userid, itemid_to_id, userid_to_id\n",
    "\n",
    "    @staticmethod\n",
    "    def fit_own_recommender(user_item_matrix):\n",
    "        \"\"\"Обучает модель, которая рекомендует товары, среди товаров, купленных юзером\"\"\"\n",
    "\n",
    "        own_recommender = ItemItemRecommender(K=1, num_threads=4)\n",
    "        own_recommender.fit(csr_matrix(user_item_matrix).tocsr())\n",
    "\n",
    "        return own_recommender\n",
    "\n",
    "    @staticmethod\n",
    "    def fit(user_item_matrix, n_factors=20, regularization=0.001, iterations=15, num_threads=4): \n",
    "        \"\"\"Обучает ALS\"\"\"\n",
    "\n",
    "        model = AlternatingLeastSquares(factors=n_factors,\n",
    "                                        regularization=regularization,\n",
    "                                        iterations=iterations,\n",
    "                                        num_threads=num_threads,\n",
    "                                        use_gpu=False,\n",
    "                                        random_state=42)\n",
    "        model.fit(csr_matrix(user_item_matrix).tocsr())\n",
    "\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def fit_ranker(user_item_matrix, factors=50, learning_rate=0.03, regularization=0.01, iterations=200):\n",
    "        ranker = BayesianPersonalizedRanking(factors=factors, \n",
    "                                     learning_rate=learning_rate, \n",
    "                                     regularization=regularization, \n",
    "                                     iterations=iterations, \n",
    "                                     num_threads=4, \n",
    "                                     random_state=42)\n",
    "        ranker.fit(csr_matrix(user_item_matrix).tocsr())\n",
    "\n",
    "        return ranker\n",
    "\n",
    "    def _update_dict(self, user_id):\n",
    "        \"\"\"Если появился новыю user / item, то нужно обновить словари\"\"\"\n",
    "\n",
    "        if user_id not in self.userid_to_id.keys():\n",
    "\n",
    "            max_id = max(list(self.userid_to_id.values()))\n",
    "            max_id += 1\n",
    "\n",
    "            self.userid_to_id.update({user_id: max_id})\n",
    "            self.id_to_userid.update({max_id: user_id})\n",
    "\n",
    "    def _get_similar_item(self, item_id):\n",
    "        \"\"\"Находит товар, похожий на item_id\"\"\"\n",
    "        recs = self.model.similar_items(self.itemid_to_id[item_id], N=2)\n",
    "        top_rec = recs[0][1] \n",
    "        return self.id_to_itemid[top_rec]\n",
    "\n",
    "    def _extend_with_top_popular(self, recommendations, user_id=None, N=5):\n",
    "        \"\"\"Если кол-во рекоммендаций < N, то дополняем их топ-популярными\"\"\"\n",
    "        \n",
    "        if user_id is not None and len(recommendations) < N:\n",
    "            recommendations.extend(self.top_purchases[self.top_purchases.user_id == user_id].item_id.head(N).tolist())\n",
    "            recommendations = pd.Series(data=recommendations).drop_duplicates().tolist()\n",
    "            if len(recommendations) > N:\n",
    "                recommendations = recommendations[:N]\n",
    "\n",
    "        if len(recommendations) < N:\n",
    "            recommendations.extend(self.overall_top_purchases[:N])\n",
    "            recommendations = recommendations[:N]\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "    def _get_recommendations(self, user, model, N=5):\n",
    "        \"\"\"Рекомендации через стардартные библиотеки implicit\"\"\"\n",
    "\n",
    "        self._update_dict(user_id=user)\n",
    "        user_id = self.userid_to_id[user]\n",
    "                \n",
    "        model_name = model.__class__.__name__\n",
    "        \n",
    "        if model_name == 'ItemItemRecommender':\n",
    "\n",
    "            res = model.recommend(userid=user_id,\n",
    "                                  user_items=csr_matrix(self.user_item_matrix).tocsr()[user_id, :],\n",
    "                                  N=N-1,\n",
    "                                  filter_already_liked_items=False,\n",
    "                                  filter_items=[self.itemid_to_id[999999]],\n",
    "                                  recalculate_user=True) # [0].tolist()\n",
    "            # ind = np.argsort(res[1])[::-1]  # sorting by scores: to preserve id order with diff. N\n",
    "            res = [self.id_to_itemid[rec] for rec in res[0]]\n",
    "\n",
    "        elif model_name == 'AlternatingLeastSquares':\n",
    "            res = [self.id_to_itemid[rec] for rec in model.recommend(userid=user_id,\n",
    "                                        user_items=csr_matrix(self.user_item_matrix).tocsr()[user_id,:],\n",
    "                                        N=N,\n",
    "                                        filter_already_liked_items=False,\n",
    "                                        filter_items=[self.itemid_to_id[999999]],\n",
    "                                        recalculate_user=True)[0]]\n",
    "\n",
    "        elif model_name == 'BayesianPersonalizedRanking':\n",
    "            res = [self.id_to_itemid[rec] for rec in model.recommend(userid=user_id, \n",
    "                                        user_items=csr_matrix(self.user_item_matrix).tocsr()[user_id, :],\n",
    "                                        N=N,\n",
    "                                        filter_already_liked_items=False,\n",
    "                                        filter_items=[self.itemid_to_id[999999]],\n",
    "                                        )[0]]\n",
    "\n",
    "        res = self._extend_with_top_popular(res, user, N=N)\n",
    "        \n",
    "        assert len(res) == N, 'Количество рекомендаций != {}'.format(N)\n",
    "        return res\n",
    "\n",
    "    def get_als_recommendations(self, user, N=5):\n",
    "        \"\"\"Рекомендации через стардартные библиотеки implicit\"\"\"\n",
    "\n",
    "        self._update_dict(user_id=user)\n",
    "        return self._get_recommendations(user, model=self.model, N=N)\n",
    "\n",
    "    def get_bayesian_recommendations(self, user, N=5):\n",
    "        \"\"\" Рекомендации на основе модели ранжирования \"\"\"\n",
    "\n",
    "        self._update_dict(user_id=user)\n",
    "        return self._get_recommendations(user, model=self.ranker, N=N)\n",
    "\n",
    "    def get_own_recommendations(self, user, N=5):\n",
    "        \"\"\"Рекомендуем товары среди тех, которые юзер уже купил\"\"\"\n",
    "        \n",
    "        self._update_dict(user_id=user)\n",
    "        return self._get_recommendations(user, model=self.own_recommender, N=N)\n",
    "\n",
    "    def get_similar_items_recommendation(self, user, N=5):\n",
    "        \"\"\"Рекомендуем товары, похожие на топ-N купленных юзером товаров\"\"\"\n",
    "\n",
    "        top_users_purchases = self.top_purchases[self.top_purchases['user_id'] == user].head(N)\n",
    "\n",
    "        res = top_users_purchases['item_id'].apply(lambda x: self._get_similar_item(x)).tolist()\n",
    "        if 999999 in res: res.remove(999999)  # prev. ver.\n",
    "\n",
    "        res = self._extend_with_top_popular(res, N=N) \n",
    "\n",
    "        assert len(res) == N, 'Количество рекомендаций != {}'.format(N)\n",
    "        return res\n",
    "\n",
    "    def get_similar_users_recommendation(self, user, N=5):\n",
    "        \"\"\"Рекомендуем топ-N товаров, среди купленных похожими юзерами: берем N похожих пользователей и с помощью трюка рекомендуем юзеру их топ товары\"\"\"\n",
    "        res = []\n",
    "\n",
    "        # Находим топ-N похожих пользователей\n",
    "        similar_users = self.model.similar_users(self.userid_to_id[user], N=N+1)\n",
    "        similar_users = [rec for rec in similar_users[0]]\n",
    "        similar_users = similar_users[1:]   # удалим юзера из запроса\n",
    "\n",
    "        for user in similar_users:\n",
    "            user = self.id_to_userid[user]  ## нужно подать для предикта оригинальный идентификатор\n",
    "            res.extend(self.get_own_recommendations(user, N=1))\n",
    "        res = pd.Series(res).drop_duplicates().tolist()\n",
    "\n",
    "        res = self._extend_with_top_popular(res, N=N)\n",
    "\n",
    "        assert len(res) == N, 'Количество рекомендаций != {}'.format(N)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1160,
   "id": "a785a84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_lvl_1 = prefilter_items(data_train_lvl_1, item_features=item_features, take_n_popular=10000, n_weeks=95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1161,
   "id": "3cdc687e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1 Train - users: 2495, items: 10001\n"
     ]
    }
   ],
   "source": [
    "print(f'Level 1 Train - users: {data_train_lvl_1.user_id.nunique()}, items: {data_train_lvl_1.item_id.nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1162,
   "id": "161a030b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88de64dc947e45c7b6b021e9d96382b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7830846d2753437c98a23dd134e8954a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b7ff59877a472e9360afee71a66d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recommender = MainRecommender(data_train_lvl_1, k1=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "id": "88023c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1813, 1984]"
      ]
     },
     "execution_count": 1163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_out = result.loc[~result.user_id.isin(data_train_lvl_1.user_id), 'user_id'].tolist()\n",
    "list_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1164,
   "id": "11d0e71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['als'] = result['user_id'].apply(lambda x: rule(x, list_out, model='als', N=N_preds))\n",
    "result['own'] = result['user_id'].apply(lambda x: rule(x, list_out, model='own', N=N_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1166,
   "id": "4dd13fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "popular_recommendation: 0.18275703088981096\n",
      "als: 0.1295527893038267\n",
      "own: 0.2439372982941448\n"
     ]
    }
   ],
   "source": [
    "for pred in calc_precision_at_k(result, N_preds):\n",
    "    print(*pred, sep=': ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1167,
   "id": "9b2b0746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "popular_recommendation: 0.023245424460641564\n",
      "als: 0.027019756635783692\n",
      "own: 0.04995886675569452\n"
     ]
    }
   ],
   "source": [
    "for pred in calc_recall(result, N_preds):\n",
    "    print(*pred, sep=': ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f410693",
   "metadata": {},
   "source": [
    "**2 level model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1168,
   "id": "dc585fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['manufacturer', 'department', 'commodity_desc', 'sub_commodity_desc', 'curr_size_of_product']\n",
    "for name in names:\n",
    "    new_name = name + '_freq'\n",
    "    a = item_features[name].value_counts()\n",
    "    ind = a.index.tolist()\n",
    "    for i in ind:\n",
    "        item_features.loc[item_features[name] == i, new_name] = a[i]\n",
    "\n",
    "commodities = item_features.commodity_desc.value_counts()\n",
    "commodities_list = commodities.keys().tolist()\n",
    "for i, name in enumerate(commodities_list):\n",
    "    item_features.loc[item_features['commodity_desc'] == name, 'commodity_category'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1169,
   "id": "c4613903",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfLDA(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_components=5, learning_method='batch', max_features=10001, n_jobs=-1, random_state=42):\n",
    "        self.n_components = n_components\n",
    "        self.learning_method = learning_method\n",
    "        self.max_features = max_features\n",
    "        self.n_jobs = n_jobs\n",
    "        self.random_state = random_state\n",
    "        self.pipe = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.pipe = make_pipeline(TfidfVectorizer(max_features=self.max_features),\n",
    "                                  LatentDirichletAllocation(n_components=self.n_components,\n",
    "                                                            learning_method=self.learning_method,\n",
    "                                                            n_jobs=self.n_jobs,\n",
    "                                                            random_state=self.random_state)).fit(X['item_id'].values)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X_ = np.zeros((X.shape[0], self.n_components))\n",
    "        for i in range(X_.shape[0]):\n",
    "            X_[i, :] = self.pipe.transform(X.values[i])\n",
    "        return pd.DataFrame(data=X_, columns=[f'topic_{i}' for i in range(self.n_components)], index=X.index)\n",
    "    \n",
    "def create_multiple_features(df_train, train_val):\n",
    "    df_train = df_train.merge(train_val.groupby(by='item_id').agg('sales_value').sum().rename('total_item_sales_value'), how='left',on='item_id')\n",
    "    df_train = df_train.merge(train_val.groupby(by='item_id').agg('quantity').sum().rename('total_quantity_value'), how='left',on='item_id')\n",
    "    df_train = df_train.merge(train_val.groupby(by='item_id').agg('user_id').count().rename('item_freq'), how='left',on='item_id')\n",
    "    df_train = df_train.merge(train_val.groupby(by='user_id').agg('user_id').count().rename('user_freq'), how='left',on='user_id')\n",
    "    df_train = df_train.merge(train_val.groupby(by='user_id').agg('sales_value').sum().rename('total_user_sales_value'), how='left',on='user_id')\n",
    "    df_train = df_train.merge(train_val.groupby(by='item_id').agg('quantity').sum().rename('item_quantity_per_week')/train_val.week_no.nunique(), how='left',on='item_id')\n",
    "    df_train = df_train.merge(train_val.groupby(by='user_id').agg('quantity').sum().rename('user_quantity_per_week')/train_val.week_no.nunique(), how='left',on='user_id')\n",
    "    df_train = df_train.merge(train_val.groupby(by='item_id').agg('quantity').sum().rename('item_quantity_per_basket')/train_val.basket_id.nunique(), how='left',on='item_id')\n",
    "    df_train = df_train.merge(train_val.groupby(by='user_id').agg('quantity').sum().rename('user_quantity_per_basket')/train_val.basket_id.nunique(), how='left',on='user_id')\n",
    "    df_train = df_train.merge(train_val.groupby(by='item_id').agg('user_id').count().rename('item_freq_per_basket')/train_val.basket_id.nunique(), how='left',on='item_id')\n",
    "    df_train = df_train.merge(train_val.groupby(by='user_id').agg('user_id').count().rename('user_freq_per_basket')/train_val.basket_id.nunique(), how='left',on='user_id')\n",
    "    return df_train\n",
    "\n",
    "def create_user_features(df_train, data_train):\n",
    "\n",
    "    # Средний чек\n",
    "    users_sales = data_train.groupby('user_id')['sales_value'].mean().reset_index()\n",
    "    users_sales.rename(columns={'sales_value': 'avg_cheque'}, inplace=True)\n",
    "    df_train = df_train.merge(users_sales[['user_id', 'avg_cheque']], on='user_id', how='left')\n",
    "\n",
    "    # Количество уникальных категорий покупателя\n",
    "    users_departments = data_train.groupby('user_id')['department'].nunique().reset_index()\n",
    "    users_departments.rename(columns = {'department':'users_unique_departments'}, inplace=True)\n",
    "    df_train = df_train.merge(users_departments, on='user_id', how='left')\n",
    "\n",
    "    # Среднее время покупки\n",
    "    bought_time = data_train.groupby('user_id')['trans_time'].mean().reset_index()\n",
    "    bought_time.rename(columns = {'trans_time':'mean_trans_time_by_user'}, inplace=True)\n",
    "    df_train = df_train.merge(bought_time, on='user_id', how='left')\n",
    "\n",
    "    # Средний чек корзины \n",
    "    baskets_sales_value = data_train.groupby(['user_id','basket_id'])['sales_value'].mean().reset_index()\n",
    "    mean_basket_sales_value = baskets_sales_value.groupby('user_id')['sales_value'].mean().reset_index()\n",
    "    mean_basket_sales_value.rename(columns = {'sales_value':'mean_sales_value_per_basket'}, inplace=True)\n",
    "    df_train = df_train.merge(mean_basket_sales_value, on='user_id', how='left')\n",
    "\n",
    "    # Количество купленных уникальных товаров \n",
    "    unique_bought_items = data_train.groupby('user_id')['item_id'].nunique().reset_index()\n",
    "    unique_bought_items.rename(columns = {'item_id':'unique_bought_items'}, inplace=True)\n",
    "    df_train = df_train.merge(unique_bought_items, on='user_id', how='left')\n",
    "\n",
    "    # Среднее количество уникальных категорий в корзине\n",
    "    users_baskets = data_train.groupby(['user_id', 'basket_id'])['department'].nunique().reset_index()\n",
    "    users_baskets = users_baskets.groupby('user_id')['department'].mean().reset_index()\n",
    "    users_baskets.rename(columns={'department': 'avg_basket_department'}, inplace=True)\n",
    "    df_train = df_train.merge(users_baskets[['user_id', 'avg_basket_department']], on='user_id', how='left')\n",
    "\n",
    "    # Средняя сумма покупки в категории\n",
    "    department_sales = data_train.groupby('department')['sales_value'].mean().reset_index()\n",
    "    department_sales.rename(columns={'sales_value': 'mean_sales_value_category'}, inplace=True)\n",
    "    df_train = df_train.merge(department_sales, on='department', how='left')\n",
    "\n",
    "    # Средная цена купленных товаров пользователем\n",
    "    users_sales = data_train.groupby('user_id')[['sales_value', 'quantity']].sum().reset_index()\n",
    "    users_sales['avg_price'] = users_sales['sales_value'] / users_sales['quantity']\n",
    "    df_train = df_train.merge(users_sales[['user_id', 'avg_price']], on='user_id', how='left')\n",
    "    return df_train\n",
    "\n",
    "def create_item_features(df_train, train_val):    \n",
    "\n",
    "    # Среднее количество покупок товара в неделю\n",
    "    num_purchase_week = train_val.groupby('item_id').agg({'week_no': 'nunique', 'quantity': 'sum'}).reset_index()\n",
    "    num_purchase_week['avg_num_purchases_week'] = num_purchase_week['quantity'] / num_purchase_week['week_no']\n",
    "    df_train = df_train.merge(num_purchase_week[['item_id', 'avg_num_purchases_week']], on='item_id', how='left')\n",
    "    df_train['avg_num_purchases_week'].fillna(0, inplace=True)\n",
    "\n",
    "    # Цена товара\n",
    "    items_sales = train_val.groupby('item_id')[['sales_value', 'quantity']].sum().reset_index()\n",
    "    items_sales['price'] = items_sales['sales_value'] / items_sales['quantity']\n",
    "    items_sales['price'].fillna(0, inplace=True)\n",
    "    df_train = df_train.merge(items_sales[['item_id', 'price']], on='item_id', how='left')\n",
    "\n",
    "    # Среднее время покупки товара\n",
    "    bought_item_time = train_val.groupby('item_id')['trans_time'].mean().reset_index()   \n",
    "    bought_item_time.rename(columns = {'trans_time':'mean_trans_time_by_item'}, inplace=True)\n",
    "    df_train = df_train.merge(bought_item_time, on = 'item_id', how = 'left')\n",
    "\n",
    "    # Количество магазинов, где есть товар\n",
    "    items_stores = train_val.groupby('item_id')['store_id'].sum().reset_index()\n",
    "    items_stores.rename(columns={'store_id': 'n_stores_with_item'}, inplace=True)\n",
    "    items_stores['n_stores_with_item'].fillna(0, inplace = True)\n",
    "    df_train = df_train.merge(items_stores, on='item_id', how='left')\n",
    "\n",
    "    # Количество уникальных магазинов, где есть товар\n",
    "    items_stores = train_val.groupby('item_id')['store_id'].nunique().reset_index()\n",
    "    items_stores.rename(columns={'store_id': 'n_unique_stores_with_item'}, inplace=True)\n",
    "    items_stores['n_unique_stores_with_item'].fillna(0, inplace = True)\n",
    "    df_train = df_train.merge(items_stores, on='item_id', how='left')\n",
    "    return df_train\n",
    "\n",
    "def get_new_features(train_val_lvl1):\n",
    "    \n",
    "    # час совершения транзакции\n",
    "    data = train_val_lvl1.copy()\n",
    "    data['hour'] = data['trans_time'] // 100\n",
    "    user_item_features = data.groupby(['user_id', 'item_id'])['hour'].median().reset_index()\n",
    "    user_item_features.columns = ['user_id', 'item_id', 'median_sales_hour']\n",
    "    \n",
    "    # день недели совершения транзакции\n",
    "    data['weekday'] = data['day'] % 7\n",
    "    df = data.groupby(['user_id', 'item_id'])['weekday'].median().reset_index()\n",
    "    df.columns = ['user_id', 'item_id', 'median_weekday']\n",
    "    user_item_features = user_item_features.merge(df, on=['user_id', 'item_id'])\n",
    "    \n",
    "    # cреднее кол-во дней между покупками\n",
    "    df = data.groupby('user_id')['day'].nunique().reset_index()\n",
    "    df['mean_visits_interval'] = (data.groupby('user_id')['day'].max() - data.groupby('user_id')['day'].min()) / df['day']\n",
    "    user_item_features = user_item_features.merge(df[['user_id', 'mean_visits_interval']], on=['user_id'])\n",
    "    \n",
    "    # кол-во транзакций клиента\n",
    "    df = data.groupby(['user_id'])['item_id'].count().reset_index()\n",
    "    df.columns = ['user_id', 'n_transactions']\n",
    "    user_item_features = user_item_features.merge(df, on=['user_id'])\n",
    "    \n",
    "    # mean / max / std кол-ва уникальных товаров в корзине клиента\n",
    "    df = data.groupby(['user_id', 'basket_id'])['item_id'].nunique().reset_index()\n",
    "    df1 = df.groupby('user_id')['item_id'].mean().reset_index()\n",
    "    df1.columns = ['user_id', 'mean_n_items_basket']\n",
    "    user_item_features = user_item_features.merge(df1, on=['user_id'])\n",
    "\n",
    "    df2 = df.groupby('user_id')['item_id'].max().reset_index()\n",
    "    df2.columns = ['user_id', 'max_n_items_basket']\n",
    "    user_item_features = user_item_features.merge(df2, on=['user_id'])\n",
    "\n",
    "    df3 = df.groupby('user_id')['item_id'].std().reset_index()\n",
    "    df3.columns = ['user_id', 'std_n_items_basket']\n",
    "    user_item_features = user_item_features.merge(df3, on=['user_id'])\n",
    "    \n",
    "    # mean / max / std кол-ва уникальных категорий в корзине клиента\n",
    "    data = data.merge(item_features[['item_id', 'commodity_desc']], on=['item_id'])\n",
    "    df = data.groupby(['user_id', 'basket_id'])['commodity_desc'].nunique().reset_index()\n",
    "    df1 = df.groupby('user_id')['commodity_desc'].mean().reset_index()\n",
    "    df1.columns = ['user_id', 'mean_n_item_categories_basket']\n",
    "    user_item_features = user_item_features.merge(df1, on=['user_id'])\n",
    "\n",
    "    df2 = df.groupby('user_id')['commodity_desc'].max().reset_index()\n",
    "    df2.columns = ['user_id', 'max_n_item_categories_basket']\n",
    "    user_item_features = user_item_features.merge(df2, on=['user_id'])\n",
    "\n",
    "    df3 = df.groupby('user_id')['commodity_desc'].std().reset_index()\n",
    "    df3.columns = ['user_id', 'std_n_item_categories_basket']\n",
    "    user_item_features = user_item_features.merge(df3, on=['user_id'])\n",
    "    \n",
    "    return user_item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1170,
   "id": "2f96ba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_userid = recommender.id_to_userid\n",
    "id_to_itemid = recommender.id_to_itemid\n",
    "\n",
    "userid_to_id = recommender.userid_to_id\n",
    "itemid_to_id = recommender.itemid_to_id\n",
    "\n",
    "user_factors = recommender.model.user_factors\n",
    "item_factors = recommender.model.item_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1171,
   "id": "adda111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# эмбеддинг юзера = среднее эмбеддингов купленниых им товаров / merge on user\n",
    "emb_df = data_train_lvl_1[['user_id', 'item_id']].copy()\n",
    "emb_df['item_id'] = emb_df['item_id'].apply(lambda x: np.array(item_factors[itemid_to_id[x], :]))\n",
    "emb_df = emb_df.groupby('user_id')['item_id'].sum().reset_index()  # mean\n",
    "\n",
    "# scalar value\n",
    "emb_df['item_id'] = emb_df['item_id'].apply(lambda x: x.mean())\n",
    "emb_df.rename(columns={'item_id': 'scalar_embedding'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1172,
   "id": "a8b2e410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# эмбеддинг товара - эмбеддинг юзера  # чем разница меньше, тем товар ближе к интересам пользователя / merge on user-item\n",
    "diff_emb = data_train_lvl_1[['user_id', 'item_id']].copy()\n",
    "diff_emb['diff_emb_scalar'] = diff_emb.apply(lambda row: (item_factors[itemid_to_id[row['item_id']]] - user_factors[userid_to_id[row['user_id']]]).mean(), axis=1)\n",
    "diff_emb.drop_duplicates(subset=['user_id', 'item_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1173,
   "id": "209279ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge on user-item; SKIP for ver. 0.4.8\n",
    "def get_item_score(user_id, item_ids):\n",
    "    items_scores = recommender.own_recommender.recommend(userid=userid_to_id[user_id],\n",
    "                                      user_items=csr_matrix(recommender.user_item_matrix).tocsr()[userid_to_id[user_id], :],\n",
    "                                      N=len(item_ids)-1,\n",
    "                                      filter_already_liked_items=False,\n",
    "                                      filter_items=[itemid_to_id[999999]],\n",
    "                                      recalculate_user=True)\n",
    "    items_recs = [id_to_itemid[item] for item in items_scores[0]]\n",
    "    indices = [items_recs.index(item) if item in items_recs else -1 for item in item_ids]\n",
    "    result = np.array([items_scores[1][i] if i != -1 else 0 for i in indices])\n",
    "    return result\n",
    "\n",
    "it_it_df = data_train_lvl_1[['user_id', 'item_id']].copy()\n",
    "for us in it_it_df.user_id.unique():\n",
    "    it_it_df.loc[it_it_df.user_id == us, 'item_item'] = get_item_score(us, it_it_df.loc[it_it_df.user_id == us, 'item_id'])\n",
    "it_it_df.drop_duplicates(subset=['user_id', 'item_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1174,
   "id": "576c10af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dot(user_id, item_id):\n",
    "    try:\n",
    "        return user_factors[userid_to_id[user_id], :] @ item_factors[itemid_to_id[item_id], :].T\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def get_inv_rank(user_id):\n",
    "    try:\n",
    "        vector = user_factors[userid_to_id[user_id], :] @ item_factors.T\n",
    "        vector = vector.flatten()\n",
    "        return 1 / np.argmax(vector)\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1175,
   "id": "b70aae35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = pd.concat([data_train_lvl_1, data_train_lvl_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1176,
   "id": "4c13acfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_popularity_rec(user, data, n=5):\n",
    "\n",
    "    recs_for_user = data[data['user_id'] == user]['item_id'].value_counts().index[:n+1].tolist()\n",
    "    if 999999 in recs_for_user:\n",
    "        recs_for_user.remove(999999)\n",
    "    if len(recs_for_user) > n:\n",
    "        recs_for_user = recs_for_user[:n]\n",
    "    if len(recs_for_user) < n:\n",
    "        recs_for_user += data['item_id'].value_counts().index[:5].tolist()[:(n - len(recs_for_user))]\n",
    "        \n",
    "    return recs_for_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1177,
   "id": "5e9d3a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_match_candidates = pd.DataFrame(data_train_lvl_2['user_id'].unique())\n",
    "df_match_candidates.columns = ['user_id']\n",
    "    \n",
    "df_match_candidates['candidates'] = df_match_candidates['user_id'].apply(\n",
    "        lambda x: self_popularity_rec(x, train_test, n=N_preds))\n",
    "\n",
    "df_items = df_match_candidates.apply(lambda x: pd.Series(x['candidates']), axis=1).stack().reset_index(level=1, drop=True)\n",
    "df_items.name = 'item_id'\n",
    "    \n",
    "df_match_candidates = df_match_candidates.drop('candidates', axis=1).join(df_items)\n",
    "    \n",
    "df_train = data_train_lvl_2[['user_id', 'item_id']].copy()\n",
    "df_train['target'] = 1 \n",
    "    \n",
    "df_train = df_match_candidates.merge(df_train, on=['user_id', 'item_id'], how='left')\n",
    "df_train = df_train.drop_duplicates(subset=['user_id', 'item_id']) \n",
    "df_train['target'].fillna(0, inplace=True)\n",
    "df_train['item_id'] = df_train['item_id'].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1178,
   "id": "fac1e84d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    0.574882\n",
       "1.0    0.425118\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 1178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1179,
   "id": "af2520ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>825123 831447 845307 852014 999999 856942 9910...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>930118 5567388 5567582 5568489 5569230 9365106...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                            item_id\n",
       "0        1  825123 831447 845307 852014 999999 856942 9910...\n",
       "1        2  930118 5567388 5567582 5568489 5569230 9365106..."
      ]
     },
     "execution_count": 1179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_lda = train_test.groupby('user_id')['item_id'].unique().reset_index()\n",
    "X_lda['item_id'] = X_lda['item_id'].apply(lambda x: ' '.join([str(i) for i in x]))\n",
    "X_lda.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1180,
   "id": "37c22f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.011969</td>\n",
       "      <td>0.011969</td>\n",
       "      <td>0.01197</td>\n",
       "      <td>0.011969</td>\n",
       "      <td>0.952123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id   topic_0   topic_1  topic_2   topic_3   topic_4\n",
       "0        1  0.011969  0.011969  0.01197  0.011969  0.952123"
      ]
     },
     "execution_count": 1180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = TfidfLDA().fit_transform(pd.DataFrame(data=X_lda['item_id'].values.reshape(-1,1), \n",
    "                                            index=X_lda['user_id'].values, columns=['item_id'])).reset_index()\n",
    "lda.rename(columns={'index': 'user_id'}, inplace=True)\n",
    "lda.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1181,
   "id": "b669cf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5, random_state=42)\n",
    "X_users = pca.fit_transform(user_factors)\n",
    "\n",
    "X_users = pd.DataFrame(data=X_users).reset_index()\n",
    "X_users.rename(columns={'index': 'user_id'}, inplace=True)\n",
    "X_users['user_id'] = X_users['user_id'].apply(lambda x: id_to_userid[x])\n",
    "\n",
    "\n",
    "pca = PCA(n_components=5, random_state=42)\n",
    "X_items = pca.fit_transform(item_factors)\n",
    "\n",
    "X_items = pd.DataFrame(data=X_items).reset_index()\n",
    "X_items.rename(columns={'index': 'item_id'}, inplace=True)\n",
    "X_items['item_id'] = X_items['item_id'].apply(lambda x: id_to_itemid[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1182,
   "id": "85890381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# week_no\tcoupon_disc\tcoupon_match_disc / merge on item\n",
    "disc_df = train_test.groupby('item_id')[['coupon_disc', 'coupon_match_disc']].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1183,
   "id": "8cf3767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first create new features in item_features! see 1st approach\n",
    "df_train = df_train.merge(item_features, on='item_id', how='left')\n",
    "df_train = df_train.merge(user_features, on='user_id', how='left')\n",
    "df_train = df_train.merge(X_users, on='user_id', how='left')\n",
    "df_train = df_train.merge(X_items, on='item_id', how='left')\n",
    "df_train = df_train.merge(emb_df, on='user_id', how='left')  \n",
    "df_train = df_train.merge(diff_emb, on=['user_id', 'item_id'], how='left')   \n",
    "df_train = df_train.merge(it_it_df, on=['user_id', 'item_id'], how='left')  \n",
    "df_train = df_train.merge(disc_df, on='item_id', how='left')   \n",
    "df_train = df_train.merge(lda, on='user_id', how='left')\n",
    "df_train['factor'] = df_train.apply(lambda row: get_dot(row['user_id'], row['item_id']), axis=1)\n",
    "df_train['inv_rank'] = df_train['user_id'].apply(lambda x: get_inv_rank(x))\n",
    "\n",
    "df_train.drop_duplicates(subset=['user_id', 'item_id'], inplace=True)\n",
    "train_test = train_test.merge(item_features[['item_id', 'department']], on='item_id', how='left')\n",
    "\n",
    "df_train = create_multiple_features(df_train, train_val=train_test)  \n",
    "df_train = create_user_features(df_train, data_train=train_test) \n",
    "df_train = create_item_features(df_train, train_val=train_test)\n",
    "df_train = df_train.merge(get_new_features(train_test), on=['user_id', 'item_id'], how='left')\n",
    "\n",
    "users_items = train_test.groupby('user_id')['item_id'].apply(list).reset_index()\n",
    "users_items['item_id'] = users_items['item_id'].apply(lambda x: x[-5:])\n",
    "\n",
    "def code_last_sales(x, df=users_items):\n",
    "    last_sales = df.loc[df['user_id'] == x[0], 'item_id'].item()\n",
    "    code = str()\n",
    "    last_sales.reverse()\n",
    "    for item in last_sales:\n",
    "        code += '1' if item == x[1] else '0'\n",
    "    return code\n",
    "\n",
    "df_train['Last5sales'] = df_train[['user_id', 'item_id']].apply(code_last_sales, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1184,
   "id": "e1902454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data: 21674\n"
     ]
    }
   ],
   "source": [
    "for column in df_train.columns:\n",
    "    if df_train[column].dtype == 'object':\n",
    "        df_train[column].fillna('unk', inplace=True)\n",
    "    elif df_train[column].dtype in ['float64', 'float32']:\n",
    "        df_train[column].fillna(0.0, inplace=True)\n",
    "\n",
    "print(f'Length of train data: {df_train.shape[0]}')\n",
    "df_train.dropna(inplace=True)\n",
    "df_train = reduce_memory(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1185,
   "id": "edcb9d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df_train.drop('target', axis=1), df_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1186,
   "id": "f059ab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_set = ['item_id', 'commodity_desc', 'sub_commodity_desc', 'curr_size_of_product', 'manufacturer_freq', \n",
    "            'department_freq', 'commodity_desc_freq', 'sub_commodity_desc_freq', 'curr_size_of_product_freq', \n",
    "            'age_desc', '0_x', '2_x', '3_x', '0_y', '2_y', '3_y', 'scalar_embedding', 'item_item', 'coupon_disc', \n",
    "            'factor', 'total_item_sales_value', 'total_quantity_value', 'total_user_sales_value', 'user_quantity_per_week',\n",
    "            'mean_trans_time_by_user', 'avg_price', 'avg_num_purchases_week', 'price', 'mean_trans_time_by_item', \n",
    "            'n_stores_with_item', 'n_unique_stores_with_item', 'median_sales_hour', 'median_weekday', 'mean_n_items_basket',\n",
    "            'mean_n_item_categories_basket', 'max_n_item_categories_basket']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1187,
   "id": "53cf056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_r = X[best_set].copy()\n",
    "cat_cols = X.select_dtypes('category').columns.tolist()\n",
    "\n",
    "grs = X.groupby('user_id', sort=False).size().to_frame()[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1188,
   "id": "c4d46563",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "              'objective':'lambdarank',\n",
    "              'boosting_type': 'gbdt',\n",
    "              'n_estimators': 2000,\n",
    "              'categorical_column': cat_cols,\n",
    "              'random_state': 42,\n",
    "              'is_unbalance': True,\n",
    "              'n_jobs': -1,\n",
    "              'max_depth': 14,\n",
    "              'learning_rate': 0.015104294331090354,\n",
    "              'num_leaves': 196,\n",
    "              'subsample': 0.9381810294606675,\n",
    "              'colsample_bytree': 0.7906492668877401,\n",
    "              'max_bin': 42,\n",
    "              'min_child_samples': 27,\n",
    "              'subsample_freq': 12\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1189,
   "id": "5d8808b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_rn = LGBMRanker(**lgb_params, silent=True)\n",
    "\n",
    "eval_h = lgb_rn.fit(X_r, \n",
    "                    y, \n",
    "                    group=grs,\n",
    "                    eval_set=[(X_r, y)], \n",
    "                    eval_group=[grs], \n",
    "                    eval_metric=['ndcg', 'map'],\n",
    "                    eval_at=[5, 10 ], \n",
    "                    early_stopping_rounds=200, \n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1190,
   "id": "f510a8ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0)"
      ]
     },
     "execution_count": 1190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_h.evals_result_['training']['ndcg@5'][-1], eval_h.evals_result_['training']['map@5'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1191,
   "id": "91a10656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>lgbm_ranker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[940947, 9655212, 856942, 9297615, 877391, 557...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[1106523, 1103898, 8090521, 1076580, 5569230, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[1106523, 998206, 1092937, 910032, 1075979, 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[883932, 1075368, 902172, 1052294, 6391541, 89...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[962199, 1126899, 1123022, 874972, 1050851, 93...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                        lgbm_ranker\n",
       "0        1  [940947, 9655212, 856942, 9297615, 877391, 557...\n",
       "1        2  [1106523, 1103898, 8090521, 1076580, 5569230, ...\n",
       "2        3  [1106523, 998206, 1092937, 910032, 1075979, 11...\n",
       "3        4  [883932, 1075368, 902172, 1052294, 6391541, 89...\n",
       "4        5  [962199, 1126899, 1123022, 874972, 1050851, 93..."
      ]
     },
     "execution_count": 1191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_preds = lgb_rn.predict(X_r)\n",
    "\n",
    "ranker_prediction = X[['user_id', 'item_id']]\n",
    "ranker_prediction[\"pred\"] = rank_preds\n",
    "\n",
    "ranker_prediction = ranker_prediction.drop_duplicates()\n",
    "ranker_prediction.sort_values(by=[\"user_id\", \"pred\"], inplace=True, ascending=False)\n",
    "\n",
    "ranked_res = ranker_prediction.groupby('user_id')['item_id'].unique().reset_index() \n",
    "ranked_res = ranked_res.rename(columns={'item_id': 'lgbm_ranker'})  \n",
    "ranked_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1192,
   "id": "54cdc0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_prediction(user_id, N=5):\n",
    "    try:\n",
    "        return ranker_prediction.loc[ranker_prediction.user_id == user_id, 'item_id'].head(N).tolist()\n",
    "    except:\n",
    "        return self_popularity_rec(user_id, train_test, n=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1193,
   "id": "1b9bbe45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranker: 0.42075750122970984\n"
     ]
    }
   ],
   "source": [
    "check = data_val_lvl_2.groupby('user_id')['item_id'].unique().reset_index()\n",
    "check.rename(columns={'item_id': 'actual'}, inplace=True)\n",
    "\n",
    "check['ranker'] = check['user_id'].apply(lambda x: get_user_prediction(x, N=5))\n",
    "\n",
    "for pred in calc_precision_at_k(check, 5):\n",
    "    print(*pred, sep=': ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1194,
   "id": "85b2807e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1.loc[~test1.user_id.isin(train_test.user_id), 'user_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1195,
   "id": "139dafd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = pd.read_csv('retail_test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1196,
   "id": "0ec3e81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranker: 0.27923033389926427\n"
     ]
    }
   ],
   "source": [
    "check2 = test1.groupby('user_id')['item_id'].unique().reset_index()\n",
    "check2.rename(columns={'item_id': 'actual'}, inplace=True)\n",
    "\n",
    "check2['ranker'] = check2['user_id'].apply(lambda x: get_user_prediction(x, N=5))\n",
    "\n",
    "for pred in calc_precision_at_k(check2, 5):\n",
    "    print(*pred, sep=': ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1199,
   "id": "e966f505",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.rename(columns={'UserId': 'user_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1200,
   "id": "77e2dd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['ranker'] = test['user_id'].apply(lambda x: get_user_prediction(x, N=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "id": "27c4c791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted to GB RecSys June-July 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/76.5k [00:00<?, ?B/s]\n",
      "100%|##########| 76.5k/76.5k [00:02<00:00, 32.2kB/s]\n"
     ]
    }
   ],
   "source": [
    "new = test[~test.user_id.isin(data_train_lvl_1.user_id.tolist())].user_id.tolist()\n",
    "test.loc[test.ranker.isna(), 'ranker'] = test.loc[test.ranker.isna(), 'user_id'].\\\n",
    "                                                                apply(lambda x: get_self_top_purchases(x))\n",
    "submission = transform_data_for_eval(test[['user_id', 'ranker']], rec_col='ranker', user_col='user_id')\n",
    "submission.to_csv('submission_1.csv', index=False)\n",
    "!kaggle competitions submit -c gb-recsys-june-july-2022 -f submission.csv -m 'test1'  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aacc7c1",
   "metadata": {},
   "source": [
    "#### Public MAP@5: 0.19247"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f52b05",
   "metadata": {},
   "source": [
    "## Second Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda5e62c",
   "metadata": {},
   "source": [
    "**1st level model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1202,
   "id": "376002a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['weighted'] = data['week_no'] ** 4\n",
    "\n",
    "test_size_weeks = 3\n",
    "\n",
    "data_train = data[data['week_no'] < data['week_no'].max() - test_size_weeks]\n",
    "data_test = data[data['week_no'] >= data['week_no'].max() - test_size_weeks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1203,
   "id": "3ef8beb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users = test['user_id'].unique()\n",
    "result_users = data_train['user_id'].unique()\n",
    "\n",
    "fake_ids = list(set(test_users) - set(result_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1204,
   "id": "a6e78375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake means unseen users in whole data_train\n",
    "fake_data = data.iloc[-len(fake_ids):,:]\n",
    "fake_data['user_id'] = fake_ids\n",
    "data_train = pd.concat([data_train, fake_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1205,
   "id": "b233a18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "popularity = data_train.groupby('item_id')['quantity'].sum().reset_index()\n",
    "popularity.rename(columns={'quantity': 'n_sold'}, inplace=True)\n",
    "\n",
    "\n",
    "top_5000 = popularity.sort_values('n_sold', ascending=False).head(5000).item_id.tolist()\n",
    "\n",
    "data_train.loc[~data_train['item_id'].isin(top_5000), 'item_id'] = 999999\n",
    "\n",
    "user_item_matrix = pd.pivot_table(data_train, \n",
    "                                  index='user_id', \n",
    "                                  columns='item_id', \n",
    "                                  values='weighted',\n",
    "                                  aggfunc='sum',\n",
    "                                  fill_value=0\n",
    "                                 )\n",
    "\n",
    "user_item_matrix = user_item_matrix.astype(float) \n",
    "\n",
    "sparse_user_item = csr_matrix(user_item_matrix).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1206,
   "id": "2dd7fc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/RostislavIllyk/HW_recommendation_systems/blob/main/final_run.ipynb\n",
    "\n",
    "def get_knn(matrix, sample_vector, n=5):\n",
    "    dist_line = cdist(sample_vector, matrix, metric='cosine')\n",
    "    items = dist_line[0].argsort()[-n:][::-1]\n",
    "    batch = matrix[items]\n",
    "    dist_selected = dist_line[0][items]\n",
    "    return batch, (dist_selected.sum() - 1) / (n - 1)\n",
    "\n",
    "def get_user_recommendations(user_item_matrix, sample_vector, k_neigbours=9, n_recomendation=5, coef1=5.5, coef2=0.001):\n",
    "    batch, dist = get_knn(user_item_matrix.values, sample_vector, k_neigbours)\n",
    "    \n",
    "    # Вектор личных предпочтений\n",
    "    rec_from_yourself = sample_vector\n",
    "    \n",
    "    # Вектор всех предпочтений\n",
    "    rec_from_all = user_item_matrix.values.mean(axis=0)\n",
    "    \n",
    "    # Вектор ближайших предпочтений\n",
    "    rec_from_neighbours = batch.mean(axis=0)\n",
    "    \n",
    "    k1, k2 = dist * coef1, dist * coef2  \n",
    "     \n",
    "    # целевой вектор\n",
    "    rec = rec_from_yourself + rec_from_all / k1 + rec_from_neighbours * k2\n",
    "    \n",
    "    # Отбор товаров с максимальными весами\n",
    "    items = rec[0].argsort()[-n_recomendation-1:][::-1]\n",
    "    \n",
    "    idx_to_zero = itemid_to_id[999999]\n",
    "    items = list(items)\n",
    "    if idx_to_zero in items:\n",
    "        items.remove(idx_to_zero)\n",
    "    items =items[:n_recomendation]\n",
    "    \n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1207,
   "id": "063250b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "userids = user_item_matrix.index.values\n",
    "itemids = user_item_matrix.columns.values\n",
    "\n",
    "matrix_userids = np.arange(len(userids))\n",
    "matrix_itemids = np.arange(len(itemids))\n",
    "\n",
    "id_to_itemid = dict(zip(matrix_itemids, itemids))\n",
    "id_to_userid = dict(zip(matrix_userids, userids))\n",
    "\n",
    "itemid_to_id = dict(zip(itemids, matrix_itemids))\n",
    "userid_to_id = dict(zip(userids, matrix_userids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1208,
   "id": "55edc2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[821867, 834484, 856942, 865456, 889248, 90795...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>[835476, 851057, 872021, 878302, 879948, 90963...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>[920308, 926804, 946489, 1006718, 1017061, 107...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>[840386, 889774, 898068, 909714, 929067, 95347...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>[835098, 872137, 910439, 924610, 992977, 10412...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                             actual\n",
       "0        1  [821867, 834484, 856942, 865456, 889248, 90795...\n",
       "1        3  [835476, 851057, 872021, 878302, 879948, 90963...\n",
       "2        6  [920308, 926804, 946489, 1006718, 1017061, 107...\n",
       "3        7  [840386, 889774, 898068, 909714, 929067, 95347...\n",
       "4        8  [835098, 872137, 910439, 924610, 992977, 10412..."
      ]
     },
     "execution_count": 1208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = data_test.groupby('user_id')['item_id'].unique().reset_index()\n",
    "result.columns=['user_id', 'actual']\n",
    "result['actual'] = result['actual'].apply(lambda x: list(x))\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1209,
   "id": "bdda39db",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = result['user_id'].map(userid_to_id).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1210,
   "id": "3c953f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2042/2042 [04:34<00:00,  7.44it/s]\n"
     ]
    }
   ],
   "source": [
    "k_neigbours = 18 \n",
    "n_recomendation = 10\n",
    "\n",
    "rec_list=[]\n",
    "for i in tqdm.trange(len(id_list)):    \n",
    "    sample_vector = np.reshape(user_item_matrix.values[id_list[i]],(1,-1))\n",
    "    item = get_user_recommendations(user_item_matrix, sample_vector, k_neigbours=k_neigbours, n_recomendation=n_recomendation)\n",
    "    rec_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1211,
   "id": "3d218717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2042/2042 [00:00<00:00, 51831.36it/s]\n"
     ]
    }
   ],
   "source": [
    "result_list=[]\n",
    "for i in tqdm.trange(len(rec_list)):\n",
    "    item = [id_to_itemid[rec] for rec in rec_list[i]]\n",
    "    result_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1212,
   "id": "24d60e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['test'] = result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1213,
   "id": "400592f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43819784524975514"
      ]
     },
     "execution_count": 1213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.apply(lambda x: precision_at_k(x['test'], x['actual'],  5), axis=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a587d0",
   "metadata": {},
   "source": [
    "**For further imputation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1214,
   "id": "4d68880c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 1214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_id_list = list(userid_to_id.values())\n",
    "len(set(train_id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1215,
   "id": "8f06e420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2500/2500 [05:47<00:00,  7.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2500/2500 [00:00<00:00, 125000.12it/s]\n"
     ]
    }
   ],
   "source": [
    "train_rec_list=[]\n",
    "for i in tqdm.trange(len(train_id_list)):    \n",
    "    sample_vector = np.reshape(user_item_matrix.values[train_id_list[i]],(1,-1))\n",
    "    item = get_user_recommendations(user_item_matrix, sample_vector, k_neigbours=9, n_recomendation=5)\n",
    "    train_rec_list.append(item)\n",
    "    \n",
    "train_result_list=[]\n",
    "for i in tqdm.trange(len(train_rec_list)):\n",
    "    item = [id_to_itemid[rec] for rec in train_rec_list[i]]\n",
    "    train_result_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1216,
   "id": "42c2253f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>recs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[856942, 1082185, 995242, 940947, 5577022]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[1106523, 838136, 1133018, 826784, 916122]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                        recs\n",
       "0        0  [856942, 1082185, 995242, 940947, 5577022]\n",
       "1        1  [1106523, 838136, 1133018, 826784, 916122]"
      ]
     },
     "execution_count": 1216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_result = pd.DataFrame(data={'user_id': train_id_list, 'recs': train_result_list})\n",
    "total_result.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1217,
   "id": "eaa350df",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_result['user_id'] = total_result['user_id'] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacc4f98",
   "metadata": {},
   "source": [
    "**2nd level model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1218,
   "id": "545bcd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_match_candidates = pd.DataFrame(data_test['user_id'].unique())\n",
    "df_match_candidates.columns = ['user_id']\n",
    "    \n",
    "df_match_candidates = df_match_candidates.merge(result[['user_id', 'test']], on='user_id', how='left')\n",
    "df_match_candidates.rename(columns={'test': 'candidates'}, inplace=True)\n",
    "\n",
    "df_items = df_match_candidates.apply(lambda x: pd.Series(x['candidates']), axis=1).stack().reset_index(level=1, drop=True)\n",
    "df_items.name = 'item_id'\n",
    "    \n",
    "df_match_candidates = df_match_candidates.drop('candidates', axis=1).join(df_items)\n",
    "    \n",
    "df_train = data_test[['user_id', 'item_id']].copy()\n",
    "df_train['target'] = 1 \n",
    "    \n",
    "df_train = df_match_candidates.merge(df_train, on=['user_id', 'item_id'], how='left')\n",
    "df_train = df_train.drop_duplicates(subset=['user_id', 'item_id']) \n",
    "df_train['target'].fillna(0, inplace=True)\n",
    "df_train['item_id'] = df_train['item_id'].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1219,
   "id": "314acb75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    13093\n",
       "1.0     7327\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 1219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2159e639",
   "metadata": {},
   "source": [
    "**Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a80a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not forget to create freqs in item_features, see 1st approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1220,
   "id": "eeb1a2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d6c7ac65354c6eb8a18396fdce8622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301af3d2934d481097f17cd2f4f5c51d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36a6f8e1b304486a71a9f09a3155593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recommender = MainRecommender(data_train, user_item_matrix=user_item_matrix, weighting=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1221,
   "id": "24d96be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_userid = recommender.id_to_userid\n",
    "id_to_itemid = recommender.id_to_itemid\n",
    "\n",
    "userid_to_id = recommender.userid_to_id\n",
    "itemid_to_id = recommender.itemid_to_id\n",
    "\n",
    "user_factors = recommender.model.user_factors\n",
    "item_factors = recommender.model.item_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1222,
   "id": "72ff6045",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df = data_train[['user_id', 'item_id']].copy()\n",
    "emb_df['item_id'] = emb_df['item_id'].apply(lambda x: np.array(item_factors[itemid_to_id[x], :]))\n",
    "emb_df = emb_df.groupby('user_id')['item_id'].sum().reset_index()\n",
    "\n",
    "# scalar value\n",
    "emb_df['item_id'] = emb_df['item_id'].apply(lambda x: x.mean())\n",
    "emb_df.rename(columns={'item_id': 'scalar_embedding'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1223,
   "id": "a0cd49d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_emb = data_train[['user_id', 'item_id']].copy()\n",
    "diff_emb['diff_emb_scalar'] = diff_emb.apply(lambda row: (item_factors[itemid_to_id[row['item_id']]] - user_factors[userid_to_id[row['user_id']]]).mean(), axis=1)\n",
    "diff_emb.drop_duplicates(subset=['user_id', 'item_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1225,
   "id": "3ed6d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_ = prefilter_items(data_train, item_features=item_features, take_n_popular=10000, n_weeks=52).drop('weighted', axis=1)\n",
    "\n",
    "train_test = pd.concat([data_train_, data_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1226,
   "id": "a2dc349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=5, random_state=42)\n",
    "X_users = pca.fit_transform(scaler.fit_transform(user_factors))\n",
    "\n",
    "X_users = pd.DataFrame(data=X_users).reset_index()\n",
    "X_users.rename(columns={'index': 'user_id'}, inplace=True)\n",
    "X_users['user_id'] = X_users['user_id'].apply(lambda x: id_to_userid[x])\n",
    "\n",
    "\n",
    "pca = PCA(n_components=5, random_state=42)\n",
    "X_items = pca.fit_transform(scaler.fit_transform(item_factors))\n",
    "\n",
    "X_items = pd.DataFrame(data=X_items).reset_index()\n",
    "X_items.rename(columns={'index': 'item_id'}, inplace=True)\n",
    "X_items['item_id'] = X_items['item_id'].apply(lambda x: id_to_itemid[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1227,
   "id": "2740d3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_df = train_test.groupby('item_id')[['coupon_disc', 'coupon_match_disc']].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1228,
   "id": "7422a238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "Step 2\n",
      "Step 3\n"
     ]
    }
   ],
   "source": [
    "print('Step 1')\n",
    "df_train = df_train.merge(item_features, on='item_id', how='left')\n",
    "df_train = df_train.merge(user_features, on='user_id', how='left')\n",
    "df_train = df_train.merge(X_users, on='user_id', how='left')\n",
    "df_train = df_train.merge(X_items, on='item_id', how='left')\n",
    "df_train = df_train.merge(emb_df, on='user_id', how='left')  \n",
    "df_train = df_train.merge(diff_emb, on=['user_id', 'item_id'], how='left')   \n",
    "df_train = df_train.merge(disc_df, on='item_id', how='left')   \n",
    "df_train['factor'] = df_train.apply(lambda row: get_dot(row['user_id'], row['item_id']), axis=1)\n",
    "df_train['inv_rank'] = df_train['user_id'].apply(lambda x: get_inv_rank(x))\n",
    "\n",
    "df_train.drop_duplicates(subset=['user_id', 'item_id'], inplace=True)\n",
    "train_test = train_test.merge(item_features[['item_id', 'department']], on='item_id', how='left')\n",
    "print('Step 2')\n",
    "df_train = create_multiple_features(df_train, train_val=train_test)  \n",
    "df_train = create_user_features(df_train, data_train=train_test) \n",
    "df_train = create_item_features(df_train, train_val=train_test)\n",
    "df_train = df_train.merge(get_new_features(train_test), on=['user_id', 'item_id'], how='left')\n",
    "\n",
    "print('Step 3')\n",
    "users_items = train_test.groupby('user_id')['item_id'].apply(list).reset_index()\n",
    "users_items['item_id'] = users_items['item_id'].apply(lambda x: x[-5:])\n",
    "\n",
    "def code_last_sales(x, df=users_items):\n",
    "    last_sales = df.loc[df['user_id'] == x[0], 'item_id'].item()\n",
    "    code = str()\n",
    "    last_sales.reverse()\n",
    "    for item in last_sales:\n",
    "        code += '1' if item == x[1] else '0'\n",
    "    return code\n",
    "\n",
    "df_train['Last5sales'] = df_train[['user_id', 'item_id']].apply(code_last_sales, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1229,
   "id": "47c5fece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data: 20420\n"
     ]
    }
   ],
   "source": [
    "for column in df_train.columns:\n",
    "    if df_train[column].dtype == 'object':\n",
    "        df_train[column].fillna('unk', inplace=True)\n",
    "    elif df_train[column].dtype in ['float64', 'float32']:\n",
    "        df_train[column].fillna(0.0, inplace=True)\n",
    "\n",
    "print(f'Length of train data: {df_train.shape[0]}')\n",
    "df_train.dropna(inplace=True)\n",
    "df_train = reduce_memory(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1230,
   "id": "e1ba19a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df_train.drop('target', axis=1), df_train['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95851024",
   "metadata": {},
   "source": [
    "**Straightforward (stupid) but most accurate feature selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "id": "51d75194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Index(['user_id', 'item_id', 'manufacturer', 'brand', 'sub_commodity_desc',\n",
       "        'curr_size_of_product', 'department_freq', 'commodity_desc_freq',\n",
       "        'sub_commodity_desc_freq', 'curr_size_of_product_freq', '0_x', '1_x',\n",
       "        '2_x', '3_x', '4_x', '0_y', '1_y', '2_y', '4_y', 'scalar_embedding',\n",
       "        'coupon_disc', 'coupon_match_disc', 'factor', 'total_quantity_value',\n",
       "        'item_freq', 'user_freq', 'total_user_sales_value',\n",
       "        'user_quantity_per_week', 'avg_cheque', 'users_unique_departments',\n",
       "        'mean_trans_time_by_user', 'mean_sales_value_per_basket',\n",
       "        'avg_basket_department', 'avg_price', 'n_stores_with_item',\n",
       "        'n_unique_stores_with_item', 'median_sales_hour', 'median_weekday',\n",
       "        'mean_visits_interval', 'n_transactions', 'mean_n_items_basket',\n",
       "        'std_n_items_basket', 'mean_n_item_categories_basket',\n",
       "        'max_n_item_categories_basket', 'std_n_item_categories_basket',\n",
       "        'Last5sales'],\n",
       "       dtype='object'),\n",
       " 0.3482228116710876)"
      ]
     },
     "execution_count": 812,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grs = X.groupby('user_id', sort=False).size().to_frame()[0].values\n",
    "\n",
    "class BestSet:\n",
    "    def __init__(self, k_features=35):\n",
    "        self.k_features = k_features\n",
    "        self.users = test1.loc[~test1.user_id.isin(data_train.user_id.tolist()), 'user_id'].unique().tolist()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        dim = X.shape[1]\n",
    "        self.indices_ = tuple(range(dim))\n",
    "        self.subsets_ = [self.indices_]\n",
    "        score = self._calc_score(X, y, self.indices_)\n",
    "        self.scores_ = [score]\n",
    "\n",
    "        while dim > self.k_features:\n",
    "            scores, subsets = [], []\n",
    "            for p in tqdm.notebook.tqdm(combinations(self.indices_, r=dim-1), total=dim, leave=False):\n",
    "                score = self._calc_score(X, y, p)\n",
    "                scores.append(score)\n",
    "                subsets.append(p)\n",
    "            best = np.argmax(scores)\n",
    "            self.indices_ = subsets[best]\n",
    "            self.subsets_.append(self.indices_)\n",
    "            dim -= 1\n",
    "            self.scores_.append(scores[best])\n",
    "        self.k_score_ = self.scores_[-1]\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        best_indices = self.subsets_[np.argmax(self.scores_)]\n",
    "        return X.iloc[:, list(best_indices)]\n",
    "\n",
    "    def _calc_score(self, X, y, indices):\n",
    "        cat_cols = X.select_dtypes('category').columns.tolist()\n",
    "\n",
    "        model = LGBMRanker(objective='lambdarank',\n",
    "                           boosting_type='gbdt',\n",
    "                           n_estimators=500,\n",
    "                           categorical_column=cat_cols,\n",
    "                           random_state=42,\n",
    "                           is_unbalance=True,\n",
    "                           n_jobs=-1,)\n",
    "\n",
    "        eval_h = model.fit(X.iloc[:, list(indices)], \n",
    "                           y, \n",
    "                           group=grs,\n",
    "                           eval_set=[(X.iloc[:, list(indices)], y)], \n",
    "                           eval_group=[grs], \n",
    "                           eval_metric=['ndcg'],\n",
    "                           eval_at=[5, 10 ], \n",
    "                           early_stopping_rounds=50, \n",
    "                           verbose=0)\n",
    "    \n",
    "        rank_preds = model.predict(X.iloc[:, list(indices)])\n",
    "        \n",
    "        ranker_prediction = X[['user_id', 'item_id']].copy()\n",
    "        ranker_prediction[\"pred\"] = rank_preds\n",
    "\n",
    "        ranker_prediction = ranker_prediction.drop_duplicates()\n",
    "        ranker_prediction.sort_values(by=[\"user_id\", \"pred\"], inplace=True, ascending=False)\n",
    "            \n",
    "        ranked_res = ranker_prediction.groupby('user_id')['item_id'].unique().reset_index() \n",
    "        ranked_res = ranked_res.rename(columns={'item_id': 'lgbm_ranker'})  \n",
    "\n",
    "        result1 = test1.groupby('user_id')['item_id'].unique().reset_index()\n",
    "        result1.columns = ['user_id', 'actual']\n",
    "\n",
    "        result1 = result1.merge(ranked_res, on='user_id', how='left')\n",
    "        result1.loc[result1.lgbm_ranker.isna(), 'lgbm_ranker'] = result1.loc[result1.lgbm_ranker.isna(), 'user_id'].\\\n",
    "                               apply(lambda x: total_result.loc[total_result.user_id == x, 'recs'].values[0])\n",
    "\n",
    "        score = result1.apply(lambda row: precision_at_k(row['lgbm_ranker'], row['actual'], k=5), axis=1).mean()\n",
    "\n",
    "        return score\n",
    "    \n",
    "selector = BestSet()\n",
    "selector.fit(X, y)\n",
    "X.columns[list(selector.subsets_[np.argmax(selector.scores_)])], max(selector.scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1231,
   "id": "9bdb170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_set = ['manufacturer', 'brand', 'commodity_desc', 'sub_commodity_desc',\n",
    "            'curr_size_of_product', 'commodity_desc_freq',\n",
    "            'curr_size_of_product_freq', '0_x', '1_x', '2_x', '4_x', '0_y', '1_y',\n",
    "            '2_y', '4_y', 'scalar_embedding', 'diff_emb_scalar', 'coupon_disc',\n",
    "            'coupon_match_disc', 'inv_rank', 'total_item_sales_value',\n",
    "            'total_quantity_value', 'item_freq', 'user_freq',\n",
    "            'total_user_sales_value', 'avg_cheque', 'users_unique_departments',\n",
    "            'mean_sales_value_per_basket', 'unique_bought_items',\n",
    "            'avg_basket_department', 'mean_sales_value_category', 'avg_price',\n",
    "            'avg_num_purchases_week', 'mean_trans_time_by_item',\n",
    "            'n_stores_with_item', 'n_unique_stores_with_item', 'median_weekday',\n",
    "            'mean_visits_interval', 'n_transactions', 'mean_n_items_basket',\n",
    "            'max_n_items_basket', 'std_n_items_basket',\n",
    "            'mean_n_item_categories_basket', 'max_n_item_categories_basket',\n",
    "            'std_n_item_categories_basket', 'Last5sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec492eea",
   "metadata": {},
   "source": [
    "**Hyperparameter tunning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1232,
   "id": "12ae6506",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_r = X[best_set].copy()\n",
    "cat_cols = X.select_dtypes('category').columns.tolist()\n",
    "\n",
    "grs = X.groupby('user_id', sort=False).size().to_frame()[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "id": "31d38fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_params = {\n",
    "                'objective':'lambdarank',\n",
    "                'boosting_type': 'gbdt',\n",
    "                'n_estimators': 2000,\n",
    "                'categorical_column': cat_cols,\n",
    "                'random_state': 42,\n",
    "                'is_unbalance': True,\n",
    "                'n_jobs': -1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "id": "cae08029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    param_trials = {\n",
    "                    'max_depth': trial.suggest_int('max_depth', 9, 15),\n",
    "                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "                    'num_leaves': trial.suggest_int('num_leaves', 20, 265),\n",
    "                    'subsample': trial.suggest_float('subsample', 0.3, 1.),\n",
    "                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 1.),\n",
    "                    'max_bin': trial.suggest_int('max_bin', 10, 260),\n",
    "                    'min_child_samples': trial.suggest_int('min_child_samples', 20, 260),\n",
    "                    'subsample_freq': trial.suggest_int('subsample_freq', 1, 20),\n",
    "                    }\n",
    "    param_trials.update(start_params)\n",
    "    lgb_rn = LGBMRanker(**param_trials, silent=True)\n",
    "\n",
    "    eval_h = lgb_rn.fit(X_r, y, group=grs,\n",
    "                        eval_set=[(X_r, y)], eval_group=[grs], \n",
    "                        eval_metric=['ndcg'],\n",
    "                        eval_at=[5, 10 ], \n",
    "                        early_stopping_rounds=50, \n",
    "                        verbose=0)\n",
    "    \n",
    "    rank_preds = lgb_rn.predict(X_r)\n",
    "    ranker_prediction = X[['user_id', 'item_id']].copy()\n",
    "    ranker_prediction[\"pred\"] = rank_preds\n",
    "\n",
    "    ranker_prediction = ranker_prediction.drop_duplicates()\n",
    "    ranker_prediction.sort_values(by=[\"user_id\", \"pred\"], inplace=True, ascending=False)\n",
    "            \n",
    "    ranked_res = ranker_prediction.groupby('user_id')['item_id'].unique().reset_index() \n",
    "    ranked_res = ranked_res.rename(columns={'item_id': 'lgbm_ranker'})  \n",
    "\n",
    "    result1 = test1.groupby('user_id')['item_id'].unique().reset_index()\n",
    "    result1.columns = ['user_id', 'actual']\n",
    "\n",
    "    result1 = result1.merge(ranked_res, on='user_id', how='left')\n",
    "    result1.loc[result1.lgbm_ranker.isna(), 'lgbm_ranker'] = result1.loc[result1.lgbm_ranker.isna(), 'user_id'].\\\n",
    "                               apply(lambda x: total_result.loc[total_result.user_id == x, 'recs'].values[0])\n",
    "\n",
    "    score = result1.apply(lambda row: precision_at_k(row['lgbm_ranker'], row['actual'], k=5), axis=1).mean()\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "id": "c1d5946a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of completed trials: 280\n",
      "Best trial\n",
      "Best score: 0.3474801061007958\n",
      "Best params:  {'max_depth': 14, 'learning_rate': 0.021262064401654825, 'num_leaves': 111, 'subsample': 0.7643251606704898, 'colsample_bytree': 0.5767734383871105, 'max_bin': 162, 'min_child_samples': 59, 'subsample_freq': 17}\n"
     ]
    }
   ],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.FATAL)\n",
    "study = optuna.create_study(sampler=TPESampler(seed=42), direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=500, timeout=3000)\n",
    "\n",
    "print(f'Number of completed trials: {len(study.trials)}')\n",
    "print('Best trial')\n",
    "trial = study.best_trial\n",
    "print(f'Best score: {trial.value}')\n",
    "print('Best params: ', trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d83028a",
   "metadata": {},
   "source": [
    "**Final predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1233,
   "id": "d38c80c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "              'objective':'lambdarank',\n",
    "              'boosting_type': 'gbdt',\n",
    "              'n_estimators': 2000,\n",
    "              'categorical_column': cat_cols,\n",
    "              'random_state': 42,\n",
    "              'is_unbalance': True,\n",
    "              'n_jobs': -1,\n",
    "              'max_depth': 12, \n",
    "              'learning_rate': 0.027892346545495567, \n",
    "              'num_leaves': 78, \n",
    "              'subsample': 0.6169142769038929, \n",
    "              'colsample_bytree': 0.3388817982402172, \n",
    "              'max_bin': 138, \n",
    "              'min_child_samples': 260, \n",
    "              'subsample_freq': 12\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1234,
   "id": "b872368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_rn = LGBMRanker(**lgb_params, silent=True)\n",
    "\n",
    "eval_h = lgb_rn.fit(X_r, \n",
    "                    y, \n",
    "                    group=grs,\n",
    "                    eval_set=[(X_r, y)], \n",
    "                    eval_group=[grs], \n",
    "                    eval_metric=['ndcg', 'map'],\n",
    "                    eval_at=[5, 10 ], \n",
    "                    early_stopping_rounds=200, \n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1235,
   "id": "b38322e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>lgbm_ranker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[995242, 1074612, 1082185, 940947, 856942, 557...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>[9526563, 1053690, 6463658, 1092937, 910032, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>[840361, 1119051, 1037863, 845208, 1024306, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>[1082185, 1013321, 1022003, 1106523, 1122358, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>[1116578, 840361, 1005186, 1082185, 1029743, 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                        lgbm_ranker\n",
       "0        1  [995242, 1074612, 1082185, 940947, 856942, 557...\n",
       "1        3  [9526563, 1053690, 6463658, 1092937, 910032, 1...\n",
       "2        6  [840361, 1119051, 1037863, 845208, 1024306, 10...\n",
       "3        7  [1082185, 1013321, 1022003, 1106523, 1122358, ...\n",
       "4        8  [1116578, 840361, 1005186, 1082185, 1029743, 8..."
      ]
     },
     "execution_count": 1235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_preds = lgb_rn.predict(X_r)\n",
    "\n",
    "ranker_prediction = X[['user_id', 'item_id']]\n",
    "ranker_prediction[\"pred\"] = rank_preds\n",
    "\n",
    "ranker_prediction = ranker_prediction.drop_duplicates()\n",
    "ranker_prediction.sort_values(by=[\"user_id\", \"pred\"], inplace=True, ascending=False)\n",
    "\n",
    "ranked_res = ranker_prediction.groupby('user_id')['item_id'].unique().reset_index() \n",
    "ranked_res = ranked_res.rename(columns={'item_id': 'lgbm_ranker'})  \n",
    "ranked_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1236,
   "id": "66aaffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_prediction(user_id, N=5):\n",
    "    \n",
    "    items = ranker_prediction.loc[ranker_prediction.user_id == user_id, 'item_id'].head(N).tolist()\n",
    "    if len(items) == 0:\n",
    "        items = total_result.loc[total_result.user_id == user_id, 'recs'].values[0]\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1237,
   "id": "b0117947",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['ranker'] = test['user_id'].apply(lambda x: get_user_prediction(x, N=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1148,
   "id": "98e37e43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted to GB RecSys June-July 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/80.2k [00:00<?, ?B/s]\n",
      "100%|##########| 80.2k/80.2k [00:04<00:00, 19.7kB/s]\n"
     ]
    }
   ],
   "source": [
    "submission = transform_data_for_eval(test[['user_id', 'ranker']], rec_col='ranker', user_col='user_id')\n",
    "submission.to_csv('submission_2.csv', index=False)\n",
    "!kaggle competitions submit -c gb-recsys-june-july-2022 -f submission.csv -m 'test1' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20540680",
   "metadata": {},
   "source": [
    "#### Public MAP@5: 0.30775"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "id": "3b763abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted to GB RecSys June-July 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/80.3k [00:00<?, ?B/s]\n",
      "100%|##########| 80.3k/80.3k [00:01<00:00, 45.0kB/s]\n"
     ]
    }
   ],
   "source": [
    "test = test.merge(total_result, on='user_id', how='left')\n",
    "submission = transform_data_for_eval(test[['user_id', 'recs']], rec_col='recs', user_col='user_id')\n",
    "submission.to_csv('submission_3.csv', index=False)\n",
    "!kaggle competitions submit -c gb-recsys-june-july-2022 -f submission.csv -m 'test1' \n",
    "# only first model is better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e536cc0",
   "metadata": {},
   "source": [
    "#### Public MAP@5: 0.33113"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689e1030",
   "metadata": {},
   "source": [
    "## Predict by 1st model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1240,
   "id": "795e3630",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('retail_train.csv')\n",
    "test = pd.read_csv('test_user.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1241,
   "id": "0755c2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2325]"
      ]
     },
     "execution_count": 1241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['weighted'] = data['week_no'] ** 4\n",
    "\n",
    "test.rename(columns={'UserId': 'user_id'}, inplace=True)\n",
    "test_users = test['user_id'].unique()\n",
    "result_users = data['user_id'].unique()\n",
    "\n",
    "fake_ids = list(set(test_users) - set(result_users))\n",
    "fake_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1242,
   "id": "003634c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = data.iloc[-len(fake_ids):,:]\n",
    "fake_data['user_id'] = fake_ids\n",
    "data = pd.concat([data, fake_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1243,
   "id": "34797b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "popularity = data.groupby('item_id')['quantity'].sum().reset_index()\n",
    "popularity.rename(columns={'quantity': 'n_sold'}, inplace=True)\n",
    "\n",
    "\n",
    "top_5000 = popularity.sort_values('n_sold', ascending=False).head(5000).item_id.tolist()\n",
    "\n",
    "data.loc[~data['item_id'].isin(top_5000), 'item_id'] = 999999\n",
    "\n",
    "user_item_matrix = pd.pivot_table(data, \n",
    "                                  index='user_id', \n",
    "                                  columns='item_id', \n",
    "                                  values='weighted', \n",
    "                                  aggfunc='sum',\n",
    "                                  fill_value=0\n",
    "                                 )\n",
    "\n",
    "user_item_matrix = user_item_matrix.astype(float) \n",
    "\n",
    "sparse_user_item = csr_matrix(user_item_matrix).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1244,
   "id": "f5936827",
   "metadata": {},
   "outputs": [],
   "source": [
    "userids = user_item_matrix.index.values\n",
    "itemids = user_item_matrix.columns.values\n",
    "\n",
    "matrix_userids = np.arange(len(userids))\n",
    "matrix_itemids = np.arange(len(itemids))\n",
    "\n",
    "id_to_itemid = dict(zip(matrix_itemids, itemids))\n",
    "id_to_userid = dict(zip(matrix_userids, userids))\n",
    "\n",
    "itemid_to_id = dict(zip(itemids, matrix_itemids))\n",
    "userid_to_id = dict(zip(userids, matrix_userids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1245,
   "id": "eb2ebb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = test['user_id'].map(userid_to_id).values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1246,
   "id": "90b1c20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1885/1885 [04:21<00:00,  7.21it/s]\n"
     ]
    }
   ],
   "source": [
    "k_neigbours=9 \n",
    "n_recomendation = 5\n",
    "\n",
    "rec_list=[]\n",
    "for i in tqdm.trange(len(id_list)):    \n",
    "    sample_vector = np.reshape(user_item_matrix.values[id_list[i]],(1,-1))\n",
    "    item = get_user_recommendations(user_item_matrix, sample_vector, k_neigbours=k_neigbours, n_recomendation=n_recomendation)\n",
    "    rec_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1247,
   "id": "bc146530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1885/1885 [00:00<00:00, 124881.74it/s]\n"
     ]
    }
   ],
   "source": [
    "result_list=[]\n",
    "for i in tqdm.trange(len(rec_list)):\n",
    "    item = [id_to_itemid[rec] for rec in rec_list[i]]\n",
    "    result_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1248,
   "id": "d1724980",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(result_list)):\n",
    "    for j in range(len(result_list[i])):\n",
    "        result_list[i][j] = str(result_list[i][j])\n",
    "    result_list[i] = ' '.join(result_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1249,
   "id": "bf081cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>856942 1082185 995242 940947 8293439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1106523 1133018 838136 826784 916122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1053690 9526563 6463658 910032 13842214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>1082185 1037863 1119051 840361 1024306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>1082185 1122358 1106523 1022003 1013321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>2496</td>\n",
       "      <td>981760 916122 1106523 883404 992870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>2497</td>\n",
       "      <td>860776 995785 1066685 834484 897125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1882</th>\n",
       "      <td>2498</td>\n",
       "      <td>1070820 1082185 1022066 901776 1053690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1883</th>\n",
       "      <td>2499</td>\n",
       "      <td>1070820 5568378 1060872 5570048 5569327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1884</th>\n",
       "      <td>2500</td>\n",
       "      <td>1082185 6534178 1065538 859237 951703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1885 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     UserId                                Predicted\n",
       "0         1     856942 1082185 995242 940947 8293439\n",
       "1         2     1106523 1133018 838136 826784 916122\n",
       "2         3  1053690 9526563 6463658 910032 13842214\n",
       "3         6   1082185 1037863 1119051 840361 1024306\n",
       "4         7  1082185 1122358 1106523 1022003 1013321\n",
       "...     ...                                      ...\n",
       "1880   2496      981760 916122 1106523 883404 992870\n",
       "1881   2497      860776 995785 1066685 834484 897125\n",
       "1882   2498   1070820 1082185 1022066 901776 1053690\n",
       "1883   2499  1070820 5568378 1060872 5570048 5569327\n",
       "1884   2500    1082185 6534178 1065538 859237 951703\n",
       "\n",
       "[1885 rows x 2 columns]"
      ]
     },
     "execution_count": 1249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['Predicted'] = result_list\n",
    "test.columns = [['UserId', 'Predicted']]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1250,
   "id": "3bd0dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 974,
   "id": "ca5aacf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted to GB RecSys June-July 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/80.3k [00:00<?, ?B/s]\n",
      "100%|##########| 80.3k/80.3k [00:01<00:00, 55.2kB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c gb-recsys-june-july-2022 -f final.csv -m 'test2'  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b905fc",
   "metadata": {},
   "source": [
    "#### Public MAP@5: 0.34917"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb6064d",
   "metadata": {},
   "source": [
    "**Predict for all**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "id": "07093feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 978,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_id_list = list(userid_to_id.values())\n",
    "len(set(train_id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "id": "4a10b0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2500/2500 [05:27<00:00,  7.63it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 2500/2500 [00:00<00:00, 131522.46it/s]\n"
     ]
    }
   ],
   "source": [
    "train_rec_list=[]\n",
    "for i in tqdm.trange(len(train_id_list)):    \n",
    "    sample_vector = np.reshape(user_item_matrix.values[train_id_list[i]],(1,-1))\n",
    "    item = get_user_recommendations(user_item_matrix, sample_vector, k_neigbours=9, n_recomendation=5)\n",
    "    train_rec_list.append(item)\n",
    "    \n",
    "train_result_list=[]\n",
    "for i in tqdm.trange(len(train_rec_list)):\n",
    "    item = [id_to_itemid[rec] for rec in train_rec_list[i]]\n",
    "    train_result_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "id": "2da2d1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_result = pd.DataFrame(data={'user_id': train_id_list, 'recs': train_result_list})\n",
    "total_result['user_id'] = total_result['user_id'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "id": "51d1223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_result['recs'] = total_result['recs'].apply(lambda x: ' '.join([str(i) for i in x]))\n",
    "total_result.columns = [['UserId', 'Predicted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 985,
   "id": "d6402559",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_result.to_csv('total_prediction.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
