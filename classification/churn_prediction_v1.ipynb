{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Credit Card Customers","metadata":{}},{"cell_type":"markdown","source":"Importing all necessary modules","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nfrom IPython.display import clear_output\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.model_selection import train_test_split, cross_val_score, learning_curve\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\nfrom sklearn.metrics import classification_report, accuracy_score, roc_curve, auc\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import PowerTransformer\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import ColumnTransformer\n\nimport category_encoders as ce\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.ensemble import AdaBoostClassifier\n\nfrom scipy import stats\nimport statsmodels.stats.power as power\n\nfrom imblearn.pipeline import Pipeline as Pipe\nfrom itertools import combinations\nfrom imblearn.combine import SMOTEENN\n\nfrom catboost import Pool, CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nimport optuna\nfrom optuna.samplers import TPESampler\n\nimport category_encoders as ce\n!pip install -q BorutaShap\nfrom BorutaShap import BorutaShap\nimport shap\n\nnp.random.seed(123)\nclear_output()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:54:39.808392Z","iopub.execute_input":"2022-03-31T11:54:39.808737Z","iopub.status.idle":"2022-03-31T11:54:55.719782Z","shell.execute_reply.started":"2022-03-31T11:54:39.808644Z","shell.execute_reply":"2022-03-31T11:54:55.718773Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# defining the path to the dataset and target name \ndata_root = '../input/credit-card-customers/BankChurners.csv'\nTARGET_NAME = 'Attrition_Flag'","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:55:00.303107Z","iopub.execute_input":"2022-03-31T11:55:00.303430Z","iopub.status.idle":"2022-03-31T11:55:00.307783Z","shell.execute_reply.started":"2022-03-31T11:55:00.303384Z","shell.execute_reply":"2022-03-31T11:55:00.306922Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# load dataset in dataframe\ndf = pd.read_csv(data_root)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:57:23.712424Z","iopub.execute_input":"2022-03-31T11:57:23.712740Z","iopub.status.idle":"2022-03-31T11:57:23.758704Z","shell.execute_reply.started":"2022-03-31T11:57:23.712705Z","shell.execute_reply":"2022-03-31T11:57:23.757617Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Next we define helper functions:\n* show_proba_calibration_plots: displays probability threshold, that helps to increase recall, precision or F-score\n* report: displays the estimators results - metrics\n* reduce_memory: transforms the input pandas dataframe dtypes into lower memory size if possible\n* plot_ROC: displays Area under curve plots to visualize roc_auc score\n* vote: helps to apply majority voting if we use multiple estimators\n* cross_validation: displays cross_validation results and mean score\n* statistic_output, effect_size, categorical_stats: helper functions to make statistical tests as for numerical rather than categorical features to see if they have any statistical impact on target variable","metadata":{}},{"cell_type":"code","source":"def show_proba_calibration_plots(y_predicted_probs, y_true_labels):\n    preds_with_true_labels = np.array(list(zip(y_predicted_probs, y_true_labels)))\n\n    thresholds = []\n    precisions = []\n    recalls = []\n    f1_scores = []\n\n    for threshold in np.linspace(0.1, 0.9, 9):\n        thresholds.append(threshold)\n        precisions.append(precision_score(y_true_labels, list(map(int, y_predicted_probs > threshold))))\n        recalls.append(recall_score(y_true_labels, list(map(int, y_predicted_probs > threshold))))\n        f1_scores.append(f1_score(y_true_labels, list(map(int, y_predicted_probs > threshold))))\n\n    scores_table = pd.DataFrame({'f1':f1_scores,\n                                 'precision':precisions,\n                                 'recall':recalls,\n                                 'probability':thresholds}).sort_values('f1', ascending=False).round(3)\n  \n    figure = plt.figure(figsize = (15, 5))\n\n    plt1 = figure.add_subplot(121)\n    plt1.plot(thresholds, precisions, label='Precision', linewidth=4)\n    plt1.plot(thresholds, recalls, label='Recall', linewidth=4)\n    plt1.plot(thresholds, f1_scores, label='F1', linewidth=4)\n    plt1.set_ylabel('Scores')\n    plt1.set_xlabel('Probability threshold')\n    plt1.set_title('Probabilities threshold calibration')\n    plt1.legend(bbox_to_anchor=(0.25, 0.25))   \n    plt1.table(cellText = scores_table.values,\n               colLabels = scores_table.columns, \n               colLoc = 'center', cellLoc = 'center', loc = 'bottom', bbox = [0, -1.3, 1, 1])\n    plt2 = figure.add_subplot(122)\n    plt2.hist(preds_with_true_labels[preds_with_true_labels[:, 1] == 0][:, 0], \n              label='Another class', color='royalblue', alpha=1)\n    plt2.hist(preds_with_true_labels[preds_with_true_labels[:, 1] == 1][:, 0], \n              label='Main class', color='darkcyan', alpha=0.8)\n    plt2.set_ylabel('Number of examples')\n    plt2.set_xlabel('Probabilities')\n    plt2.set_title('Probability histogram')\n    plt2.legend(bbox_to_anchor=(1, 1))\n\n    plt.show()\n\ndef report(y_train, y_train_pred, y_test, y_test_pred, y_train_proba=None, y_test_proba=None):\n    print('Train\\n', classification_report(y_train, y_train_pred, digits=3))\n    print('Test\\n', classification_report(y_test, y_test_pred, digits=3))\n    if y_train_proba is not None and y_test_proba is not None:\n        roc_train, roc_test = roc_auc_score(y_train, y_train_proba), roc_auc_score(y_test, y_test_proba)\n        print(f'Train ROC_AUC: {roc_train:.3f}, Test ROC_AUC: {roc_test:.3f}')\n    print('Confusion Matrix', '\\n', pd.crosstab(y_test, y_test_pred))\n\ndef reduce_memory(df, verbose=0):\n    if verbose != 0:\n        start_mem = df.memory_usage().sum() / 1024 ** 2\n        print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    for col in df.columns:\n        col_type = df[col].dtype\n        if col_type != object and str(col_type)[:4] != 'uint' and str(col_type) != 'category':\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        elif str(col_type)[:4] != 'uint':\n            df[col] = df[col].astype('category')\n    if verbose != 0:\n        end_mem = df.memory_usage().sum() / 1024 ** 2\n        print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    return df\n\ndef roc_plot(y_true, probs):\n    fpr, tpr, threshold = roc_curve(y_true, probs)\n    roc_auc = auc(fpr, tpr)\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n\ndef vote(predictions: list, weights: list):\n    predictions = np.asarray(predictions).T\n    maj_vote = np.apply_along_axis(lambda x: np.argmax(np.bincount(x, weights=weights)), axis=1, arr=predictions)\n    return maj_vote\n\ndef cross_validation(clf, X, y, scoring='f1'):\n    scores = cross_val_score(estimator=clf, X=X, y=y, cv=10, scoring=scoring, n_jobs=-1)\n    print(f'Меры правильности перекрекстной оценки: {scores}')\n    print(f'Точность перекретсной оценки: {np.mean(scores):.3f} +/- {np.std(scores):.3f}')\n    return scores\n\ndef effect_size(factor_a, factor_b, cohen=True, desired_power=0.8, alpha=0.05):\n    n1, n2 = len(factor_a), len(factor_b)\n    s1, s2 = factor_a.std(ddof=1), factor_b.std(ddof=1)\n    df = (s1 ** 2 / n1 + s2 ** 2 / n2) ** 2 / \\\n          ((s1 ** 2 / n1) ** 2 / (n1 - 1) + (s2 ** 2 / n2) ** 2 / (n2 - 1))\n    if cohen:\n        sigma_pooled = np.sqrt(((n1 - 1) * s1**2 + (n2 - 1) * s2**2) / (n1 + n2 - 2))\n        return np.abs(factor_a.mean() - factor_b.mean()) / sigma_pooled, df\n    else:\n        return power.tt_ind_solve_power(effect_size=None, nobs1=len(factor_a), alpha=alpha, power=desired_power,\n                                        ratio=len(factor_b)/len(factor_a), alternative='two-sided'), df\n    \ndef statistic_output(*columns, df=df, cat=False, target=TARGET_NAME, alpha=0.05, sample_size=0):\n    data = df.copy()\n    data.drop_duplicates(inplace=True)\n    if sample_size == 0:\n        sample_size = int(0.05 * len(data))\n    if not cat:\n        columns = data.drop(target, axis=1).select_dtypes(exclude=['category', 'object']).columns\n        for column in columns:\n            df_sampled = data[[column, target]].sample(sample_size, random_state=1)\n            factor_a = df_sampled.loc[df_sampled[target] == 0][column]   \n            factor_b = df_sampled.loc[df_sampled[target] == 1][column]\n            var_a, var_b = factor_a.var(), factor_b.var()   \n            _, pvalue = stats.shapiro(df_sampled[column])\n            if pvalue >= alpha:\n                _, pvalue = stats.ttest_ind(factor_a, factor_b, equal_var=False)\n                test = power.TTestIndPower()\n                eff_size, deg_free = effect_size(factor_a, factor_b, cohen=False)\n                pow = test.power(effect_size=eff_size, nobs1=len(factor_a), alpha=alpha, df=deg_free, \n                                 ratio=len(factor_b)/len(factor_a), alternative='two-sided')\n            else:\n                _, pvalue = stats.mannwhitneyu(factor_a, factor_b)\n                pow, eff_size = None, None\n            if pvalue < alpha:\n                result = f'with effect_size = {eff_size:.4f} and ttest power {pow*100:.2f}%' if pow is not None else ''\n                print(f'Factor \"{column}\" has statistical impact on target (var_a: {var_a:.2f}, var_b: {var_b:.2f}). {result}')\n            else:\n                print(f'Factor \"{column}\" does not affect target.')\n    else:\n        for column in columns:\n            print(column)\n            categories = data[column].unique().tolist()\n            for pair in combinations(categories, r=2):\n                a, b = pair\n                if a != b:\n                    data_ = data.loc[data[column].isin(pair), ['CLIENTNUM', column, target]].sample(sample_size, random_state=1)\n                    table = data_.pivot_table(values='CLIENTNUM', index=column, columns=target, aggfunc='count')\n                    try:\n                        _, pvalue, _, _ = stats.chi2_contingency(table, correction=False)\n                    except ValueError:\n                        continue\n                    if pvalue >= alpha:\n                        print(f'Categories {a} and {b} can be united. P-value: {pvalue:.6f}')\n                    else:\n                        print(f'Categories {a} and {b} have different frequencies with target.')\n                        \ndef categorical_stats(df=df, target=TARGET_NAME, alpha=0.05, sample_size=500):\n    data = df.copy().sample(sample_size)\n    columns_to_analize = data.select_dtypes(include=['category', 'object']).columns\n    weak_list = []\n    for factor in columns_to_analize:\n        if factor == target:\n            continue\n        print(f'{factor}')\n        table = pd.crosstab(data[factor], data[TARGET_NAME])\n        p_value = stats.chi2_contingency(table, correction=False)[1]\n        if p_value < alpha:\n            print(f'Feature {factor} has statistical impact on target. P-value: {p_value:.6f}')\n        else:\n            weak_list.append(factor)\n    if len(weak_list) > 0:\n        print(f'Statistically weak categorical features: ', *weak_list)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:02:01.237885Z","iopub.execute_input":"2022-03-31T12:02:01.238186Z","iopub.status.idle":"2022-03-31T12:02:01.316956Z","shell.execute_reply.started":"2022-03-31T12:02:01.238157Z","shell.execute_reply":"2022-03-31T12:02:01.315595Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"### Some EDA and data visualisations","metadata":{}},{"cell_type":"code","source":"df.sample(8).transpose()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:55:12.708251Z","iopub.execute_input":"2022-03-31T11:55:12.708676Z","iopub.status.idle":"2022-03-31T11:55:12.743227Z","shell.execute_reply.started":"2022-03-31T11:55:12.708644Z","shell.execute_reply":"2022-03-31T11:55:12.742368Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# remove two last columns as the author suggests\ndf = df.iloc[:, :-2]\nprint(f'Total columns {len(df.columns)}')\n\n# check for duplicates\nprint(f'Number of duplicates: {df.duplicated().astype(int).sum()}')\ndf.drop_duplicates(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:58:41.325238Z","iopub.execute_input":"2022-03-31T11:58:41.325545Z","iopub.status.idle":"2022-03-31T11:58:41.356717Z","shell.execute_reply.started":"2022-03-31T11:58:41.325512Z","shell.execute_reply":"2022-03-31T11:58:41.355923Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# common characteristics for numerical features: \n# it does not seem, that there any extreme values in data\n# some of the columns may follow normal distribution as their means and medians are close\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:55:31.818780Z","iopub.execute_input":"2022-03-31T11:55:31.819483Z","iopub.status.idle":"2022-03-31T11:55:31.882083Z","shell.execute_reply.started":"2022-03-31T11:55:31.819432Z","shell.execute_reply":"2022-03-31T11:55:31.881106Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# common characteristics for categorical features\n# there are not high cardinality in here, seems it would be fine to use one-hot encoding\ndf.describe(include=['object'])","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:55:38.504201Z","iopub.execute_input":"2022-03-31T11:55:38.504651Z","iopub.status.idle":"2022-03-31T11:55:38.544242Z","shell.execute_reply.started":"2022-03-31T11:55:38.504620Z","shell.execute_reply":"2022-03-31T11:55:38.543420Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# overall statistics\n# good news: there are no missing values in dataset (missingno visualisation is not needed)\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:59:18.402276Z","iopub.execute_input":"2022-03-31T11:59:18.403120Z","iopub.status.idle":"2022-03-31T11:59:18.420653Z","shell.execute_reply.started":"2022-03-31T11:59:18.403064Z","shell.execute_reply":"2022-03-31T11:59:18.419487Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# we can observe class imbalance in data\n# next we will try three ways to handle it after predicting on raw data with base model\n# 1) SMOTEENN - that generates new sinthetical samples and discards samples that are close\n# to the decision boundary\n# 2) Use in-built class_weight attributes in models (as alt. sklearn provides a method:\n# sklearn.utils.class_weight.compute_class_weight) to penalize the loss function\n# 3) Hybrid of sampling (SMOTE) and class_weight methods\ndf[TARGET_NAME].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:55:45.158862Z","iopub.execute_input":"2022-03-31T11:55:45.159522Z","iopub.status.idle":"2022-03-31T11:55:45.170949Z","shell.execute_reply.started":"2022-03-31T11:55:45.159482Z","shell.execute_reply":"2022-03-31T11:55:45.170052Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# let's reduce memory usage to speed up and optimize our calculations\ndf = reduce_memory(df, verbose=123)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:57:36.030703Z","iopub.execute_input":"2022-03-31T11:57:36.030987Z","iopub.status.idle":"2022-03-31T11:57:36.063683Z","shell.execute_reply.started":"2022-03-31T11:57:36.030955Z","shell.execute_reply":"2022-03-31T11:57:36.062793Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"#### Some useful data visualizations","metadata":{}},{"cell_type":"code","source":"numerical_cols = df.select_dtypes(include=[np.int8, np.int16, np.float32]).columns.tolist()\n\nplt.figure(figsize=(22, 20))\nfor idx, column in enumerate(numerical_cols):\n    plt.subplot(4, 4, idx + 1)\n    dist = 'Normal Distribution' if stats.shapiro(df[column].sample(1000))[1] > 0.05 else 'Not normal distribution'\n    plt.title(f'{column}: {dist}')\n    sns.histplot(data=df, x=column, hue=TARGET_NAME, bins=60, kde=True)\nplt.subplots_adjust(hspace=0.4, wspace=0.4)\nplt.show()\n# Age is close to normal distribution\n# We can clearly see that numerical data has different distribution via target\n# also there are strange tail in Credit_Limit and Total_Revolvong_Bal\n# we will inspect them with IQR boxplots\n# months_on_book have large amount of median values\n# also we have to admit that there are some count features, which sounds good for \n# boosting models","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:55:50.746065Z","iopub.execute_input":"2022-03-31T11:55:50.746373Z","iopub.status.idle":"2022-03-31T11:55:59.192515Z","shell.execute_reply.started":"2022-03-31T11:55:50.746338Z","shell.execute_reply":"2022-03-31T11:55:59.191474Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"categorical_cols = df.drop(TARGET_NAME, axis=1).select_dtypes(include=['category']).columns\n\nplt.figure(figsize=(15, 10))\nfor idx, column in enumerate(categorical_cols, 1):\n    plt.subplot(2, 3, idx)\n    plt.title(f'{column}')\n    sns.countplot(x=column, hue=TARGET_NAME, data=df)\nplt.subplots_adjust(hspace=0.4, wspace=0.4)\nplt.show()\n# for the first look there are no any significant insites\n# for card_category it is better to apply label smoothing if not using one-hot encoding","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:56:03.837210Z","iopub.execute_input":"2022-03-31T11:56:03.837520Z","iopub.status.idle":"2022-03-31T11:56:04.925665Z","shell.execute_reply.started":"2022-03-31T11:56:03.837489Z","shell.execute_reply":"2022-03-31T11:56:04.924800Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# we do not see any extreme outliers here\n# Inspect: every column, except Customer_Age, Total_Relationship_Count,Total_Revolving_Bal\n# we will try next after predicting on raw data:\n# 1) IQR adjusting\n# 2) PowerTransformer to make the data more normal and less skewed\n# 3) IterativeImputer to impute possible outliers, detected by IQR\n# NOTE: also good techniques for outlier detection are IsolationForest, DBSCAN, \n# dimensionality reduction methods\n\nplt.figure(figsize=(18,20))\nfor idx, column in enumerate(numerical_cols, 1):\n    plt.subplot(4, 4, idx)\n    sns.boxplot(y=df[column], x=df[TARGET_NAME], data=df)\n    plt.title(f'{column}')\nplt.subplots_adjust(hspace=0.5, wspace=0.5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:56:08.672843Z","iopub.execute_input":"2022-03-31T11:56:08.673117Z","iopub.status.idle":"2022-03-31T11:56:10.899365Z","shell.execute_reply.started":"2022-03-31T11:56:08.673090Z","shell.execute_reply":"2022-03-31T11:56:10.898377Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# let's see what linear correlations between features do we have\n# we have significant correlation between Customer_Age and Months_on_book,\n# Total_Trans_Amt and Total_Trans_Ct\n# meddle high correlations between: Total_Revolving_Bal and Avg_Utilization_Ratio\n# Avg_Open_To_Buy and Avg_Utilization_Ratio\n# looks like the author created a few variables using groupby and count, averaging \n# methods\n# As the correlations are high it might be useful to drop one of the dependent feature\n# but we would rather use feature-selection methods to decide whether to drop some, \n# as otherwise we can loose meaningful information\nplt.figure(figsize = (14,12))\ncorr_matrix = df.corr()\ncorr_matrix = np.round(corr_matrix, 2)\ncorr_matrix[np.abs(corr_matrix) < 0.3] = 0\nsns.heatmap(corr_matrix, annot=True, linewidths=.5, cmap='coolwarm')\nplt.title('Correlation matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:56:15.873634Z","iopub.execute_input":"2022-03-31T11:56:15.873911Z","iopub.status.idle":"2022-03-31T11:56:16.982241Z","shell.execute_reply.started":"2022-03-31T11:56:15.873883Z","shell.execute_reply":"2022-03-31T11:56:16.981292Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"#### Let's compute some statistical tests to find out the significance of given variables","metadata":{}},{"cell_type":"code","source":"df[TARGET_NAME] = df[TARGET_NAME].map({'Existing Customer': 0, 'Attrited Customer': 1})\ndf[TARGET_NAME] = df[TARGET_NAME].astype(np.uint8)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:57:47.923256Z","iopub.execute_input":"2022-03-31T11:57:47.923820Z","iopub.status.idle":"2022-03-31T11:57:47.931749Z","shell.execute_reply.started":"2022-03-31T11:57:47.923786Z","shell.execute_reply":"2022-03-31T11:57:47.931043Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# first we will check the histplot results, assuming all numerical features are useful\n# we got some interesting insites! For a given significance level (0.05) and sample_size\n# (5%) we can conclude that Customer_Age, Dependent_count, Months_on_book, Avg_Open_To_Buy\n# have no statisticaly meaningful impact on target\n# again: we will note this fact in head and later apply feature-selection to decide\nstatistic_output(*numerical_cols)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:02:05.873539Z","iopub.execute_input":"2022-03-31T12:02:05.874011Z","iopub.status.idle":"2022-03-31T12:02:05.960718Z","shell.execute_reply.started":"2022-03-31T12:02:05.873960Z","shell.execute_reply":"2022-03-31T12:02:05.959799Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# we use here only gender, as it is not enough category samples for such type of test\n# But again we see that Gender has no impact on target as there are two equally \n# distributed categories\nstatistic_output('Gender', cat=True, sample_size=500)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:57:59.165488Z","iopub.execute_input":"2022-03-31T11:57:59.165765Z","iopub.status.idle":"2022-03-31T11:57:59.209644Z","shell.execute_reply.started":"2022-03-31T11:57:59.165739Z","shell.execute_reply":"2022-03-31T11:57:59.208777Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# with statistic output we decide whether it is possible to union two taken categories\n# here we determin if there are statistical dependency between two \n# variables (categorical feature and target)\n# we got ALL statistically weak features. In the end, we cannot drop them all\n# we have to make feature-selection\ncategorical_stats()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:58:03.945261Z","iopub.execute_input":"2022-03-31T11:58:03.945924Z","iopub.status.idle":"2022-03-31T11:58:04.008612Z","shell.execute_reply.started":"2022-03-31T11:58:03.945858Z","shell.execute_reply":"2022-03-31T11:58:04.007587Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"#### Complete Raw dataset and define base model","metadata":{}},{"cell_type":"code","source":"# set id as index as we won't use it in predictions, but also we want to \n# identify every customer later\ndf.set_index('CLIENTNUM', drop=True, inplace=True)\ndf.head(1)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:02:11.120361Z","iopub.execute_input":"2022-03-31T12:02:11.120661Z","iopub.status.idle":"2022-03-31T12:02:11.143157Z","shell.execute_reply.started":"2022-03-31T12:02:11.120628Z","shell.execute_reply":"2022-03-31T12:02:11.142491Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# just mapping binary caterories and changing the correct dtype\ndf['Gender'] = df['Gender'].map({'M': 0, 'F': 1})\ndf['Gender'] = df['Gender'].astype(np.uint8)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:02:14.048184Z","iopub.execute_input":"2022-03-31T12:02:14.048846Z","iopub.status.idle":"2022-03-31T12:02:14.056127Z","shell.execute_reply.started":"2022-03-31T12:02:14.048808Z","shell.execute_reply":"2022-03-31T12:02:14.055317Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# simply one-hot encode categorical features\ndf = pd.get_dummies(df, prefix=['col1', 'col2', 'col3', 'col4'])\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:02:16.383242Z","iopub.execute_input":"2022-03-31T12:02:16.383884Z","iopub.status.idle":"2022-03-31T12:02:16.399634Z","shell.execute_reply.started":"2022-03-31T12:02:16.383850Z","shell.execute_reply":"2022-03-31T12:02:16.398860Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# getting all model parameters as default\nbase = LGBMClassifier(verbose=-1)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:02:18.930146Z","iopub.execute_input":"2022-03-31T12:02:18.930540Z","iopub.status.idle":"2022-03-31T12:02:18.934241Z","shell.execute_reply.started":"2022-03-31T12:02:18.930498Z","shell.execute_reply":"2022-03-31T12:02:18.933621Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"#### Search of the best split","metadata":{}},{"cell_type":"code","source":"X, y = df.drop(TARGET_NAME, axis=1), df[TARGET_NAME]\n\ntrain_sizes, train_scores, test_scores = learning_curve(estimator=base, X=X, y=y, \n                                                        train_sizes=np.linspace(0.1, 1.0, 10), cv=10, scoring='roc_auc', n_jobs=-1)\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nplt.figure(figsize=(10, 8))\nplt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='правильность при обучении')\nplt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\nplt.plot(train_sizes, test_mean, color='green', linestyle='--', marker='s', markersize=5, \n         label='правильность при проверке')\nplt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green')\nplt.axvline(X.shape[0]*0.7, color='orange', linestyle='-.', label='test_size=0.3')\nplt.grid()\nplt.xlabel('Количество обучающих образцов')\nplt.ylabel('Правильность')\nplt.legend(loc='best')\nplt.show()\n# setting test_size=0.3 looks fine, but in case of validation data we will set\n# test_size=0.25, valid_size=0.15","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:02:20.998919Z","iopub.execute_input":"2022-03-31T12:02:20.999629Z","iopub.status.idle":"2022-03-31T12:02:35.287931Z","shell.execute_reply.started":"2022-03-31T12:02:20.999582Z","shell.execute_reply":"2022-03-31T12:02:35.287228Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=y, random_state=123)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, shuffle=True, stratify=y_train, random_state=123)\nprint(f'Train size: {X_train.shape[0]}, Validation size: {X_valid.shape[0]}, Test size: {X_test.shape[0]}')","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:08:41.494199Z","iopub.execute_input":"2022-03-31T12:08:41.494582Z","iopub.status.idle":"2022-03-31T12:08:41.524809Z","shell.execute_reply.started":"2022-03-31T12:08:41.494544Z","shell.execute_reply":"2022-03-31T12:08:41.523840Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"#### Dealing with outliers","metadata":{}},{"cell_type":"code","source":"# raw predictions\nbase.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False, early_stopping_rounds=5, eval_metric='roc_auc')\nbase_raw_train = base.predict(X_train)\nbase_raw_test = base.predict(X_test)\nbase_train_proba = base.predict_proba(X_train)[:,1]\nbase_test_proba = base.predict_proba(X_test)[:,1]\n\nreport(y_train, base_raw_train, y_test, base_raw_test, base_train_proba, base_test_proba)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:02:56.472281Z","iopub.execute_input":"2022-03-31T12:02:56.472703Z","iopub.status.idle":"2022-03-31T12:02:56.770206Z","shell.execute_reply.started":"2022-03-31T12:02:56.472674Z","shell.execute_reply":"2022-03-31T12:02:56.769163Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"columns_with_outliers = ['Dependent_count','Months_on_book', 'Months_Inactive_12_mon',\n                         'Contacts_Count_12_mon', 'Credit_Limit', 'Avg_Open_To_Buy',\n                         'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt', 'Total_Trans_Ct',\n                         'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio']","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:02:59.922135Z","iopub.execute_input":"2022-03-31T12:02:59.922860Z","iopub.status.idle":"2022-03-31T12:02:59.928204Z","shell.execute_reply.started":"2022-03-31T12:02:59.922820Z","shell.execute_reply":"2022-03-31T12:02:59.927069Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# IQR adjusting\nfor column in columns_with_outliers:\n    q1 = np.quantile(X_train[column], 0.25)\n    q3 = np.quantile(X_train[column], 0.75)\n    iq_low = q1 - 1.5 * (q3 - q1)\n    iq_high = q3 + 1.5 * (q3 - q1)\n    X_train.loc[X_train[column] > iq_high, column] = iq_high\n    X_train.loc[X_train[column] < iq_low, column] = iq_low\n    \n    X_valid.loc[X_valid[column] > iq_high, column] = iq_high\n    X_valid.loc[X_valid[column] < iq_low, column] = iq_low\n    \n    X_test.loc[X_test[column] > iq_high, column] = iq_high\n    X_test.loc[X_test[column] < iq_low, column] = iq_low","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:03:01.904609Z","iopub.execute_input":"2022-03-31T12:03:01.905404Z","iopub.status.idle":"2022-03-31T12:03:01.962857Z","shell.execute_reply.started":"2022-03-31T12:03:01.905343Z","shell.execute_reply":"2022-03-31T12:03:01.961576Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"base.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False, early_stopping_rounds=5, eval_metric='roc_auc')\nbase_iqr_train = base.predict(X_train)\nbase_iqr_test = base.predict(X_test)\nbase_train_proba = base.predict_proba(X_train)[:,1]\nbase_test_proba = base.predict_proba(X_test)[:,1]\n\nreport(y_train, base_iqr_train, y_test, base_iqr_test, base_train_proba, base_test_proba)\n# actually we cannot see any meaningful difference. Thus FN is higher than in raw-case\n# We assume that it is better to leave less FN","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:03:04.722084Z","iopub.execute_input":"2022-03-31T12:03:04.722417Z","iopub.status.idle":"2022-03-31T12:03:05.016769Z","shell.execute_reply.started":"2022-03-31T12:03:04.722373Z","shell.execute_reply":"2022-03-31T12:03:05.015765Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# Transformation to normal distribution\ncolumn_transformer = ColumnTransformer(transformers=[\n    ('power', PowerTransformer(), columns_with_outliers)\n], remainder='passthrough')\n\nX_train = column_transformer.fit_transform(X_train, y_train)\nX_valid = column_transformer.transform(X_valid)\nX_test = column_transformer.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:03:16.816591Z","iopub.execute_input":"2022-03-31T12:03:16.816884Z","iopub.status.idle":"2022-03-31T12:03:17.000140Z","shell.execute_reply.started":"2022-03-31T12:03:16.816857Z","shell.execute_reply":"2022-03-31T12:03:16.999147Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"base.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False, early_stopping_rounds=5, eval_metric='roc_auc')\nbase_pow_train = base.predict(X_train)\nbase_pow_test = base.predict(X_test)\nbase_train_proba = base.predict_proba(X_train)[:,1]\nbase_test_proba = base.predict_proba(X_test)[:,1]\n\nreport(y_train, base_pow_train, y_test, base_pow_test, base_train_proba, base_test_proba)\n# as excpected normal-transformation method does not fit this particular situation","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:03:19.482547Z","iopub.execute_input":"2022-03-31T12:03:19.482816Z","iopub.status.idle":"2022-03-31T12:03:19.687839Z","shell.execute_reply.started":"2022-03-31T12:03:19.482788Z","shell.execute_reply":"2022-03-31T12:03:19.686719Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# Imputation\nfor column in columns_with_outliers:\n    q1 = np.quantile(X_train[column], 0.25)\n    q3 = np.quantile(X_train[column], 0.75)\n    iq_low = q1 - 1.5 * (q3 - q1)\n    iq_high = q3 + 1.5 * (q3 - q1)\n    X_train.loc[X_train[column] > iq_high, column] = None\n    X_train.loc[X_train[column] < iq_low, column] = None\n    \n    X_valid.loc[X_valid[column] > iq_high, column] = None\n    X_valid.loc[X_valid[column] < iq_low, column] = None\n    \n    X_test.loc[X_test[column] > iq_high, column] = None\n    X_test.loc[X_test[column] < iq_low, column] = None","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:03:31.313587Z","iopub.execute_input":"2022-03-31T12:03:31.313889Z","iopub.status.idle":"2022-03-31T12:03:31.367387Z","shell.execute_reply.started":"2022-03-31T12:03:31.313854Z","shell.execute_reply":"2022-03-31T12:03:31.366622Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nimputer = IterativeImputer(estimator=LinearRegression(), random_state=123)\nX_train = imputer.fit_transform(X_train)\nX_valid = imputer.transform(X_valid)\nX_test = imputer.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:03:33.705340Z","iopub.execute_input":"2022-03-31T12:03:33.705753Z","iopub.status.idle":"2022-03-31T12:03:37.654932Z","shell.execute_reply.started":"2022-03-31T12:03:33.705723Z","shell.execute_reply":"2022-03-31T12:03:37.654252Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"base.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False, early_stopping_rounds=5, eval_metric='roc_auc')\nbase_imp_train = base.predict(X_train)\nbase_imp_test = base.predict(X_test)\nbase_train_proba = base.predict_proba(X_train)[:,1]\nbase_test_proba = base.predict_proba(X_test)[:,1]\n\nreport(y_train, base_imp_train, y_test, base_imp_test, base_train_proba, base_test_proba)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:03:39.171148Z","iopub.execute_input":"2022-03-31T12:03:39.171950Z","iopub.status.idle":"2022-03-31T12:03:39.424878Z","shell.execute_reply.started":"2022-03-31T12:03:39.171888Z","shell.execute_reply":"2022-03-31T12:03:39.424018Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"##### Also we may just drop outliers, but as if they are not so extreme, we may loose important information","metadata":{}},{"cell_type":"markdown","source":"### Feature Engeneering\n#### We are using no more features, as there are a lot of counts and some kind of synthetic features\nGrouping by categories with averaging functions and counting is good practice for boosting models.\nLinear transform is good for Linear models: LogisticRegression, LinearSVM, etc.","metadata":{}},{"cell_type":"markdown","source":"#### Feature Selection","metadata":{}},{"cell_type":"markdown","source":"Feature Selection will have two different methods after predicting on raw data:\n1) A class that selects the best subset of features, according to the given score, \nstraightforward method, only computationaly reasonable for not large datasets\n\n2) BorutaShap object, that will select features to remove: uses statistical tests and\nshap values with given model (estimator)\nwe will compare the EDA results with this approach","metadata":{}},{"cell_type":"code","source":"class BestSet(BaseEstimator, TransformerMixin):\n    def __init__(self, k_features=12, scoring=f1_score, test_size=0.2):\n        self.scoring = scoring\n        self.k_features = k_features\n        self.test_size = test_size\n\n    def fit(self, X, y):\n        X_train, X_test, y_train, y_test = train_test_split(X,\n                                                            y, \n                                                            test_size=self.test_size, \n                                                            stratify=y,\n                                                            random_state=1)\n        dim = X_train.shape[1]\n        self.indices_ = tuple(range(dim))\n        self.subsets_ = [self.indices_]\n        score = self._calc_score(X_train, y_train, X_test, y_test, self.indices_)\n        self.scores_ = [score]\n\n        while dim > self.k_features:\n            scores, subsets = [], []\n            for p in combinations(self.indices_, r=dim-1):\n                score = self._calc_score(X_train, y_train, X_test, y_test, p)\n                scores.append(score)\n                subsets.append(p)\n            best = np.argmax(scores)\n            self.indices_ = subsets[best]\n            self.subsets_.append(self.indices_)\n            dim -= 1\n            self.scores_.append(scores[best])\n        self.k_score_ = self.scores_[-1]\n        return self\n    def transform(self, X):\n        best_indices = self.subsets_[np.argmax(self.scores_)]\n        return X[:, best_indices]\n\n    def _calc_score(self, X_train, y_train, X_test, y_test, indices):\n        model = LGBMClassifier(boosting_type='goss',\n                               is_unbalance=True,\n                               max_depth=3,\n                               verbose=-1,\n                               random_state=1)\n        model.fit(X_train[:, indices], y_train, verbose=False)\n        y_pred = model.predict(X_test[:, indices])\n        score = self.scoring(y_test, y_pred)\n        return score","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:03:51.998834Z","iopub.execute_input":"2022-03-31T12:03:51.999309Z","iopub.status.idle":"2022-03-31T12:03:52.014657Z","shell.execute_reply.started":"2022-03-31T12:03:51.999262Z","shell.execute_reply":"2022-03-31T12:03:52.013686Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"pipe = Pipeline(steps=[\n    ('selector', BestSet(k_features=30, scoring=roc_auc_score)),\n    ('base_model', LGBMClassifier(verbose=-1))\n])\nfit_params = {'base_model__verbose': False}\n\npipe.fit(X_train.values, y_train, **fit_params)\npipe_train = pipe.predict(X_train.values)\npipe_test = pipe.predict(X_test.values)\npipe_train_proba = pipe.predict_proba(X_train.values)[:,1]\npipe_test_proba = pipe.predict_proba(X_test.values)[:,1]\n\nreport(y_train, pipe_train, y_test, pipe_test, pipe_train_proba, pipe_test_proba)\n# we got a tiny improve in FN","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:04:07.974307Z","iopub.execute_input":"2022-03-31T12:04:07.974634Z","iopub.status.idle":"2022-03-31T12:04:26.300081Z","shell.execute_reply.started":"2022-03-31T12:04:07.974605Z","shell.execute_reply":"2022-03-31T12:04:26.299213Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"indices = pipe[0].subsets_[np.argmax(pipe[0].scores_)]\nprint(f'Remain columns: {len(indices)}')\n# dropped 4 features\nX.iloc[0, list(indices)]\n# it is curious as Gender feature was not dropped","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:04:30.226229Z","iopub.execute_input":"2022-03-31T12:04:30.226512Z","iopub.status.idle":"2022-03-31T12:04:30.239543Z","shell.execute_reply.started":"2022-03-31T12:04:30.226484Z","shell.execute_reply":"2022-03-31T12:04:30.238588Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"# borutaShap\nselector = BorutaShap(model=LGBMClassifier(verbose=-1), importance_measure='shap', classification=True)\nselector.fit(X_train, y_train, n_trials=50, sample=False, verbose=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:05:06.055201Z","iopub.execute_input":"2022-03-31T12:05:06.055496Z","iopub.status.idle":"2022-03-31T12:07:54.147229Z","shell.execute_reply.started":"2022-03-31T12:05:06.055468Z","shell.execute_reply":"2022-03-31T12:07:54.146383Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"selector.features_to_remove","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:07:54.148942Z","iopub.execute_input":"2022-03-31T12:07:54.149198Z","iopub.status.idle":"2022-03-31T12:07:54.155298Z","shell.execute_reply.started":"2022-03-31T12:07:54.149168Z","shell.execute_reply":"2022-03-31T12:07:54.154460Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"X_train.drop(selector.features_to_remove.tolist(), axis=1, inplace=True)\nX_valid.drop(selector.features_to_remove.tolist(), axis=1, inplace=True)\nX_test.drop(selector.features_to_remove.tolist(), axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:08:27.194423Z","iopub.execute_input":"2022-03-31T12:08:27.194861Z","iopub.status.idle":"2022-03-31T12:08:27.202340Z","shell.execute_reply.started":"2022-03-31T12:08:27.194813Z","shell.execute_reply":"2022-03-31T12:08:27.201745Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"base.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False, early_stopping_rounds=5, eval_metric='roc_auc')\nbase_bor_train = base.predict(X_train)\nbase_bor_test = base.predict(X_test)\nbase_train_proba = base.predict_proba(X_train)[:,1]\nbase_test_proba = base.predict_proba(X_test)[:,1]\n\nreport(y_train, base_bor_train, y_test, base_bor_test, base_train_proba, base_test_proba)\n# after dropping we got almost the same confusion matrix but model overfits more\n# if it was large dataset it would be the best choice in computational order and in order\n# to reduce the job for scrapping the data.\n# So we will use BestSet with lgbm and raw data with CatBoostClassifier","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:08:30.624259Z","iopub.execute_input":"2022-03-31T12:08:30.624900Z","iopub.status.idle":"2022-03-31T12:08:30.822672Z","shell.execute_reply.started":"2022-03-31T12:08:30.624854Z","shell.execute_reply":"2022-03-31T12:08:30.821754Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"#### SMOTE and class weighting","metadata":{}},{"cell_type":"code","source":"# class weights balancing\npipe = Pipeline(steps=[\n    ('selector', BestSet(k_features=30, scoring=roc_auc_score)),\n    ('base_model', LGBMClassifier(is_unbalance=True, verbose=-1))\n])\nfit_params = {'base_model__verbose': False}\n\npipe.fit(X_train.values, y_train, **fit_params)\npipe_train = pipe.predict(X_train.values)\npipe_test = pipe.predict(X_test.values)\npipe_train_proba = pipe.predict_proba(X_train.values)[:,1]\npipe_test_proba = pipe.predict_proba(X_test.values)[:,1]\n\nreport(y_train, pipe_train, y_test, pipe_test, pipe_train_proba, pipe_test_proba)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:08:47.677002Z","iopub.execute_input":"2022-03-31T12:08:47.678017Z","iopub.status.idle":"2022-03-31T12:09:05.201190Z","shell.execute_reply.started":"2022-03-31T12:08:47.677964Z","shell.execute_reply":"2022-03-31T12:09:05.200240Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"# single SMOTE\npipe = Pipe(steps=[\n    ('selector', BestSet(k_features=30, scoring=roc_auc_score)),\n    ('SMOTE', SMOTEENN(sampling_strategy=1., random_state=123)),\n    ('base_model', LGBMClassifier(verbose=-1))\n])\nfit_params = {'base_model__verbose': False}\n\npipe.fit(X_train.values, y_train, **fit_params)\npipe_train = pipe.predict(X_train.values)\npipe_test = pipe.predict(X_test.values)\npipe_train_proba = pipe.predict_proba(X_train.values)[:,1]\npipe_test_proba = pipe.predict_proba(X_test.values)[:,1]\n\nreport(y_train, pipe_train, y_test, pipe_test, pipe_train_proba, pipe_test_proba)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:09:10.013108Z","iopub.execute_input":"2022-03-31T12:09:10.013852Z","iopub.status.idle":"2022-03-31T12:09:32.839942Z","shell.execute_reply.started":"2022-03-31T12:09:10.013808Z","shell.execute_reply":"2022-03-31T12:09:32.839029Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"# SMOTE and class_weights\npipe = Pipe(steps=[\n    ('selector', BestSet(k_features=30, scoring=roc_auc_score)),\n    ('SMOTE', SMOTEENN(sampling_strategy=0.7, random_state=123)),\n    ('base_model', LGBMClassifier(is_unbalance=True, verbose=-1))\n])\nfit_params = {'base_model__verbose': False}\n\npipe.fit(X_train.values, y_train, **fit_params)\npipe_train = pipe.predict(X_train.values)\npipe_test = pipe.predict(X_test.values)\npipe_train_proba = pipe.predict_proba(X_train.values)[:,1]\npipe_test_proba = pipe.predict_proba(X_test.values)[:,1]\n\nreport(y_train, pipe_train, y_test, pipe_test, pipe_train_proba, pipe_test_proba)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:09:32.841761Z","iopub.execute_input":"2022-03-31T12:09:32.842088Z","iopub.status.idle":"2022-03-31T12:09:58.138491Z","shell.execute_reply.started":"2022-03-31T12:09:32.842045Z","shell.execute_reply":"2022-03-31T12:09:58.137316Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"#### We will use only class_weights","metadata":{}},{"cell_type":"markdown","source":"### LGBM Hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"# in order not to select every trial\nX_train = pipe[0].fit_transform(X_train.values, y_train)\nX_valid = pipe[0].transform(X_valid.values)\nX_test = pipe[0].transform(X_test.values)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:10:55.034627Z","iopub.execute_input":"2022-03-31T12:10:55.036038Z","iopub.status.idle":"2022-03-31T12:11:12.316235Z","shell.execute_reply.started":"2022-03-31T12:10:55.035933Z","shell.execute_reply":"2022-03-31T12:11:12.315362Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"model_params = {\n                'objective': 'binary', # cross_entropy\n                'n_estimators': 800,\n                'n_jobs': -1,\n                'is_unbalance': True,\n                'random_state': 123\n}\nfit_params = {'early_stopping_rounds': 10,  \n              'eval_set': [(X_valid, y_valid)],  \n              'eval_metric': 'auc',\n              'verbose': False\n}","metadata":{"execution":{"iopub.status.busy":"2022-03-26T11:57:05.304696Z","iopub.execute_input":"2022-03-26T11:57:05.305057Z","iopub.status.idle":"2022-03-26T11:57:05.311165Z","shell.execute_reply.started":"2022-03-26T11:57:05.305025Z","shell.execute_reply":"2022-03-26T11:57:05.310152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n    param_trials = {\n                    'max_depth': trial.suggest_int('max_depth', 3, 9),\n                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5),\n                    'reg_alpha': trial.suggest_float('reg_alpha', 1., 5.),\n                    'reg_lambda': trial.suggest_float('reg_lambda', 1., 5.),\n                    'num_leaves': trial.suggest_int('num_leaves', 20, 265),\n                    'subsample': trial.suggest_float('subsample', 0.3, 1.),\n                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 1.),\n                    'max_bin': trial.suggest_int('max_bin', 60, 260),\n                    'min_child_samples': trial.suggest_int('min_child_samples', 120, 260),\n                    'min_child_weight': trial.suggest_float('min_child_weight', 0.001, 0.1),\n                    'boosting_type': trial.suggest_categorical('boosting_type', ['goss', 'gbdt']),\n                    }\n    param_trials.update(model_params)\n    opt_model = LGBMClassifier(**param_trials)\n    opt_model.fit(X_train, y_train, **fit_params)\n    \n    y_pred = opt_model.predict_proba(X_valid)[:,1]\n    score = roc_auc_score(y_valid, y_pred)\n    \n    return score","metadata":{"execution":{"iopub.status.busy":"2022-03-26T11:57:11.884607Z","iopub.execute_input":"2022-03-26T11:57:11.884931Z","iopub.status.idle":"2022-03-26T11:57:11.894406Z","shell.execute_reply.started":"2022-03-26T11:57:11.884898Z","shell.execute_reply":"2022-03-26T11:57:11.893738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.logging.set_verbosity(optuna.logging.FATAL)\nstudy = optuna.create_study(sampler=TPESampler(seed=123), direction=\"maximize\")\nstudy.optimize(objective, n_trials=300, timeout=6000)\n\nprint(f'Number of completed trials: {len(study.trials)}')\nprint('Best trial')\ntrial = study.best_trial\nprint(f'Best score: {trial.value}')\nprint('Best params')\nfor key, value in trial.params.items():\n    print(f'{key}: {value}')","metadata":{"execution":{"iopub.status.busy":"2022-03-26T11:57:31.760626Z","iopub.execute_input":"2022-03-26T11:57:31.761229Z","iopub.status.idle":"2022-03-26T11:58:33.522757Z","shell.execute_reply.started":"2022-03-26T11:57:31.761195Z","shell.execute_reply":"2022-03-26T11:58:33.521717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LGBM stratified predcitions","metadata":{}},{"cell_type":"code","source":"model_params = {\n                'objective': 'binary',\n                'n_estimators': 800,\n                'n_jobs': -1,\n                'is_unbalance': True,\n                'random_state': 123,\n                'max_depth': 8,\n                'learning_rate': 0.06334858903177704, # 0.16334858903177704\n                'reg_alpha': 1.7581994842893593,\n                'reg_lambda': 4.962605889056071,\n                'num_leaves': 144,\n                'subsample': 0.35154539457722855,\n                'colsample_bytree': 0.5436963612580455, # 0.6436963612580455\n                'max_bin': 219,\n                'min_child_samples': 124,\n                'min_child_weight': 0.02483278328435734,\n                'boosting_type': 'gbdt',\n}\nfit_params = {'early_stopping_rounds': 10,  \n              'eval_set': [(X_valid, y_valid)],  \n              'eval_metric': 'auc',\n              'verbose': False\n}","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:11:19.542582Z","iopub.execute_input":"2022-03-31T12:11:19.542882Z","iopub.status.idle":"2022-03-31T12:11:19.550666Z","shell.execute_reply.started":"2022-03-31T12:11:19.542848Z","shell.execute_reply":"2022-03-31T12:11:19.549812Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"# save index\nindices = pipe[0].subsets_[np.argmax(pipe[0].scores_)]\nX_index = X.index\nX_ = X.values[:, indices]\nX_fold, X_test, y_fold, y_test = train_test_split(X_, y, test_size=0.2, shuffle=True, stratify=y, random_state=123)\nprint(f'Train size: {X_fold.shape[0]}, Test size: {X_test.shape[0]}')","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:11:26.652980Z","iopub.execute_input":"2022-03-31T12:11:26.653270Z","iopub.status.idle":"2022-03-31T12:11:26.669546Z","shell.execute_reply.started":"2022-03-31T12:11:26.653237Z","shell.execute_reply":"2022-03-31T12:11:26.668821Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"# get result probabilities on 3 folds\ntest_results, train_roc, test_pred, train_f1, train_acc = [], [], [], [], []\nskf = StratifiedKFold(n_splits=3)\n\nfor train_index, valid_index in skf.split(X_fold, y_fold):\n    X_train, X_valid = X_fold[train_index, :], X_fold[valid_index, :]\n    y_train, y_valid = y_fold.values[train_index], y_fold.values[valid_index]\n\n    lgbm = LGBMClassifier(**model_params)\n    fit_params = {'early_stopping_rounds': 10,  \n              'eval_set': [(X_valid, y_valid)],  \n              'eval_metric': 'auc',\n              'verbose': False\n              }\n    lgbm.fit(X_train, y_train, **fit_params)\n    test_labels = lgbm.predict(X_test)\n    test_pred.append(test_labels)\n    \n    train_proba = lgbm.predict_proba(X_train)[:,1]\n    train_roc.append(roc_auc_score(y_train, train_proba))\n    \n    train_labels = lgbm.predict(X_train)\n    train_f1.append(f1_score(y_train, train_labels))\n    train_acc.append(accuracy_score(y_train, train_labels))\n    \n    pred_test = lgbm.predict_proba(X_test)[:,1]\n    test_results.append(pred_test)\n\nfinal_test = np.array(test_results).mean(axis=0)\ntest_pred = vote(test_pred, weights=[1.,1.,1.])","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:11:31.867587Z","iopub.execute_input":"2022-03-31T12:11:31.868009Z","iopub.status.idle":"2022-03-31T12:11:33.102398Z","shell.execute_reply.started":"2022-03-31T12:11:31.867980Z","shell.execute_reply":"2022-03-31T12:11:33.101626Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"print(f'train mean accuracy: {np.array(train_acc).mean():.4f}, test accuracy: {accuracy_score(y_test, test_pred):.4f}')\nprint(f'train mean f1: {np.array(train_f1).mean():.4f}, test accuracy: {f1_score(y_test, test_pred):.4f}')\nprint(f'train mean auc: {np.array(train_roc).mean():.4f}, test auc: {roc_auc_score(y_test, final_test):.4f}')\n# the model overfits a bit, but not critically","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:11:36.639172Z","iopub.execute_input":"2022-03-31T12:11:36.639861Z","iopub.status.idle":"2022-03-31T12:11:36.651132Z","shell.execute_reply.started":"2022-03-31T12:11:36.639823Z","shell.execute_reply":"2022-03-31T12:11:36.650191Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"# let's see if calibration is needed\n# to get higher f1, recall and precision we have to \nshow_proba_calibration_plots(final_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:11:46.831611Z","iopub.execute_input":"2022-03-31T12:11:46.831897Z","iopub.status.idle":"2022-03-31T12:11:47.848247Z","shell.execute_reply.started":"2022-03-31T12:11:46.831869Z","shell.execute_reply":"2022-03-31T12:11:47.847232Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"### CatBoost Model\nHere we take all the same preprocessing steps except one-hot encoding","metadata":{}},{"cell_type":"code","source":"# let's repeat all necessary steps\ndf = pd.read_csv(data_root) # read\ndf = df.iloc[:, :-2] # remove redundant columns\ndf.drop_duplicates(inplace=True) # drop duplicates\ndf = reduce_memory(df, verbose=0) # reduce memory\ndf[TARGET_NAME] = df[TARGET_NAME].map({'Existing Customer': 0, 'Attrited Customer': 1}) # map target\ndf[TARGET_NAME] = df[TARGET_NAME].astype(np.uint8)\ndf.set_index('CLIENTNUM', drop=True, inplace=True) # set client id as index\n# split\nX, y = df.drop(TARGET_NAME, axis=1), df[TARGET_NAME]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=y, random_state=123)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, shuffle=True, stratify=y_train, random_state=123)\nprint(f'Train size: {X_train.shape[0]}, Validation size: {X_valid.shape[0]}, Test size: {X_test.shape[0]}')","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:13:49.155745Z","iopub.execute_input":"2022-03-31T12:13:49.156045Z","iopub.status.idle":"2022-03-31T12:13:49.261186Z","shell.execute_reply.started":"2022-03-31T12:13:49.156012Z","shell.execute_reply":"2022-03-31T12:13:49.260252Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"# categorical columns; we are using Pool as a good practice, it is not necessary here as the dataset is small\ncat_cols = X_train.select_dtypes(include=['category']).columns.tolist()\n\ntrain_pool = Pool(X_train, y_train, cat_features=cat_cols)\nvalid_pool = Pool(X_valid, y_valid, cat_features=cat_cols)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:12:05.617311Z","iopub.execute_input":"2022-03-31T12:12:05.617621Z","iopub.status.idle":"2022-03-31T12:12:05.659878Z","shell.execute_reply.started":"2022-03-31T12:12:05.617592Z","shell.execute_reply":"2022-03-31T12:12:05.659244Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"params_model = {\n    'eval_metric': 'AUC', \n    'iterations': 1000,\n    'auto_class_weights': 'Balanced',\n    'silent': True,\n    'one_hot_max_size': 15,\n    'early_stopping_rounds': 10,\n    'grow_policy': 'SymmetricTree',\n    'allow_writing_files': False,\n    'use_best_model': True,\n    'random_seed': 123,\n    }","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:10:28.824476Z","iopub.execute_input":"2022-03-26T13:10:28.824809Z","iopub.status.idle":"2022-03-26T13:10:28.830598Z","shell.execute_reply.started":"2022-03-26T13:10:28.824773Z","shell.execute_reply":"2022-03-26T13:10:28.829493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n    param_trials = {\n                    'depth': trial.suggest_int('depth', 3, 9),\n                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5),\n                    'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1., 5.),\n                    'bagging_temperature': trial.suggest_float('bagging_temperature', 1., 3.),\n                    'subsample': trial.suggest_float('subsample', 0.3, 1.),\n                    'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.3, 1.),\n                    'boosting_type': trial.suggest_categorical('boosting_type', ['Ordered', 'Plain']),\n                    }\n    param_trials.update(params_model)\n    opt_model = CatBoostClassifier(**param_trials)\n    opt_model.fit(train_pool, eval_set=valid_pool)\n    \n    y_pred = opt_model.predict_proba(X_valid)[:,1]\n    score = roc_auc_score(y_valid, y_pred)\n    \n    return score","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:11:00.000737Z","iopub.execute_input":"2022-03-26T13:11:00.001662Z","iopub.status.idle":"2022-03-26T13:11:00.010305Z","shell.execute_reply.started":"2022-03-26T13:11:00.001597Z","shell.execute_reply":"2022-03-26T13:11:00.009302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study = optuna.create_study(sampler=TPESampler(seed=123), direction=\"maximize\")\nstudy.optimize(objective, n_trials=300, timeout=6000)\n\nprint(f'Number of completed trials: {len(study.trials)}')\nprint('Best trial')\ntrial = study.best_trial\nprint(f'Best score: {trial.value}')\nprint('Best params')\nfor key, value in trial.params.items():\n    print(f'{key}: {value}')","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:13:49.256446Z","iopub.execute_input":"2022-03-26T13:13:49.256699Z","iopub.status.idle":"2022-03-26T13:15:43.658885Z","shell.execute_reply.started":"2022-03-26T13:13:49.256654Z","shell.execute_reply":"2022-03-26T13:15:43.657909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_model = {\n                'eval_metric': 'AUC', \n                'iterations': 1000,\n                'auto_class_weights': 'Balanced',\n                'silent': True,\n                'one_hot_max_size': 15,\n                'early_stopping_rounds': 5,\n                'grow_policy': 'SymmetricTree',\n                'allow_writing_files': False,\n                'use_best_model': True,\n                'random_seed': 123,\n                'depth': 4,\n                'learning_rate': 0.3741239438544723, \n                'l2_leaf_reg': 3.1351883224555164,\n                'bagging_temperature': 2.070491556934441, \n                'subsample': 0.4848286374085917, \n                'colsample_bylevel': 0.4501924728694299, \n                'boosting_type': 'Plain', # 'Ordered'\n                }","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:12:09.582376Z","iopub.execute_input":"2022-03-31T12:12:09.582951Z","iopub.status.idle":"2022-03-31T12:12:09.588363Z","shell.execute_reply.started":"2022-03-31T12:12:09.582917Z","shell.execute_reply":"2022-03-31T12:12:09.587808Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"X_fold, X_test, y_fold, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=y, random_state=123)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:12:12.949598Z","iopub.execute_input":"2022-03-31T12:12:12.950203Z","iopub.status.idle":"2022-03-31T12:12:12.962720Z","shell.execute_reply.started":"2022-03-31T12:12:12.950167Z","shell.execute_reply":"2022-03-31T12:12:12.961656Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"# get result probabilities on 3 folds\ntest_results, train_roc, test_pred, train_f1, train_acc = [], [], [], [], []\nskf = StratifiedKFold(n_splits=3)\n\nfor train_index, valid_index in skf.split(X_fold, y_fold):\n    X_train, X_valid = X_fold.iloc[train_index, :], X_fold.iloc[valid_index, :]\n    y_train, y_valid = y_fold.iloc[train_index], y_fold.iloc[valid_index]\n\n    cat = CatBoostClassifier(cat_features=cat_cols, **params_model)\n\n    cat.fit(X_train, y_train, eval_set=[(X_valid, y_valid)])\n    test_labels = cat.predict(X_test)\n    test_pred.append(test_labels)\n    \n    train_proba = cat.predict_proba(X_train)[:,1]\n    train_roc.append(roc_auc_score(y_train, train_proba))\n    \n    train_labels = cat.predict(X_train)\n    train_f1.append(f1_score(y_train, train_labels))\n    train_acc.append(accuracy_score(y_train, train_labels))\n    \n    pred_test = cat.predict_proba(X_test)[:,1]\n    test_results.append(pred_test)\n\nfinal_test = np.array(test_results).mean(axis=0)\ntest_pred = vote(test_pred, weights=[1.,1.,1.])","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:12:15.430838Z","iopub.execute_input":"2022-03-31T12:12:15.431139Z","iopub.status.idle":"2022-03-31T12:12:16.067995Z","shell.execute_reply.started":"2022-03-31T12:12:15.431107Z","shell.execute_reply":"2022-03-31T12:12:16.067121Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"print(f'train mean accuracy: {np.array(train_acc).mean():.4f}, test accuracy: {accuracy_score(y_test, test_pred):.4f}')\nprint(f'train mean f1: {np.array(train_f1).mean():.4f}, test accuracy: {f1_score(y_test, test_pred):.4f}')\nprint(f'train mean auc: {np.array(train_roc).mean():.4f}, test auc: {roc_auc_score(y_test, final_test):.4f}')\n# cat model also overfits a bit, but also not critically","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:12:18.491045Z","iopub.execute_input":"2022-03-31T12:12:18.491743Z","iopub.status.idle":"2022-03-31T12:12:18.504118Z","shell.execute_reply.started":"2022-03-31T12:12:18.491698Z","shell.execute_reply":"2022-03-31T12:12:18.503072Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_true=y_test, y_pred=test_pred)\nconf_df = pd.DataFrame(cm, index=[0,1], columns=[0,1])\nplt.figure(figsize=(16,6))\nplt.subplot(121)\nsns.heatmap(conf_df, cmap=\"Oranges\", annot=True, annot_kws={\"size\": 16}, fmt='d')\nplt.title('Confusion matrix for test')\nplt.ylabel('True')\nplt.xlabel('Predicted')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:12:22.928455Z","iopub.execute_input":"2022-03-31T12:12:22.928776Z","iopub.status.idle":"2022-03-31T12:12:23.188917Z","shell.execute_reply.started":"2022-03-31T12:12:22.928740Z","shell.execute_reply":"2022-03-31T12:12:23.188280Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"roc_plot(y_test, final_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:12:25.783807Z","iopub.execute_input":"2022-03-31T12:12:25.784694Z","iopub.status.idle":"2022-03-31T12:12:25.987782Z","shell.execute_reply.started":"2022-03-31T12:12:25.784643Z","shell.execute_reply":"2022-03-31T12:12:25.986797Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"# shap importancies\nshap_values = cat.get_feature_importance(train_pool, type='ShapValues')\n\nexpected_value = shap_values[0,-1]\nshap_values = shap_values[:,:-1]","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:12:29.347380Z","iopub.execute_input":"2022-03-31T12:12:29.347706Z","iopub.status.idle":"2022-03-31T12:12:29.684870Z","shell.execute_reply.started":"2022-03-31T12:12:29.347671Z","shell.execute_reply":"2022-03-31T12:12:29.684131Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"shap.initjs()\nshap.force_plot(expected_value, shap_values[0,:], X_fold.iloc[0,:])\n# so client with big sum of transactions is more likely to stay, otherhand with low frequency of transations and\n# long time inactive he is going to churn","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:12:32.308623Z","iopub.execute_input":"2022-03-31T12:12:32.309495Z","iopub.status.idle":"2022-03-31T12:12:32.333491Z","shell.execute_reply.started":"2022-03-31T12:12:32.309446Z","shell.execute_reply":"2022-03-31T12:12:32.332577Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"shap.initjs()\nshap.summary_plot(shap_values, X_train) # initial, not from kfold loop!\n# so Total_Trans_Ct if high -> leads to churn, big sums of transactions leads client to stay, High amount of contancts\n# leads client to stay","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:13:55.744835Z","iopub.execute_input":"2022-03-31T12:13:55.745572Z","iopub.status.idle":"2022-03-31T12:13:57.673064Z","shell.execute_reply.started":"2022-03-31T12:13:55.745519Z","shell.execute_reply":"2022-03-31T12:13:57.672225Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":"#### Let's try to retrain models during kfold splits","metadata":{}},{"cell_type":"code","source":"# experiment\ncats = [CatBoostClassifier(cat_features=cat_cols, **params_model)] * 3\ntest_results, train_roc, test_pred, train_f1, train_acc = [], [], [], [], []\nskf = StratifiedKFold(n_splits=3)\n\ni = 0\nfor train_index, valid_index in skf.split(X_fold, y_fold):\n    X_train, X_valid = X_fold.iloc[train_index, :], X_fold.iloc[valid_index, :]\n    y_train, y_valid = y_fold.iloc[train_index], y_fold.iloc[valid_index]\n    \n    if i == 0:\n        cats[i].fit(X_train, y_train, eval_set=[(X_valid, y_valid)])\n    else: \n        cats[i].fit(X_train, y_train, eval_set=[(X_valid, y_valid)], init_model=cats[i-1])\n    test_labels = cats[i].predict(X_test)\n    test_pred.append(test_labels)\n    \n    train_proba = cats[i].predict_proba(X_train)[:,1]\n    train_roc.append(roc_auc_score(y_train, train_proba))\n    \n    train_labels = cats[i].predict(X_train)\n    train_f1.append(f1_score(y_train, train_labels))\n    train_acc.append(accuracy_score(y_train, train_labels))\n    \n    pred_test = cats[i].predict_proba(X_test)[:,1]\n    test_results.append(pred_test)\n    i+=1\n\nfinal_test = np.array(test_results).mean(axis=0)\ntest_pred = vote(test_pred, weights=[1.,1.,1.])","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:14:21.391973Z","iopub.execute_input":"2022-03-31T12:14:21.392551Z","iopub.status.idle":"2022-03-31T12:14:21.820732Z","shell.execute_reply.started":"2022-03-31T12:14:21.392496Z","shell.execute_reply":"2022-03-31T12:14:21.819735Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"print(f'train mean accuracy: {np.array(train_acc).mean():.4f}, test accuracy: {accuracy_score(y_test, test_pred):.4f}')\nprint(f'train mean f1: {np.array(train_f1).mean():.4f}, test accuracy: {f1_score(y_test, test_pred):.4f}')\nprint(f'train mean auc: {np.array(train_roc).mean():.4f}, test auc: {roc_auc_score(y_test, final_test):.4f}')\n# f1-score a bit better","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:14:25.434717Z","iopub.execute_input":"2022-03-31T12:14:25.435402Z","iopub.status.idle":"2022-03-31T12:14:25.447528Z","shell.execute_reply.started":"2022-03-31T12:14:25.435362Z","shell.execute_reply":"2022-03-31T12:14:25.446542Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"markdown","source":"#### Let's try another approach: weighted sequential ensemble learing - AdaBoost","metadata":{}},{"cell_type":"code","source":"# again repeat all necessary steps\ndf = pd.read_csv(data_root) # read\ndf = df.iloc[:, :-2] # remove redundant columns\ndf.drop_duplicates(inplace=True) # drop duplicates\ndf = reduce_memory(df, verbose=0) # reduce memory\ndf[TARGET_NAME] = df[TARGET_NAME].map({'Existing Customer': 0, 'Attrited Customer': 1}) # map target\ndf[TARGET_NAME] = df[TARGET_NAME].astype(np.uint8)\ndf['Gender'] = df['Gender'].map({'M': 0, 'F': 1})\ndf['Gender'] = df['Gender'].astype(np.uint8)\ndf.set_index('CLIENTNUM', drop=True, inplace=True) # set client id as index\ndf = pd.get_dummies(df, prefix=['col1', 'col2', 'col3', 'col4'])\n# as Ada Boost algorithm also uses Decision Trees, no data scaling is needed\n# split\nX, y = df.drop(TARGET_NAME, axis=1), df[TARGET_NAME]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=y, random_state=123)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, shuffle=True, stratify=y_train, random_state=123)\nprint(f'Train size: {X_train.shape}, Validation size: {X_valid.shape}, Test size: {X_test.shape}')","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:17:28.256737Z","iopub.execute_input":"2022-03-31T12:17:28.257154Z","iopub.status.idle":"2022-03-31T12:17:28.370249Z","shell.execute_reply.started":"2022-03-31T12:17:28.257117Z","shell.execute_reply":"2022-03-31T12:17:28.369224Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"model_params = {\n                'random_state': 123,  \n}","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:19:12.308395Z","iopub.execute_input":"2022-03-31T12:19:12.308728Z","iopub.status.idle":"2022-03-31T12:19:12.314116Z","shell.execute_reply.started":"2022-03-31T12:19:12.308698Z","shell.execute_reply":"2022-03-31T12:19:12.312750Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n    param_trials = {\n                    'n_estimators': trial.suggest_int('n_estimators', 50, 800),\n                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 2.),\n                    'algorithm': trial.suggest_categorical('algorithm', ['SAMME', 'SAMME.R'])\n                    }\n    param_trials.update(model_params)\n    opt_model = AdaBoostClassifier(**param_trials)\n    opt_model.fit(X_train, y_train)\n    \n    y_pred = opt_model.predict_proba(X_valid)[:,1]\n    score = roc_auc_score(y_valid, y_pred)\n    \n    return score","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:22:58.826614Z","iopub.execute_input":"2022-03-31T12:22:58.826896Z","iopub.status.idle":"2022-03-31T12:22:58.834535Z","shell.execute_reply.started":"2022-03-31T12:22:58.826869Z","shell.execute_reply":"2022-03-31T12:22:58.833268Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"optuna.logging.set_verbosity(optuna.logging.FATAL)\nstudy = optuna.create_study(sampler=TPESampler(seed=123), direction=\"maximize\")\nstudy.optimize(objective, n_trials=300, timeout=6000)\n\nprint(f'Number of completed trials: {len(study.trials)}')\nprint('Best trial')\ntrial = study.best_trial\nprint(f'Best score: {trial.value}')\nprint('Best params')\nfor key, value in trial.params.items():\n    print(f'{key}: {value}')","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:23:37.329420Z","iopub.execute_input":"2022-03-31T12:23:37.330173Z","iopub.status.idle":"2022-03-31T12:44:59.349439Z","shell.execute_reply.started":"2022-03-31T12:23:37.330133Z","shell.execute_reply":"2022-03-31T12:44:59.348473Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"X_fold, X_test, y_fold, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=y, random_state=123)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:55:05.180346Z","iopub.execute_input":"2022-03-31T12:55:05.180724Z","iopub.status.idle":"2022-03-31T12:55:05.196662Z","shell.execute_reply.started":"2022-03-31T12:55:05.180686Z","shell.execute_reply":"2022-03-31T12:55:05.195735Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"model_params = {\n                'random_state': 123,\n                'n_estimators': 351,\n                'learning_rate': 1.7243852737318626,\n                'algorithm': 'SAMME',\n}","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:55:08.211231Z","iopub.execute_input":"2022-03-31T12:55:08.211699Z","iopub.status.idle":"2022-03-31T12:55:08.216277Z","shell.execute_reply.started":"2022-03-31T12:55:08.211654Z","shell.execute_reply":"2022-03-31T12:55:08.215638Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"# get result probabilities on 3 folds\ntest_results, train_roc, test_pred, train_f1, train_acc = [], [], [], [], []\nskf = StratifiedKFold(n_splits=3)\n\nfor train_index, valid_index in skf.split(X_fold, y_fold):\n    X_train, X_valid = X_fold.iloc[train_index, :], X_fold.iloc[valid_index, :]\n    y_train, y_valid = y_fold.iloc[train_index], y_fold.iloc[valid_index]\n\n    cat = AdaBoostClassifier(**model_params)\n\n    cat.fit(X_train, y_train)\n    test_labels = cat.predict(X_test)\n    test_pred.append(test_labels)\n    \n    train_proba = cat.predict_proba(X_train)[:,1]\n    train_roc.append(roc_auc_score(y_train, train_proba))\n    \n    train_labels = cat.predict(X_train)\n    train_f1.append(f1_score(y_train, train_labels))\n    train_acc.append(accuracy_score(y_train, train_labels))\n    \n    pred_test = cat.predict_proba(X_test)[:,1]\n    test_results.append(pred_test)\n\nfinal_test = np.array(test_results).mean(axis=0)\ntest_pred = vote(test_pred, weights=[1.,1.,1.])","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:57:03.658125Z","iopub.execute_input":"2022-03-31T12:57:03.658458Z","iopub.status.idle":"2022-03-31T12:57:13.344303Z","shell.execute_reply.started":"2022-03-31T12:57:03.658427Z","shell.execute_reply":"2022-03-31T12:57:13.343363Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"print(f'train mean accuracy: {np.array(train_acc).mean():.4f}, test accuracy: {accuracy_score(y_test, test_pred):.4f}')\nprint(f'train mean f1: {np.array(train_f1).mean():.4f}, test accuracy: {f1_score(y_test, test_pred):.4f}')\nprint(f'train mean auc: {np.array(train_roc).mean():.4f}, test auc: {roc_auc_score(y_test, final_test):.4f}')\n# ada boost is a bit worse","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:57:20.533445Z","iopub.execute_input":"2022-03-31T12:57:20.533741Z","iopub.status.idle":"2022-03-31T12:57:20.547927Z","shell.execute_reply.started":"2022-03-31T12:57:20.533710Z","shell.execute_reply":"2022-03-31T12:57:20.547069Z"},"trusted":true},"execution_count":101,"outputs":[]}]}