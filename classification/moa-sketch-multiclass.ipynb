{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -qqU py-boost iterative-stratification","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-30T14:49:51.306455Z","iopub.execute_input":"2022-12-30T14:49:51.306894Z","iopub.status.idle":"2022-12-30T14:50:14.177540Z","shell.execute_reply.started":"2022-12-30T14:49:51.306812Z","shell.execute_reply":"2022-12-30T14:50:14.176330Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbeatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\nvirtualenv 20.16.5 requires importlib-metadata>=4.8.3; python_version < \"3.8\", but you have importlib-metadata 1.7.0 which is incompatible.\npandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.1 which is incompatible.\nmarkdown 3.3.7 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 1.7.0 which is incompatible.\nkeyring 23.6.0 requires importlib-metadata>=3.6; python_version < \"3.10\", but you have importlib-metadata 1.7.0 which is incompatible.\nibis-framework 2.1.1 requires importlib-metadata<5,>=4; python_version < \"3.8\", but you have importlib-metadata 1.7.0 which is incompatible.\ngym 0.26.2 requires importlib-metadata>=4.8.0; python_version < \"3.10\", but you have importlib-metadata 1.7.0 which is incompatible.\nflask 2.2.2 requires importlib-metadata>=3.6.0; python_version < \"3.10\", but you have importlib-metadata 1.7.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from py_boost import GradientBoosting, SketchBoost\n\nfrom py_boost.multioutput.sketching import *\nfrom py_boost.multioutput.target_splitter import *\n\nfrom py_boost.gpu.losses import Loss, Metric\nimport cupy as cp","metadata":{"execution":{"iopub.status.busy":"2022-12-30T14:50:14.180089Z","iopub.execute_input":"2022-12-30T14:50:14.180430Z","iopub.status.idle":"2022-12-30T14:50:27.937512Z","shell.execute_reply.started":"2022-12-30T14:50:14.180398Z","shell.execute_reply":"2022-12-30T14:50:27.936492Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport time\nfrom functools import partial\nfrom tqdm import trange\nimport copy\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport missingno as msno\n\nimport category_encoders as ce\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, QuantileTransformer, StandardScaler\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.decomposition import PCA\n\nimport sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport optuna\nfrom optuna.samplers import TPESampler","metadata":{"execution":{"iopub.status.busy":"2022-12-30T14:50:27.938953Z","iopub.execute_input":"2022-12-30T14:50:27.939351Z","iopub.status.idle":"2022-12-30T14:50:28.161440Z","shell.execute_reply.started":"2022-12-30T14:50:27.939312Z","shell.execute_reply":"2022-12-30T14:50:28.160428Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_drug = pd.read_csv(\"/kaggle/input/lish-moa/train_drug.csv\")\ntrain_features = pd.read_csv(\"/kaggle/input/lish-moa/train_features.csv\")\n\ntargets = pd.read_csv(\"/kaggle/input/lish-moa/train_targets_scored.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-12-30T14:50:28.163990Z","iopub.execute_input":"2022-12-30T14:50:28.164720Z","iopub.status.idle":"2022-12-30T14:50:33.490196Z","shell.execute_reply.started":"2022-12-30T14:50:28.164684Z","shell.execute_reply":"2022-12-30T14:50:33.489088Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def multi_logloss(y_true, y_pred, sample_weights=None, activation=True):\n    \"\"\"\n    y_true: array of shape [N, M], where sample X_ij - 0 or 1 (dtype: np.int32),\n    y_pred: array of shape [N, M], where sample X_ij - [0, 1] - estimated probability of being positive (dtype: np.float32)\n    sample_weigts: array of shape [N, M], where every vector [N, 1] consists of sample weights (dtype: np.float32)\n    N - number of samples \n    M - number of classes\n    returns:\n        Scalar float value of computed loss\n    \"\"\"\n    if sample_weights is None:\n        sample_weights = np.ones_like(y_true)  # [N, M]\n\n    norm = np.sum(sample_weights, axis=0)\n    if activation:\n        y_pred = 1 / (1 + np.exp(-y_pred))  # sigmoid activation\n    y_pred = np.clip(y_pred, 0.001, 0.999)\n    loss = y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)  # [N, M]\n    loss = loss * sample_weights\n    loss = np.sum(loss, axis=0)  # sum over all samples per class  [1, M]\n    loss = loss / norm  # normalize by weights sum, [1, M]\n    return - loss.mean()  # .sum() sum[1, M] / M\n\n\nclass CustomMetric(Metric):\n    \n    def compare(self, v0 ,v1):\n        \n        return v0 < v1\n    \n    def __call__(self, y_true, y_pred, sample_weight=None):\n        final_metric = multi_logloss(y_true, y_pred, sample_weights=sample_weight, activation=False)\n        \n        return final_metric\n\n\nclass CustomLoss(Loss):\n    smooth = 0.0  # 0.001\n    clip_value = 1e-7\n    \n    def preprocess_input(self, y_true):\n        y_true = y_true * (1 - self.smooth) + 0.5 * self.smooth\n        \n        return y_true\n    \n    def postprocess_output(self, y_pred):\n        pred = 1 / (1 + cp.exp(-y_pred))  # sigmoid activation\n        pred = cp.clip(pred, self.clip_value, 1 - self.clip_value)\n        \n        return pred\n    \n    def get_grad_hess(self, y_true, y_pred):\n        pred = 1 / (1 + cp.exp(-y_pred))  # sigmoid activation\n        pred = cp.clip(pred, 0.001, 0.999)  # clip extreme cases\n        \n        # first order derivative\n        grad = pred - y_true  # same as for mse loss\n        # second order derivative\n        hess = pred * (1 - pred)\n        \n        return grad, hess\n\n    def base_score(self, y_true):\n        means = cp.clip(y_true.mean(axis=0), self.clip_value, 1 - self.clip_value)\n        mean_log_odds = cp.log(means / (1 - means))  # logits to apply activation after\n        \n        return mean_log_odds\n    \n    def get_metric_from_string(self, name=None):\n        if name is None:\n            return CustomMetric()\n        return metric_alias[name]\n\n\ndef print_scores(folds_scores, train_scores):\n    print(f\"Train score by each fold: {train_scores}\")\n    print(f\"Valid score by each fold: {folds_scores}\")\n    print(f\"Train mean score by each fold:{np.mean(train_scores):.5f} +/- {np.std(train_scores):.5f}\")\n    print(f\"Valid mean score by each fold:{np.mean(folds_scores):.5f} +/- {np.std(folds_scores):.5f}\")\n    print(\"*\" * 50)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T14:50:33.491630Z","iopub.execute_input":"2022-12-30T14:50:33.492027Z","iopub.status.idle":"2022-12-30T14:50:33.512466Z","shell.execute_reply.started":"2022-12-30T14:50:33.491988Z","shell.execute_reply":"2022-12-30T14:50:33.511326Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"(np.unique(train_drug.sig_id) != np.unique(train_features.sig_id)).sum()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T09:36:07.628641Z","iopub.execute_input":"2022-12-30T09:36:07.629348Z","iopub.status.idle":"2022-12-30T09:36:07.670956Z","shell.execute_reply.started":"2022-12-30T09:36:07.629314Z","shell.execute_reply":"2022-12-30T09:36:07.670117Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"train_features.isna().sum().sum(), targets.isna().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T09:36:07.673001Z","iopub.execute_input":"2022-12-30T09:36:07.673337Z","iopub.status.idle":"2022-12-30T09:36:07.727183Z","shell.execute_reply.started":"2022-12-30T09:36:07.673305Z","shell.execute_reply":"2022-12-30T09:36:07.726193Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(0, 0)"},"metadata":{}}]},{"cell_type":"code","source":"msno.matrix(targets.replace({0.0, np.nan}), figsize=(14,6))","metadata":{"execution":{"iopub.status.busy":"2022-12-30T09:36:07.728600Z","iopub.execute_input":"2022-12-30T09:36:07.729018Z","iopub.status.idle":"2022-12-30T09:36:10.183533Z","shell.execute_reply.started":"2022-12-30T09:36:07.728982Z","shell.execute_reply":"2022-12-30T09:36:10.182211Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1008x432 with 2 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAA3wAAAFnCAYAAAD9mktTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQMUlEQVR4nO3decxld13H8c8XkABpCYhCDRAWlRRFRQwgi3EGKEQxAlISFYVUC6iBBpVWKKAjGsEqQqMQtZQ21cEEQzA2iktlkKWEaGWUCtSQoiym7EJrkaHl6x/nDtxepu1szzztd16v5OZuZ/k956/nnd+551R3BwAAgHlus90DAAAAYGsIPgAAgKEEHwAAwFCCDwAAYCjBBwAAMJTgAwAAGErwAQAADHXcBV9VnVpVv19V76iqL1RVV9Wfbve4AAAAjrbbbfcAtsFLknxPkmuSfCzJyds7HAAAgK1x3M3wJfnFJA9IcuckP7/NYwEAANgyx90MX3fv2f+6qrZzKAAAAFvqeJzhAwAAOC4IPgAAgKEE32HasWNH79ixo/fu3dt79+7tA72/sdeH+51t3DK3cWsYo23Yhm3Yxq19G7eGMdqGbUzexv7HNv3r3VvxuOKKK3rnzp196aWXbsn2V49tJ/gAAACGEnwAAABDCT4AAIChBB8AAMBQgg8AAGCo4+7G61X15CRPXr09afX8iKq6cPX60939gmM8LAAAgKPuuAu+JA9O8syNz+6/eiTJfyURfAAAwK3ecXdKZ3fv6u66icd9t3uMAAAAR8NxF3wAAADHC8EHAAAwlOADAAAYSvABAAAMJfgAAACGEnwAAABDCT4AAIChBB8AAMBQgg8AAGAowQcAADCU4AMAABhK8AEAAAwl+AAAAIYSfAAAAEMJPgAAgKEEHwAAwFCCDwAAYCjBBwAAMJTgAwAAGErwAQAADCX4AAAAhhJ8AAAAQwk+AACAoQQfAADAUIIPAABgKMEHAAAwlOADAAAYSvABAAAMJfgAAACGEnwAAABDCT4AAIChBB8AAMBQgg8AAGAowQcAADCU4AMAABhK8AEAAAwl+AAAAIYSfAAAAEMJPgAAgKEEHwAAwFCCDwAAYCjBBwAAMJTgAwAAGErwAQAADCX4AAAAhhJ8AAAAQwk+AACAoQQfAADAUIIPAABgKMEHAAAwlOADAAAYSvABAAAMJfgAAACGEnwAAABDCT4AAIChBB8AAMBQgg8AAGAowQcAADCU4AMAABhK8AEAAAwl+AAAAIYSfAAAAEMJPgAAgKEEHwAAwFCCDwAAYCjBBwAAMJTgAwAAGErwAQAADCX4AAAAhhJ8AAAAQwk+AACAoQQfAADAUIIPAABgKMEHAAAwlOADAAAYSvABAAAMJfgAAACGEnwAAABDCT4AAIChBB8AAMBQgg8AAGAowQcAADCU4AMAABhK8AEAAAwl+AAAAIYSfAAAAEMJPgAAgKEEHwAAwFCCDwAAYCjBBwAAMJTgAwAAGErwAQAADCX4AAAAhhJ8AAAAQwk+AACAoQQfAADAUIIPAABgKMEHAAAwlOADAAAYSvABAAAMJfgAAACGEnwAAABDCT4AAIChBB8AAMBQgg8AAGAowQcAADCU4AMAABhK8AEAAAwl+AAAAIYSfAAAAEMJPgAAgKEEHwAAwFCCDwAAYCjBBwAAMJTgAwAAGErwAQAADCX4AAAAhhJ8AAAAQwk+AACAoQQfAADAUIIPAABgKMEHAAAwlOADAAAYSvABAAAMJfgAAACGEnwAAABDCT4AAIChBB8AAMBQgg8AAGAowQcAADCU4AMAABhK8AEAAAwl+AAAAIYSfAAAAEMJPgAAgKEEHwAAwFCCDwAAYCjBBwAAMJTgAwAAGErwAQAADCX4AAAAhhJ8AAAAQwk+AACAoQQfAADAUIIPAABgKMEHAAAwlOADAAAYSvABAAAMJfgAAACGEnwAAABDCT4AAIChBB8AAMBQgg8AAGAowQcAADCU4AMAABhK8AEAAAwl+AAAAIYSfAAAAEMJPgAAgKEEHwAAwFCCDwAAYCjBBwAAMJTgAwAAGErwAQAADCX4AAAAhhJ8AAAAQwk+AACAoQQfAADAUIIPAABgKMEHAAAwlOADAAAYSvABAAAMJfgAAACGEnwAAABDCT4AAIChBB8AAMBQgg8AAGAowQcAADCU4AMAABhK8AEAAAwl+AAAAIYSfAAAAEMJPgAAgKEEHwAAwFCCDwAAYCjBBwAAMJTgAwAAGErwAQAADCX4AAAAhhJ8AAAAQwk+AACAoQQfAADAUIIPAABgKMEHAAAwlOADAAAYSvABAAAMJfgAAACGEnwAAABDCT4AAIChBB8AAMBQgg8AAGAowQcAADCU4AMAABhK8AEAAAwl+AAAAIYSfAAAAEMJPgAAgKEEHwAAwFCCDwAAYCjBBwAAMJTgAwAAGErwAQAADCX4AAAAhhJ8AAAAQwk+AACAoQQfAADAUIIPAABgKMEHAAAwlOADAAAYSvABAAAMJfgAAACGEnwAAABDCT4AAIChBB8AAMBQgg8AAGAowQcAADCU4AMAABhK8AEAAAwl+AAAAIYSfAAAAEMJPgAAgKEEHwAAwFCCDwAAYCjBBwAAMJTgAwAAGErwAQAADCX4AAAAhhJ8AAAAQwk+AACAoQQfAADAUIIPAABgKMEHAAAwlOADAAAYSvABAAAMJfgAAACGEnwAAABDCT4AAIChBB8AAMBQgg8AAGAowQcAADCU4AMAABhK8AEAAAwl+AAAAIYSfAAAAEMJPgAAgKEEHwAAwFCCDwAAYCjBBwAAMJTgAwAAGErwAQAADCX4AAAAhhJ8AAAAQwk+AACAoQQfAADAUIIPAABgKMEHAAAwlOADAAAYSvABAAAMJfgAAACGEnwAAABDCT4AAIChBB8AAMBQgg8AAGAowQcAADCU4AMAABhK8AEAAAwl+AAAAIYSfAAAAEMJPgAAgKEEHwAAwFCCDwAAYCjBBwAAMJTgAwAAGErwAQAADCX4AAAAhhJ8AAAAQwk+AACAoQQfAADAUIIPAABgKMEHAAAwlOADAAAYSvABAAAMJfgAAACGEnwAAABDCT4AAIChBB8AAMBQgg8AAGAowQcAADCU4AMAABhK8AEAAAwl+AAAAIYSfAAAAEMJPgAAgKEEHwAAwFCCDwAAYCjBBwAAMJTgAwAAGErwAQAADCX4AAAAhhJ8AAAAQwk+AACAoQ4q+KrqblV1elW9uao+VFVfrKrPV9U7q+pnq+o2G8vfu6peW1XvqaqrqupLVfXfVfWOqjqtqr7hRvZz96o6p6our6qrq+ozVXVZVZ1ZVSceYPmHVdXLq+otq/10VX3sUA5AVf3Uar2uqtMPZV0AAIBbsoOd4XtakvOSPDzJe5K8OsmbkjwoyeuSvLGqam35b03y9CSfT/IXSV6Z5OIk90ny+iR/W1W3W99BVd03yfuSnJnkU0n+MMkbkpyQ5Jwk76yqO26M6yeTvDDJY5NcdZB/y/o+753kD5Jcc6jrAgDA8ayqXlRV/1RVX6iqT1XVxVX1oI1lqqp2rSZ/vlhVb6uq71z7fsfa5Mvm42nH/q86+rb7OB1s8P1Hkh9Ncq/ufnp3v6i7fybJyUk+muSpSX5sbflLk9y1ux/f3T/X3Wd393OyhODbkuzcWD5ZQu/uSXZ1987uPrO7n5fkO5K8Ncl3ZwnPdRcmeUiSE7r7wQf5tyRZDmqSC5J8JktcAgAAB29HktcmeWSSxyS5LsklVfWNa8ucleSXkzwvyUOTfDLJ36+dvXdpkm/ZeLw8y4TMW7b+TzgmdmQbj9NBBV93v7W7L+7ur2x8flW+Fks71j7ft7ns6vMvZ5nxS5Jv3/j6/qvnv9xY5/okf7V6+80b3+3t7vd2976D+Ts2nJHlgJ+W5H8PY30AADhudfcTuvuC7r68u9+X5Kez/L/+qOSrEyzPT/KK7n5Td1+e5JlJTsxypt7+brhq/ZHk1CR/1t1behbeddddlyQ5//zzc8EFF2TfvsNJipu33cfpaFy05cur5+tubsGqum2SH169/beNr/999fzEjXVuk+SHknwly0zfEauqByZ5RZJzu/vtR2ObAABwnDsxS198bvX+fklOSvJ3+xfo7i8meXuW2a6vU1U7skwM/fEWjjP79u3LWWedlSS58sorc9FFF+XUU0/dsujbcEyP0+1uboGbsvod3jNWb//mAN9/U5LnJqksFXtKkm9L8obuvnhj8XOS/EiS36iqnUn+Jcntkzw+ywE4vbvfeyTjXRvznyT5SJKzj3R7AABAkuTcJHuTvHv1/qTV8yc2lvtEknveyDaenWRvd//zUR/dmt27d+faa6+9wWdXX311du/endNOO20rd50c4+NU3X0YY1ytXPW7Wc41/evufuIBvj85yQfWPuosF3A5e3V65+byd8lyUZenbKxzXpLf7O6P3sx4OsnHu/teN7HMy5K8OMmju/vdq892Jfm1JM/q7tfd1D4AAIAbqqrfS/LjWf7HvnL12SOTvCvJfbr7I2vLvj7JPbv7CRvbuFuSjyf5pe5+7VaOd+fOnZdkufDjpkv27NlzylbtdzuO02HP8FXVGVli74NZzkP9Ot39wWXRum2WOn1KkpcleXRVPbG7P7u2vftm+f3eHbOc9vmuJHdK8qQskfikqnpEd3/4CMb88Cyzeq/cH3sAAMDhq6pXZYmYnfsjZmX/VfTvkeXsuqy9P9AV9p+R5Poku7dinOv27NnzuK3ex6btOk6H9Ru+qnpulqnI92cZ8Gdvavnuvr67P9Ld5yZ5TpLvzxJ+6y5M8l1Jntrdb+nuL6x+kPhHWWbk7pFlFu6wrE7lvCjLFUdferjbAQAAFlV1bpKfSPKY1WTPug9nCZZT1pa/Q5IfyHLVyU2nJ/nz7v78Fg1322zncTrkGb6qen6SVyW5PMlju/uTh7iJ/ZcN3bG2zROT/GCSz3b35sVckmTP6vn7DnFf605I8oDV6/+74W0Dv+q8qjovy8Vcnn8E+wIAgNGq6jVZzvR7cpLPVdX+36Jd093XdHdX1auTnF1VH8wy8fKSLLcSeMPGth6d5XZszz5Gwz9mtvs4HVLwVdWvZLm65d4kp3T3pw9l/ZX9Pzxcv6rn7VfPd66q2x/gNgv7b8dwJJfN+VKS82/ku4ck+d4k70xyRb72A0oAAODAfmH1/A8bn/96kl2r1+dk+cnWa5LcNcl7kjy+u6/eWOdZST7Q3e/amqFuq209Tgd90ZaqemmW0zAvW+38Rk/jrKqHJPnX1T301j8/IcmbkzwuyW9194vXvnt/kgdmuTjLS9c+v0OWWcEdSX6nu8+6if3e7EVbbmS9XXHRFgAAYJiDmuGrqmdmib3rk7wjyRkHOCXyP7v7wtXrX03yqKq6NMsPD69Ncu8s99O7S5ZzUV++sf4ZWW6w/pKqOmW1zB1X69wnyYeS/PbGuE5O8sKN7dy1qi5ce/+Cw5yJBAAAuFU72FM677d6vm2Wu8AfyD9mufBKstxG4ZokD8syM3enLDcWvCzJG5O8vrtvcKP27r6kqh6a5Mwsv+d7bpbAvDJLHJ7T3f+zsc+TstyFft2dNj7blUTwAQAAx50jug8fAAAAt1yHdVsGAAAAbvkEHwAAwFCCDwAAYCjBBwAAMJTgAwAAGErwAQAADCX4AAAAhhJ8AAAAQwk+AACAoQQfAADAUP8PB6Hgrnqv2bIAAAAASUVORK5CYII=\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"num_empty = 0\nfor column in targets.columns:\n    if not np.any(targets[column].values):\n        num_empty += 1\nnum_empty","metadata":{"execution":{"iopub.status.busy":"2022-12-30T09:36:10.186987Z","iopub.execute_input":"2022-12-30T09:36:10.187323Z","iopub.status.idle":"2022-12-30T09:36:10.213702Z","shell.execute_reply.started":"2022-12-30T09:36:10.187291Z","shell.execute_reply":"2022-12-30T09:36:10.212669Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"np.sum(targets.sum(axis=1) == 0)  # number of no-response","metadata":{"execution":{"iopub.status.busy":"2022-12-30T09:36:10.215360Z","iopub.execute_input":"2022-12-30T09:36:10.215731Z","iopub.status.idle":"2022-12-30T09:36:10.639231Z","shell.execute_reply.started":"2022-12-30T09:36:10.215696Z","shell.execute_reply":"2022-12-30T09:36:10.638179Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"9367"},"metadata":{}}]},{"cell_type":"code","source":"np.sum(targets.sum(axis=1) > 1)  # number of multilabels","metadata":{"execution":{"iopub.status.busy":"2022-12-30T09:36:13.597326Z","iopub.execute_input":"2022-12-30T09:36:13.597687Z","iopub.status.idle":"2022-12-30T09:36:14.014535Z","shell.execute_reply.started":"2022-12-30T09:36:13.597659Z","shell.execute_reply":"2022-12-30T09:36:14.013076Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"1915"},"metadata":{}}]},{"cell_type":"code","source":"train_features.describe()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T09:36:15.777561Z","iopub.execute_input":"2022-12-30T09:36:15.777932Z","iopub.status.idle":"2022-12-30T09:36:18.410064Z","shell.execute_reply.started":"2022-12-30T09:36:15.777901Z","shell.execute_reply":"2022-12-30T09:36:18.408946Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"            cp_time           g-0           g-1           g-2           g-3  \\\ncount  23814.000000  23814.000000  23814.000000  23814.000000  23814.000000   \nmean      48.020156      0.248366     -0.095684      0.152253      0.081971   \nstd       19.402807      1.393399      0.812363      1.035731      0.950012   \nmin       24.000000     -5.513000     -5.737000     -9.104000     -5.998000   \n25%       24.000000     -0.473075     -0.562200     -0.437750     -0.429575   \n50%       48.000000     -0.008850     -0.046600      0.075200      0.008050   \n75%       72.000000      0.525700      0.403075      0.663925      0.463400   \nmax       72.000000     10.000000      5.039000      8.257000     10.000000   \n\n                g-4           g-5           g-6           g-7           g-8  \\\ncount  23814.000000  23814.000000  23814.000000  23814.000000  23814.000000   \nmean       0.057347     -0.138836      0.035961     -0.202651     -0.190083   \nstd        1.032091      1.179388      0.882395      1.125494      1.749885   \nmin       -6.369000    -10.000000    -10.000000    -10.000000    -10.000000   \n25%       -0.470925     -0.602225     -0.493900     -0.525175     -0.511675   \n50%       -0.026900     -0.015650     -0.000650     -0.017900      0.010000   \n75%        0.465375      0.510425      0.528725      0.411900      0.549225   \nmax       10.000000      7.282000      7.333000      5.473000      8.887000   \n\n       ...          c-90          c-91          c-92          c-93  \\\ncount  ...  23814.000000  23814.000000  23814.000000  23814.000000   \nmean   ...     -0.469244     -0.461411     -0.513256     -0.500142   \nstd    ...      2.000488      2.042475      2.001714      2.107105   \nmin    ...    -10.000000    -10.000000    -10.000000    -10.000000   \n25%    ...     -0.566175     -0.565975     -0.589975     -0.568700   \n50%    ...     -0.009900      0.003250     -0.009100     -0.013750   \n75%    ...      0.457750      0.461500      0.445675      0.452900   \nmax    ...      4.069000      3.960000      3.927000      3.596000   \n\n               c-94          c-95          c-96          c-97          c-98  \\\ncount  23814.000000  23814.000000  23814.000000  23814.000000  23814.000000   \nmean      -0.507093     -0.353726     -0.463485     -0.378241     -0.470252   \nstd        2.159589      1.629291      2.059725      1.703615      1.834828   \nmin      -10.000000    -10.000000    -10.000000    -10.000000    -10.000000   \n25%       -0.563775     -0.567975     -0.552575     -0.561000     -0.592600   \n50%       -0.003300     -0.010250     -0.001250     -0.006800      0.014000   \n75%        0.470900      0.444750      0.465225      0.446400      0.461275   \nmax        3.747000      2.814000      3.505000      2.924000      3.111000   \n\n               c-99  \ncount  23814.000000  \nmean      -0.301505  \nstd        1.407918  \nmin      -10.000000  \n25%       -0.562900  \n50%       -0.019500  \n75%        0.438650  \nmax        3.805000  \n\n[8 rows x 873 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cp_time</th>\n      <th>g-0</th>\n      <th>g-1</th>\n      <th>g-2</th>\n      <th>g-3</th>\n      <th>g-4</th>\n      <th>g-5</th>\n      <th>g-6</th>\n      <th>g-7</th>\n      <th>g-8</th>\n      <th>...</th>\n      <th>c-90</th>\n      <th>c-91</th>\n      <th>c-92</th>\n      <th>c-93</th>\n      <th>c-94</th>\n      <th>c-95</th>\n      <th>c-96</th>\n      <th>c-97</th>\n      <th>c-98</th>\n      <th>c-99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>23814.000000</td>\n      <td>23814.000000</td>\n      <td>23814.000000</td>\n      <td>23814.000000</td>\n      <td>23814.000000</td>\n      <td>23814.000000</td>\n      <td>23814.000000</td>\n      <td>23814.000000</td>\n      <td>23814.000000</td>\n      <td>23814.000000</td>\n      <td>...</td>\n      <td>23814.000000</td>\n      <td>23814.000000</td>\n      <td>23814.000000</td>\n      <td>23814.000000</td>\n      <td>23814.000000</td>\n      <td>23814.000000</td>\n      <td>23814.000000</td>\n      <td>23814.000000</td>\n      <td>23814.000000</td>\n      <td>23814.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>48.020156</td>\n      <td>0.248366</td>\n      <td>-0.095684</td>\n      <td>0.152253</td>\n      <td>0.081971</td>\n      <td>0.057347</td>\n      <td>-0.138836</td>\n      <td>0.035961</td>\n      <td>-0.202651</td>\n      <td>-0.190083</td>\n      <td>...</td>\n      <td>-0.469244</td>\n      <td>-0.461411</td>\n      <td>-0.513256</td>\n      <td>-0.500142</td>\n      <td>-0.507093</td>\n      <td>-0.353726</td>\n      <td>-0.463485</td>\n      <td>-0.378241</td>\n      <td>-0.470252</td>\n      <td>-0.301505</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>19.402807</td>\n      <td>1.393399</td>\n      <td>0.812363</td>\n      <td>1.035731</td>\n      <td>0.950012</td>\n      <td>1.032091</td>\n      <td>1.179388</td>\n      <td>0.882395</td>\n      <td>1.125494</td>\n      <td>1.749885</td>\n      <td>...</td>\n      <td>2.000488</td>\n      <td>2.042475</td>\n      <td>2.001714</td>\n      <td>2.107105</td>\n      <td>2.159589</td>\n      <td>1.629291</td>\n      <td>2.059725</td>\n      <td>1.703615</td>\n      <td>1.834828</td>\n      <td>1.407918</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>24.000000</td>\n      <td>-5.513000</td>\n      <td>-5.737000</td>\n      <td>-9.104000</td>\n      <td>-5.998000</td>\n      <td>-6.369000</td>\n      <td>-10.000000</td>\n      <td>-10.000000</td>\n      <td>-10.000000</td>\n      <td>-10.000000</td>\n      <td>...</td>\n      <td>-10.000000</td>\n      <td>-10.000000</td>\n      <td>-10.000000</td>\n      <td>-10.000000</td>\n      <td>-10.000000</td>\n      <td>-10.000000</td>\n      <td>-10.000000</td>\n      <td>-10.000000</td>\n      <td>-10.000000</td>\n      <td>-10.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>24.000000</td>\n      <td>-0.473075</td>\n      <td>-0.562200</td>\n      <td>-0.437750</td>\n      <td>-0.429575</td>\n      <td>-0.470925</td>\n      <td>-0.602225</td>\n      <td>-0.493900</td>\n      <td>-0.525175</td>\n      <td>-0.511675</td>\n      <td>...</td>\n      <td>-0.566175</td>\n      <td>-0.565975</td>\n      <td>-0.589975</td>\n      <td>-0.568700</td>\n      <td>-0.563775</td>\n      <td>-0.567975</td>\n      <td>-0.552575</td>\n      <td>-0.561000</td>\n      <td>-0.592600</td>\n      <td>-0.562900</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>48.000000</td>\n      <td>-0.008850</td>\n      <td>-0.046600</td>\n      <td>0.075200</td>\n      <td>0.008050</td>\n      <td>-0.026900</td>\n      <td>-0.015650</td>\n      <td>-0.000650</td>\n      <td>-0.017900</td>\n      <td>0.010000</td>\n      <td>...</td>\n      <td>-0.009900</td>\n      <td>0.003250</td>\n      <td>-0.009100</td>\n      <td>-0.013750</td>\n      <td>-0.003300</td>\n      <td>-0.010250</td>\n      <td>-0.001250</td>\n      <td>-0.006800</td>\n      <td>0.014000</td>\n      <td>-0.019500</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>72.000000</td>\n      <td>0.525700</td>\n      <td>0.403075</td>\n      <td>0.663925</td>\n      <td>0.463400</td>\n      <td>0.465375</td>\n      <td>0.510425</td>\n      <td>0.528725</td>\n      <td>0.411900</td>\n      <td>0.549225</td>\n      <td>...</td>\n      <td>0.457750</td>\n      <td>0.461500</td>\n      <td>0.445675</td>\n      <td>0.452900</td>\n      <td>0.470900</td>\n      <td>0.444750</td>\n      <td>0.465225</td>\n      <td>0.446400</td>\n      <td>0.461275</td>\n      <td>0.438650</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>72.000000</td>\n      <td>10.000000</td>\n      <td>5.039000</td>\n      <td>8.257000</td>\n      <td>10.000000</td>\n      <td>10.000000</td>\n      <td>7.282000</td>\n      <td>7.333000</td>\n      <td>5.473000</td>\n      <td>8.887000</td>\n      <td>...</td>\n      <td>4.069000</td>\n      <td>3.960000</td>\n      <td>3.927000</td>\n      <td>3.596000</td>\n      <td>3.747000</td>\n      <td>2.814000</td>\n      <td>3.505000</td>\n      <td>2.924000</td>\n      <td>3.111000</td>\n      <td>3.805000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 873 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"genes = [column for column in train_features.columns if column.startswith(\"g-\")]\ncells = [column for column in train_features.columns if column.startswith(\"c-\")]","metadata":{"execution":{"iopub.status.busy":"2022-12-30T14:53:08.420751Z","iopub.execute_input":"2022-12-30T14:53:08.421185Z","iopub.status.idle":"2022-12-30T14:53:08.428969Z","shell.execute_reply.started":"2022-12-30T14:53:08.421150Z","shell.execute_reply":"2022-12-30T14:53:08.427914Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_features.describe(exclude=\"number\")","metadata":{"execution":{"iopub.status.busy":"2022-12-30T09:36:18.412287Z","iopub.execute_input":"2022-12-30T09:36:18.412999Z","iopub.status.idle":"2022-12-30T09:36:18.445991Z","shell.execute_reply.started":"2022-12-30T09:36:18.412957Z","shell.execute_reply":"2022-12-30T09:36:18.445059Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"              sig_id cp_type cp_dose\ncount          23814   23814   23814\nunique         23814       2       2\ntop     id_000644bb2  trt_cp      D1\nfreq               1   21948   12147","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sig_id</th>\n      <th>cp_type</th>\n      <th>cp_dose</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>23814</td>\n      <td>23814</td>\n      <td>23814</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>23814</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>id_000644bb2</td>\n      <td>trt_cp</td>\n      <td>D1</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>1</td>\n      <td>21948</td>\n      <td>12147</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_drug.drug_id.nunique()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T09:36:18.447796Z","iopub.execute_input":"2022-12-30T09:36:18.448115Z","iopub.status.idle":"2022-12-30T09:36:18.456474Z","shell.execute_reply.started":"2022-12-30T09:36:18.448083Z","shell.execute_reply":"2022-12-30T09:36:18.455397Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"3289"},"metadata":{}}]},{"cell_type":"code","source":"train_features.cp_type = train_features.cp_type.map({'trt_cp': 0, 'ctl_vehicle': 1})\ntrain_features.cp_dose = train_features.cp_dose.map({\"D1\": 0, \"D2\": 1})","metadata":{"execution":{"iopub.status.busy":"2022-12-30T14:50:33.529247Z","iopub.execute_input":"2022-12-30T14:50:33.529671Z","iopub.status.idle":"2022-12-30T14:50:33.550741Z","shell.execute_reply.started":"2022-12-30T14:50:33.529634Z","shell.execute_reply":"2022-12-30T14:50:33.549665Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# train_drug = train_drug.merge(train_features, on=\"sig_id\", how=\"left\")","metadata":{"execution":{"iopub.status.busy":"2022-12-27T08:44:58.436153Z","iopub.execute_input":"2022-12-27T08:44:58.436540Z","iopub.status.idle":"2022-12-27T08:44:58.664499Z","shell.execute_reply.started":"2022-12-27T08:44:58.436503Z","shell.execute_reply":"2022-12-27T08:44:58.663497Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_features.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T09:36:21.776775Z","iopub.execute_input":"2022-12-30T09:36:21.777200Z","iopub.status.idle":"2022-12-30T09:36:21.806113Z","shell.execute_reply.started":"2022-12-30T09:36:21.777165Z","shell.execute_reply":"2022-12-30T09:36:21.805126Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"         sig_id  cp_type  cp_time  cp_dose     g-0     g-1     g-2     g-3  \\\n0  id_000644bb2        0       24        0  1.0620  0.5577 -0.2479 -0.6208   \n1  id_000779bfc        0       72        0  0.0743  0.4087  0.2991  0.0604   \n2  id_000a6266a        0       48        0  0.6280  0.5817  1.5540 -0.0764   \n3  id_0015fd391        0       48        0 -0.5138 -0.2491 -0.2656  0.5288   \n4  id_001626bd3        0       72        1 -0.3254 -0.4009  0.9700  0.6919   \n\n      g-4     g-5  ...    c-90    c-91    c-92    c-93    c-94    c-95  \\\n0 -0.1944 -1.0120  ...  0.2862  0.2584  0.8076  0.5523 -0.1912  0.6584   \n1  1.0190  0.5207  ... -0.4265  0.7543  0.4708  0.0230  0.2957  0.4899   \n2 -0.0323  1.2390  ... -0.7250 -0.6297  0.6103  0.0223 -1.3240 -0.3174   \n3  4.0620 -0.8095  ... -2.0990 -0.6441 -5.6300 -1.3780 -0.8632 -1.2880   \n4  1.4180 -0.8244  ...  0.0042  0.0048  0.6670  1.0690  0.5523 -0.3031   \n\n     c-96    c-97    c-98    c-99  \n0 -0.3981  0.2139  0.3801  0.4176  \n1  0.1522  0.1241  0.6077  0.7371  \n2 -0.6417 -0.2187 -1.4080  0.6931  \n3 -1.6210 -0.8784 -0.3876 -0.8154  \n4  0.1094  0.2885 -0.3786  0.7125  \n\n[5 rows x 876 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sig_id</th>\n      <th>cp_type</th>\n      <th>cp_time</th>\n      <th>cp_dose</th>\n      <th>g-0</th>\n      <th>g-1</th>\n      <th>g-2</th>\n      <th>g-3</th>\n      <th>g-4</th>\n      <th>g-5</th>\n      <th>...</th>\n      <th>c-90</th>\n      <th>c-91</th>\n      <th>c-92</th>\n      <th>c-93</th>\n      <th>c-94</th>\n      <th>c-95</th>\n      <th>c-96</th>\n      <th>c-97</th>\n      <th>c-98</th>\n      <th>c-99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_000644bb2</td>\n      <td>0</td>\n      <td>24</td>\n      <td>0</td>\n      <td>1.0620</td>\n      <td>0.5577</td>\n      <td>-0.2479</td>\n      <td>-0.6208</td>\n      <td>-0.1944</td>\n      <td>-1.0120</td>\n      <td>...</td>\n      <td>0.2862</td>\n      <td>0.2584</td>\n      <td>0.8076</td>\n      <td>0.5523</td>\n      <td>-0.1912</td>\n      <td>0.6584</td>\n      <td>-0.3981</td>\n      <td>0.2139</td>\n      <td>0.3801</td>\n      <td>0.4176</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id_000779bfc</td>\n      <td>0</td>\n      <td>72</td>\n      <td>0</td>\n      <td>0.0743</td>\n      <td>0.4087</td>\n      <td>0.2991</td>\n      <td>0.0604</td>\n      <td>1.0190</td>\n      <td>0.5207</td>\n      <td>...</td>\n      <td>-0.4265</td>\n      <td>0.7543</td>\n      <td>0.4708</td>\n      <td>0.0230</td>\n      <td>0.2957</td>\n      <td>0.4899</td>\n      <td>0.1522</td>\n      <td>0.1241</td>\n      <td>0.6077</td>\n      <td>0.7371</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id_000a6266a</td>\n      <td>0</td>\n      <td>48</td>\n      <td>0</td>\n      <td>0.6280</td>\n      <td>0.5817</td>\n      <td>1.5540</td>\n      <td>-0.0764</td>\n      <td>-0.0323</td>\n      <td>1.2390</td>\n      <td>...</td>\n      <td>-0.7250</td>\n      <td>-0.6297</td>\n      <td>0.6103</td>\n      <td>0.0223</td>\n      <td>-1.3240</td>\n      <td>-0.3174</td>\n      <td>-0.6417</td>\n      <td>-0.2187</td>\n      <td>-1.4080</td>\n      <td>0.6931</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id_0015fd391</td>\n      <td>0</td>\n      <td>48</td>\n      <td>0</td>\n      <td>-0.5138</td>\n      <td>-0.2491</td>\n      <td>-0.2656</td>\n      <td>0.5288</td>\n      <td>4.0620</td>\n      <td>-0.8095</td>\n      <td>...</td>\n      <td>-2.0990</td>\n      <td>-0.6441</td>\n      <td>-5.6300</td>\n      <td>-1.3780</td>\n      <td>-0.8632</td>\n      <td>-1.2880</td>\n      <td>-1.6210</td>\n      <td>-0.8784</td>\n      <td>-0.3876</td>\n      <td>-0.8154</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id_001626bd3</td>\n      <td>0</td>\n      <td>72</td>\n      <td>1</td>\n      <td>-0.3254</td>\n      <td>-0.4009</td>\n      <td>0.9700</td>\n      <td>0.6919</td>\n      <td>1.4180</td>\n      <td>-0.8244</td>\n      <td>...</td>\n      <td>0.0042</td>\n      <td>0.0048</td>\n      <td>0.6670</td>\n      <td>1.0690</td>\n      <td>0.5523</td>\n      <td>-0.3031</td>\n      <td>0.1094</td>\n      <td>0.2885</td>\n      <td>-0.3786</td>\n      <td>0.7125</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 876 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"targets[(targets.drop(\"sig_id\", axis=1) < 1) & (targets.drop(\"sig_id\", axis=1) > 0)].isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T09:36:28.039215Z","iopub.execute_input":"2022-12-30T09:36:28.039684Z","iopub.status.idle":"2022-12-30T09:36:28.642121Z","shell.execute_reply.started":"2022-12-30T09:36:28.039648Z","shell.execute_reply":"2022-12-30T09:36:28.641057Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"sig_id                                   23814\n5-alpha_reductase_inhibitor              23814\n11-beta-hsd1_inhibitor                   23814\nacat_inhibitor                           23814\nacetylcholine_receptor_agonist           23814\n                                         ...  \nubiquitin_specific_protease_inhibitor    23814\nvegfr_inhibitor                          23814\nvitamin_b                                23814\nvitamin_d_receptor_agonist               23814\nwnt_inhibitor                            23814\nLength: 207, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"targets.set_index(\"sig_id\", inplace=True)\ntrain_features.set_index(\"sig_id\", inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T14:50:33.551946Z","iopub.execute_input":"2022-12-30T14:50:33.552346Z","iopub.status.idle":"2022-12-30T14:50:33.560031Z","shell.execute_reply.started":"2022-12-30T14:50:33.552308Z","shell.execute_reply":"2022-12-30T14:50:33.559015Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# optionally\n# remove all zero rows, so that we have only scored labels\n# expand multilabel rows, so that every row contains only one label, thus crossentropy\n\nrows, indexes = [], []\nfor i, row in targets.iterrows():\n    if np.sum(row) > 1:\n        idxs = np.nonzero(row.values)\n        for j in range(len(idxs)):\n            new_row = np.zeros(len(row))\n            new_row[j] = 1\n            rows.append(new_row)\n            indexes.append(row.name)\n    elif np.sum(row) == 1:\n        rows.append(row.values)\n        indexes.append(row.name)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T09:36:34.618966Z","iopub.execute_input":"2022-12-30T09:36:34.619342Z","iopub.status.idle":"2022-12-30T09:36:38.222423Z","shell.execute_reply.started":"2022-12-30T09:36:34.619310Z","shell.execute_reply":"2022-12-30T09:36:38.221323Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"new_targets = pd.DataFrame(np.array(rows), columns=targets.columns.tolist(), index=indexes).astype(np.int32)\nnew_train = train_features[train_features.index.isin(new_targets.index)]","metadata":{"execution":{"iopub.status.busy":"2022-12-30T09:36:38.224699Z","iopub.execute_input":"2022-12-30T09:36:38.226444Z","iopub.status.idle":"2022-12-30T09:36:38.344986Z","shell.execute_reply.started":"2022-12-30T09:36:38.226399Z","shell.execute_reply":"2022-12-30T09:36:38.343958Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"new_targets_flat = new_targets.idxmax(axis=1).map({v: i for i, v in enumerate(targets.columns)}).astype(np.int32)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T09:36:38.346703Z","iopub.execute_input":"2022-12-30T09:36:38.347080Z","iopub.status.idle":"2022-12-30T09:36:38.383128Z","shell.execute_reply.started":"2022-12-30T09:36:38.347045Z","shell.execute_reply":"2022-12-30T09:36:38.382185Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"k = int(np.sqrt(targets.shape[1]))\n\n# sketch = RandomProjectionSketch(k)  # second order derivative: use_hess=False\nsketch = RandomSamplingSketch(k)  # use_hess=True\n# sketch = TopOutputsSketch(k)\n# sketch = SVDSketch(n_components=k)\n\nparams = {\"ntrees\": 6000,  # number of trees (base estimators)\n          \"lr\": 0.01, # every tree multiplier\n          \"verbose\": 600, # print loss value after every 500 trees are constructed\n          \"es\": 100,  # stop training, when loss didn't improve during 100 trees\n          \"lambda_l2\": 10, # l2 regularization: sum(grads)*idx / (sum(hess) + lambda)\n          \"gd_steps\": 1, # number of newton iterations\n          \"subsample\": 0.6,  # sample 0.6 part of training data for each tree\n          \"colsample\": 0.8,  # sample 0.8 number of features per tree / tree level\n          \"min_data_in_leaf\": 20,  # min data samples in leaf (prevents overfitting)\n          \"use_hess\": True,  # use second derivative in scoring function when constructing tree, False - to speedup\n          \"max_bin\": 32,  # group feature values into bins to speedup optimal tree structure searching \n          \"max_depth\": 3,  # maximum depth of each tree to prevent overfitting\n          \"multioutput_sketch\": sketch  # type of output dimension reduction: sort, prob sample or projection\n         }  ","metadata":{"execution":{"iopub.status.busy":"2022-12-30T14:50:33.561316Z","iopub.execute_input":"2022-12-30T14:50:33.561818Z","iopub.status.idle":"2022-12-30T14:50:33.570829Z","shell.execute_reply.started":"2022-12-30T14:50:33.561779Z","shell.execute_reply":"2022-12-30T14:50:33.569874Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def sketch_cross_validation(params, X, y, cv, categorical=None, verbose=True, transform=False):\n    estimators, encoders = [], {}\n    folds_scores, train_scores = [], []\n    oof_preds = np.zeros((X.shape[0], y.shape[1]))\n    \n    if categorical is not None:\n        for feature in categorical:\n            encoder = LabelEncoder()\n            X[feature] = encoder.fit_transform(X[feature])\n            encoders[feature] = encoder\n            \n    if transform:  # use all data statistics [which is not quite honest: data leakage]\n        # such tranforms may be more suitable for models that use optimization approaches (e.g. neural networks)\n        # DCNN architecture would well fit this task\n        X = X.copy(deep=True)\n        pipeline = make_pipeline(QuantileTransformer(n_quantiles=100, output_distribution=\"normal\", random_state=42),\n                                 PCA(n_components=600, random_state=42))\n        xgenes = pd.DataFrame(pipeline.fit_transform(X[genes]), \n                              columns=[f\"G_{i}\" for i in range(1, pipeline[-1].n_components_+1)],\n                              index=X.index)\n\n        pipeline = make_pipeline(QuantileTransformer(n_quantiles=100, output_distribution=\"normal\", random_state=42),\n                                 PCA(n_components=50, random_state=42))\n        xcells = pd.DataFrame(pipeline.fit_transform(X[cells]), \n                              columns=[f\"G_{i}\" for i in range(1, pipeline[-1].n_components_+1)],\n                              index=X.index)\n\n        X.drop(genes+cells, axis=1, inplace=True)\n        X = pd.concat([X, xgenes, xcells], axis=1)\n\n    if verbose:\n        print(f\"{time.ctime()}, Cross-Validation, {X.shape[0]} rows, {X.shape[1]} cols\")\n\n    for fold, (train_idx, valid_idx) in enumerate(cv.split(X, y)):\n\n        x_train, x_valid = X.iloc[train_idx, :], X.iloc[valid_idx, :]\n        y_train, y_valid = y.iloc[train_idx, :], y.iloc[valid_idx, :]\n        \n        # here define honest (non-shifted) encoder\n        # encoder = ce.cat_boost.CatBoostEncoder(a=3.0, random_state=42)  # CatBoost encoding - drug_id\n        # x_train[feature] = encoder.fit_transform(x_train[feature], y_train.idxmax(1).astype(np.int32))\n        # x_valid[feature] = encoder.transform(x_valid[feature])\n        # encoders.update\n\n        # predict each prob with sigmoid: [N, M], otherwise \"crossentropy\" [N, ] having target [1,N] - each row labeled [0,num_classes)\n        # model = GradientBoosting(\"multilabel\", **params)  # oof-score+projection: 0.01586 \n        \n        model = GradientBoosting(CustomLoss(), CustomMetric(), **params)\n        model.fit(x_train.values.astype(np.float32), y_train.values.astype(np.int32), \n                  eval_sets=[{'X': x_valid.values.astype(np.float32), 'y': y_valid.values.astype(np.int32)}])\n        \n        train_pred = model.predict(x_train.values.astype(np.float32))\n        train_score = multi_logloss(y_train.values, train_pred, activation=False)\n        \n        train_auc_score = []\n        for i in range(y_train.shape[1]):  # cv takes some all-zero classes\n            if np.any(y_train.values[:, i]):\n                auc_score = roc_auc_score(y_true=y_train.values[:, i], y_score=train_pred[:, i])\n            else:\n                auc_score = 0  # or random = 0.5\n            train_auc_score.append(auc_score)\n        train_auc_score = np.mean(train_auc_score)\n\n        oof_preds[valid_idx] = model.predict(x_valid.values.astype(np.float32))\n        score = multi_logloss(y_valid.values, oof_preds[valid_idx], activation=False)\n        \n        valid_auc_score = []\n        for i in range(y_valid.shape[1]):\n            if np.any(y_valid.values[:, i]):\n                auc_score = roc_auc_score(y_true=y_valid.values[:, i], y_score=oof_preds[valid_idx, i])\n            else:\n                auc_score = 0\n            valid_auc_score.append(auc_score)\n        valid_auc_score = np.mean(valid_auc_score)\n        \n        folds_scores.append(round(score, 5))\n        train_scores.append(round(train_score, 5))\n        \n        if verbose:\n            print(f\"Fold {fold + 1}, Train score = {train_score:.5f}, Valid score = {score:.5f}\")\n            print(f\"\\tTrain roc_auc: {train_auc_score:.5f}, valid roc_auc_score: {valid_auc_score:.5f}\")\n        estimators.append(model)\n\n    if verbose:\n        print_scores(folds_scores, train_scores)\n        print(f\"OOF-score: {multi_logloss(y.values, oof_preds, activation=False):.5f}\")\n\n    return estimators, encoders, oof_preds","metadata":{"execution":{"iopub.status.busy":"2022-12-30T14:56:56.506778Z","iopub.execute_input":"2022-12-30T14:56:56.507166Z","iopub.status.idle":"2022-12-30T14:56:56.529966Z","shell.execute_reply.started":"2022-12-30T14:56:56.507134Z","shell.execute_reply":"2022-12-30T14:56:56.528798Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"cv = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # [N,M] - thus labels are not mutualy exclusive\n\nestimators, encoders, oof_preds = sketch_cross_validation(params, train_features, targets, cv, categorical=None)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T13:45:12.946033Z","iopub.execute_input":"2022-12-30T13:45:12.946401Z","iopub.status.idle":"2022-12-30T14:05:40.700260Z","shell.execute_reply.started":"2022-12-30T13:45:12.946370Z","shell.execute_reply":"2022-12-30T14:05:40.699043Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Fri Dec 30 13:45:12 2022, Cross-Validation, 23814 rows, 875 cols\n[13:45:16] Stdout logging level is INFO.\n[13:45:16] GDBT train starts. Max iter 6000, early stopping rounds 100\n[13:45:16] Iter 0; Sample 0, score = 0.02078499928852655; \n[13:45:39] Iter 600; Sample 0, score = 0.017289330252865654; \n[13:46:02] Iter 1200; Sample 0, score = 0.016782122382183476; \n[13:46:24] Iter 1800; Sample 0, score = 0.016497246722867044; \n[13:46:47] Iter 2400; Sample 0, score = 0.01630995624930399; \n[13:47:09] Iter 3000; Sample 0, score = 0.016174393673905305; \n[13:47:32] Iter 3600; Sample 0, score = 0.01607290882531078; \n[13:47:54] Iter 4200; Sample 0, score = 0.01599172496911858; \n[13:48:17] Iter 4800; Sample 0, score = 0.015926424347597605; \n[13:48:40] Iter 5400; Sample 0, score = 0.015871727017801658; \n[13:49:02] Iter 5999; Sample 0, score = 0.01582718617569896; \nFold 1, Train score = 0.01228, Valid score = 0.01583\n\tTrain roc_auc: 0.97338, valid roc_auc_score: 0.72167\n[13:49:20] Stdout logging level is INFO.\n[13:49:20] GDBT train starts. Max iter 6000, early stopping rounds 100\n[13:49:20] Iter 0; Sample 0, score = 0.020828298884072918; \n[13:49:43] Iter 600; Sample 0, score = 0.01742255277450586; \n[13:50:06] Iter 1200; Sample 0, score = 0.016928682241799095; \n[13:50:28] Iter 1800; Sample 0, score = 0.01665925829047007; \n[13:50:51] Iter 2400; Sample 0, score = 0.01647848952000952; \n[13:51:14] Iter 3000; Sample 0, score = 0.016341748534319484; \n[13:51:36] Iter 3600; Sample 0, score = 0.016243127678466102; \n[13:51:59] Iter 4200; Sample 0, score = 0.016164661282851378; \n[13:52:21] Iter 4800; Sample 0, score = 0.01609625264242139; \n[13:52:44] Iter 5400; Sample 0, score = 0.01604074516622825; \n[13:53:07] Iter 5999; Sample 0, score = 0.01598939011427946; \nFold 2, Train score = 0.01226, Valid score = 0.01599\n\tTrain roc_auc: 0.97276, valid roc_auc_score: 0.73522\n[13:53:25] Stdout logging level is INFO.\n[13:53:25] GDBT train starts. Max iter 6000, early stopping rounds 100\n[13:53:25] Iter 0; Sample 0, score = 0.020767686704497686; \n[13:53:47] Iter 600; Sample 0, score = 0.01734516071655969; \n[13:54:10] Iter 1200; Sample 0, score = 0.016831509999213352; \n[13:54:33] Iter 1800; Sample 0, score = 0.016552560424486506; \n[13:54:55] Iter 2400; Sample 0, score = 0.01635729464047821; \n[13:55:18] Iter 3000; Sample 0, score = 0.016209269769669066; \n[13:55:40] Iter 3600; Sample 0, score = 0.01610660595174045; \n[13:56:03] Iter 4200; Sample 0, score = 0.016022324179012427; \n[13:56:26] Iter 4800; Sample 0, score = 0.015953090554812564; \n[13:56:48] Iter 5400; Sample 0, score = 0.01589591572897937; \n[13:57:11] Iter 5999; Sample 0, score = 0.015848144387814025; \nFold 3, Train score = 0.01226, Valid score = 0.01585\n\tTrain roc_auc: 0.97712, valid roc_auc_score: 0.73770\n[13:57:29] Stdout logging level is INFO.\n[13:57:29] GDBT train starts. Max iter 6000, early stopping rounds 100\n[13:57:29] Iter 0; Sample 0, score = 0.020646086207718056; \n[13:57:52] Iter 600; Sample 0, score = 0.01712455204340616; \n[13:58:14] Iter 1200; Sample 0, score = 0.01659707175752646; \n[13:58:37] Iter 1800; Sample 0, score = 0.016302162965457462; \n[13:58:59] Iter 2400; Sample 0, score = 0.01610781727615944; \n[13:59:22] Iter 3000; Sample 0, score = 0.01596364632665346; \n[13:59:45] Iter 3600; Sample 0, score = 0.015856100651924963; \n[14:00:07] Iter 4200; Sample 0, score = 0.015766730398329033; \n[14:00:30] Iter 4800; Sample 0, score = 0.015690713036623852; \n[14:00:52] Iter 5400; Sample 0, score = 0.01563144994632459; \n[14:01:15] Iter 5999; Sample 0, score = 0.015579916207607977; \nFold 4, Train score = 0.01229, Valid score = 0.01558\n\tTrain roc_auc: 0.97637, valid roc_auc_score: 0.73627\n[14:01:32] Stdout logging level is INFO.\n[14:01:32] GDBT train starts. Max iter 6000, early stopping rounds 100\n[14:01:32] Iter 0; Sample 0, score = 0.020733143153718582; \n[14:01:55] Iter 600; Sample 0, score = 0.017276126439792754; \n[14:02:18] Iter 1200; Sample 0, score = 0.01675671970296351; \n[14:02:40] Iter 1800; Sample 0, score = 0.01646860575733434; \n[14:03:03] Iter 2400; Sample 0, score = 0.01627310717996844; \n[14:03:25] Iter 3000; Sample 0, score = 0.01613185029092244; \n[14:03:48] Iter 3600; Sample 0, score = 0.016028117157755026; \n[14:04:10] Iter 4200; Sample 0, score = 0.015947818089097104; \n[14:04:33] Iter 4800; Sample 0, score = 0.01588190988605945; \n[14:04:56] Iter 5400; Sample 0, score = 0.01582649246962898; \n[14:05:18] Iter 5999; Sample 0, score = 0.015779283250912556; \nFold 5, Train score = 0.01228, Valid score = 0.01578\n\tTrain roc_auc: 0.97786, valid roc_auc_score: 0.73116\nTrain score by each fold: [0.01228, 0.01226, 0.01226, 0.01229, 0.01228]\nValid score by each fold: [0.01583, 0.01599, 0.01585, 0.01558, 0.01578]\nTrain mean score by each fold:0.01227 +/- 0.00001\nValid mean score by each fold:0.01581 +/- 0.00013\n**************************************************\nOOF-score: 0.01580\n","output_type":"stream"}]},{"cell_type":"code","source":"cv = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nestimators, encoders, oof_preds = sketch_cross_validation(params, train_features, targets, cv, categorical=None, transform=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T14:56:58.503917Z","iopub.execute_input":"2022-12-30T14:56:58.504613Z","iopub.status.idle":"2022-12-30T15:14:47.236295Z","shell.execute_reply.started":"2022-12-30T14:56:58.504574Z","shell.execute_reply":"2022-12-30T15:14:47.235062Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Fri Dec 30 14:57:16 2022, Cross-Validation, 23814 rows, 653 cols\n[14:57:30] Stdout logging level is INFO.\n[14:57:30] GDBT train starts. Max iter 6000, early stopping rounds 100\n[14:57:47] Iter 0; Sample 0, score = 0.020788348468649374; \n[14:58:06] Iter 600; Sample 0, score = 0.017952960191992186; \n[14:58:24] Iter 1200; Sample 0, score = 0.01741784492316739; \n[14:58:43] Iter 1800; Sample 0, score = 0.017144145117571223; \n[14:59:01] Iter 2400; Sample 0, score = 0.01697146836599343; \n[14:59:20] Iter 3000; Sample 0, score = 0.01684484400887352; \n[14:59:38] Iter 3600; Sample 0, score = 0.01673899862728735; \n[14:59:57] Iter 4200; Sample 0, score = 0.016652159715913703; \n[15:00:15] Iter 4800; Sample 0, score = 0.016580531441676653; \n[15:00:34] Iter 5400; Sample 0, score = 0.016521525770064414; \n[15:00:52] Iter 5999; Sample 0, score = 0.01646595360942152; \nFold 1, Train score = 0.01247, Valid score = 0.01647\n\tTrain roc_auc: 0.98655, valid roc_auc_score: 0.70969\n[15:01:11] Stdout logging level is INFO.\n[15:01:11] GDBT train starts. Max iter 6000, early stopping rounds 100\n[15:01:11] Iter 0; Sample 0, score = 0.020827834532369263; \n[15:01:30] Iter 600; Sample 0, score = 0.01803023147196664; \n[15:01:49] Iter 1200; Sample 0, score = 0.017498427801294326; \n[15:02:07] Iter 1800; Sample 0, score = 0.01722140921947112; \n[15:02:26] Iter 2400; Sample 0, score = 0.017048605088414225; \n[15:02:44] Iter 3000; Sample 0, score = 0.016916059567408465; \n[15:03:03] Iter 3600; Sample 0, score = 0.016808480974170925; \n[15:03:21] Iter 4200; Sample 0, score = 0.016718229538096083; \n[15:03:40] Iter 4800; Sample 0, score = 0.016645310706594595; \n[15:03:58] Iter 5400; Sample 0, score = 0.016583074943958692; \n[15:04:17] Iter 5999; Sample 0, score = 0.016533311439871537; \nFold 2, Train score = 0.01245, Valid score = 0.01653\n\tTrain roc_auc: 0.98613, valid roc_auc_score: 0.71567\n[15:04:35] Stdout logging level is INFO.\n[15:04:35] GDBT train starts. Max iter 6000, early stopping rounds 100\n[15:04:35] Iter 0; Sample 0, score = 0.020763901910898438; \n[15:04:54] Iter 600; Sample 0, score = 0.017938267461401737; \n[15:05:12] Iter 1200; Sample 0, score = 0.017423023524668062; \n[15:05:31] Iter 1800; Sample 0, score = 0.017152629357521012; \n[15:05:49] Iter 2400; Sample 0, score = 0.016979921597985767; \n[15:06:08] Iter 3000; Sample 0, score = 0.016849587497521596; \n[15:06:26] Iter 3600; Sample 0, score = 0.016740811599735863; \n[15:06:45] Iter 4200; Sample 0, score = 0.016650923142837892; \n[15:07:04] Iter 4800; Sample 0, score = 0.016576806387767245; \n[15:07:22] Iter 5400; Sample 0, score = 0.016510734193939006; \n[15:07:41] Iter 5999; Sample 0, score = 0.01645937488851595; \nFold 3, Train score = 0.01247, Valid score = 0.01646\n\tTrain roc_auc: 0.98637, valid roc_auc_score: 0.71992\n[15:07:59] Stdout logging level is INFO.\n[15:07:59] GDBT train starts. Max iter 6000, early stopping rounds 100\n[15:07:59] Iter 0; Sample 0, score = 0.020688684834222985; \n[15:08:18] Iter 600; Sample 0, score = 0.017764007089827085; \n[15:08:37] Iter 1200; Sample 0, score = 0.01721612297817511; \n[15:08:55] Iter 1800; Sample 0, score = 0.016938892164253542; \n[15:09:14] Iter 2400; Sample 0, score = 0.01675209228225858; \n[15:09:32] Iter 3000; Sample 0, score = 0.016607417493050194; \n[15:09:51] Iter 3600; Sample 0, score = 0.016500215134238657; \n[15:10:10] Iter 4200; Sample 0, score = 0.016412709167446186; \n[15:10:28] Iter 4800; Sample 0, score = 0.016339964249125864; \n[15:10:47] Iter 5400; Sample 0, score = 0.016274089079580972; \n[15:11:05] Iter 5999; Sample 0, score = 0.016219939276504114; \nFold 4, Train score = 0.01256, Valid score = 0.01622\n\tTrain roc_auc: 0.98422, valid roc_auc_score: 0.71539\n[15:11:24] Stdout logging level is INFO.\n[15:11:24] GDBT train starts. Max iter 6000, early stopping rounds 100\n[15:11:24] Iter 0; Sample 0, score = 0.020733328851676426; \n[15:11:42] Iter 600; Sample 0, score = 0.01788661928050666; \n[15:12:01] Iter 1200; Sample 0, score = 0.01734281315926743; \n[15:12:20] Iter 1800; Sample 0, score = 0.017067806067725934; \n[15:12:38] Iter 2400; Sample 0, score = 0.016894760616672416; \n[15:12:57] Iter 3000; Sample 0, score = 0.01676176480596467; \n[15:13:15] Iter 3600; Sample 0, score = 0.016652318578159804; \n[15:13:34] Iter 4200; Sample 0, score = 0.016561957445290616; \n[15:13:52] Iter 4800; Sample 0, score = 0.016492567556355656; \n[15:14:11] Iter 5400; Sample 0, score = 0.01643145243221776; \n[15:14:30] Iter 5999; Sample 0, score = 0.016376639809938615; \nFold 5, Train score = 0.01250, Valid score = 0.01638\n\tTrain roc_auc: 0.98504, valid roc_auc_score: 0.71248\nTrain score by each fold: [0.01247, 0.01245, 0.01247, 0.01256, 0.0125]\nValid score by each fold: [0.01647, 0.01653, 0.01646, 0.01622, 0.01638]\nTrain mean score by each fold:0.01249 +/- 0.00004\nValid mean score by each fold:0.01641 +/- 0.00011\n**************************************************\nOOF-score: 0.01641\n","output_type":"stream"}]},{"cell_type":"code","source":"# TODO\n# feature engineering\n\n# mean, kurtosis, skew, square, interactions: separatelly for G nad C: in progress\n# PCA: Genes_comp = 600, cells_comp = 50 + QuantileTransform (normal distribution) / StandardScaler","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CI on oof-score\n\ndef create_bootstrap_samples(data: np.array, n_samples: int = 1000) -> np.array:\n    bootstrap_idx = np.random.randint(\n        low=0, high=len(data), size=(n_samples, len(data))\n    )\n    return bootstrap_idx\n\n\ndef create_bootstrap_metrics(y_true: np.array,\n                             y_pred: np.array,\n                             metric: callable,\n                             n_samlpes: int = 1000) -> list:\n    scores = []\n\n    if isinstance(y_true, pd.Series):\n        y_true = y_true.values\n\n    bootstrap_idx = create_bootstrap_samples(y_true)\n    for idx in bootstrap_idx:\n        y_true_bootstrap = y_true[idx]\n        y_pred_bootstrap = y_pred[idx]\n\n        score = metric(y_true_bootstrap, y_pred_bootstrap)\n        scores.append(score)\n\n    return scores\n\n\ndef calculate_confidence_interval(scores: list, conf_interval: float = 0.95) -> tuple:\n    left_bound = np.percentile(\n        scores, ((1 - conf_interval) / 2) * 100\n    )\n    right_bound = np.percentile(\n        scores, (conf_interval + ((1 - conf_interval) / 2)) * 100\n    )\n\n    return left_bound, right_bound","metadata":{"execution":{"iopub.status.busy":"2022-12-27T12:36:26.348204Z","iopub.execute_input":"2022-12-27T12:36:26.348562Z","iopub.status.idle":"2022-12-27T12:36:26.358583Z","shell.execute_reply.started":"2022-12-27T12:36:26.348517Z","shell.execute_reply":"2022-12-27T12:36:26.357576Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# train full model\n\nparams = {\"ntrees\": 6000,\n          \"lr\": 0.01,\n          \"verbose\": -1,\n          \"lambda_l2\": 10,\n          \"gd_steps\": 1,\n          \"subsample\": 0.6,\n          \"colsample\": 0.8,\n          \"min_data_in_leaf\": 20,\n          \"use_hess\": False,\n          \"max_bin\": 32,\n          \"max_depth\": 3,\n          \"multioutput_sketch\": sketch\n         }  \n\n\nmodel = GradientBoosting(\"multilabel\", **params)  \n\n# drug_id was encoded inplace in CV loop / not used as test_features has not such feature\nmodel.fit(train_features.values.astype(np.float32), targets.values.astype(np.int32))","metadata":{"execution":{"iopub.status.busy":"2022-12-27T15:42:11.313192Z","iopub.execute_input":"2022-12-27T15:42:11.313589Z","iopub.status.idle":"2022-12-27T15:44:44.549791Z","shell.execute_reply.started":"2022-12-27T15:42:11.313550Z","shell.execute_reply":"2022-12-27T15:44:44.548815Z"},"trusted":true},"execution_count":73,"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"<py_boost.gpu.boosting.GradientBoosting at 0x7fba57ee5450>"},"metadata":{}}]},{"cell_type":"code","source":"# make bootstrap samples to model metric distribution, select 95% confidence interval by percentile\n# so, if the given model is stable it is expected due to Central Limit Theoreme that metrics are normally distributed\n# and with confidence (probability) of p (here 95%) the truth metric value on inifity size sample would be in \n# inside this range, thus we are expecting to obtain metric in this range on unseen data with equal to train distribution\n\nbootstrap_scores = create_bootstrap_metrics(targets.values, oof_preds, metric=partial(multi_logloss, activation=False))\ncalculate_confidence_interval(bootstrap_scores)","metadata":{"execution":{"iopub.status.busy":"2022-12-27T10:02:26.305926Z","iopub.execute_input":"2022-12-27T10:02:26.306277Z","iopub.status.idle":"2022-12-27T10:06:09.085432Z","shell.execute_reply.started":"2022-12-27T10:02:26.306240Z","shell.execute_reply":"2022-12-27T10:06:09.084384Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"(0.015667531393919795, 0.01604594758308204)"},"metadata":{}}]},{"cell_type":"code","source":"# test submission\ntest_features = pd.read_csv(\"/kaggle/input/lish-moa/test_features.csv\")\nsample_sub = pd.read_csv(\"/kaggle/input/lish-moa/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:36:54.042762Z","iopub.execute_input":"2022-12-30T11:36:54.043113Z","iopub.status.idle":"2022-12-30T11:36:54.905878Z","shell.execute_reply.started":"2022-12-30T11:36:54.043083Z","shell.execute_reply":"2022-12-30T11:36:54.904903Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"train_drug[train_drug[\"sig_id\"].isin(test_features.sig_id)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_features.cp_type = test_features.cp_type.map({'trt_cp': 0, 'ctl_vehicle': 1})\ntest_features.cp_dose = test_features.cp_dose.map({\"D1\": 0, \"D2\": 1})\ntest_features.set_index(\"sig_id\", inplace=True)\n\n# t = pd.read_csv(\"/kaggle/input/lish-moa/train_drug.csv\")[\"drug_id\"]\n# encoders[\"drug_id\"] = dict(zip(t.values.tolist(), train_drug[\"drug_id\"].values.tolist()))\n# no drug_id?\n# test_features[\"drug_id\"] = test_features[\"drug_id\"].map(encoders[\"drug_id\"])","metadata":{"execution":{"iopub.status.busy":"2022-12-27T15:44:45.471196Z","iopub.execute_input":"2022-12-27T15:44:45.471552Z","iopub.status.idle":"2022-12-27T15:44:45.481766Z","shell.execute_reply.started":"2022-12-27T15:44:45.471517Z","shell.execute_reply":"2022-12-27T15:44:45.480473Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"sample_sub.iloc[:, 1:] = model.predict(test_features.values.astype(np.float32))\nsample_sub.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-27T15:44:45.483704Z","iopub.execute_input":"2022-12-27T15:44:45.484204Z","iopub.status.idle":"2022-12-27T15:44:54.154415Z","shell.execute_reply.started":"2022-12-27T15:44:45.484169Z","shell.execute_reply":"2022-12-27T15:44:54.153365Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"# feature selection: permutation importance\n\ndef calculate_permutation_importance(estimator,\n                                     metric: callable,\n                                     x_valid: np.ndarray,\n                                     y_valid: np.ndarray,\n                                     maximize: bool = False,\n                                     probas: bool = False,\n                                     columns: list = None,\n                                     ) -> pd.Series:\n\n    def _predict(estimator, x_valid, probas=True):\n        if hasattr(estimator, \"predict_proba\") and probas:\n            y_pred = estimator.predict_proba(x_valid)[:, 1]\n        else:\n            y_pred = estimator.predict(x_valid)\n\n        return y_pred\n\n    y_pred = _predict(estimator, x_valid, probas)\n    base_score = metric(y_valid, y_pred)\n    scores, delta = {}, {}\n\n    for i in trange(x_valid.shape[1]):\n        x_valid_ = copy.deepcopy(x_valid)\n        dtype = x_valid_[:, i].dtype\n        np.random.seed(42)\n        x_valid_[:, i] = np.random.permutation(x_valid_[:, i])\n        x_valid_[:, i] = x_valid_[:, i].astype(dtype)\n\n        y_pred = _predict(estimator, x_valid_, probas)\n        feature_score = metric(y_valid, y_pred)\n\n        if maximize:\n            delta[columns[i]] = base_score - feature_score\n        else:\n            delta[columns[i]] = feature_score - base_score\n\n        scores[columns[i]] = feature_score\n\n    scores, delta = pd.Series(scores), pd.Series(delta)\n    scores = scores.sort_values(ascending=False)\n    delta = delta.sort_values(ascending=False)\n\n    return scores, delta","metadata":{"execution":{"iopub.status.busy":"2022-12-27T13:02:51.959662Z","iopub.execute_input":"2022-12-27T13:02:51.960046Z","iopub.status.idle":"2022-12-27T13:02:51.970479Z","shell.execute_reply.started":"2022-12-27T13:02:51.960011Z","shell.execute_reply":"2022-12-27T13:02:51.969290Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"cv = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ntrain_idx, valid_idx = next(iter(cv.split(train_features, targets)))\n\nperm_model = GradientBoosting(\"multilabel\", **params)  \nperm_model.fit(train_features.iloc[train_idx, :].values.astype(np.float32), \n               targets.iloc[train_idx, :].values.astype(np.int32))\n\nscores, deltas = calculate_permutation_importance(perm_model,\n                                                  partial(multi_logloss, activation=False), \n                                                  train_features.iloc[valid_idx, :].values.astype(np.float32),\n                                                  targets.iloc[valid_idx, :].values.astype(np.int32),\n                                                  columns=train_features.columns.tolist())","metadata":{"execution":{"iopub.status.busy":"2022-12-27T13:03:01.572898Z","iopub.execute_input":"2022-12-27T13:03:01.573256Z","iopub.status.idle":"2022-12-27T14:17:44.695417Z","shell.execute_reply.started":"2022-12-27T13:03:01.573224Z","shell.execute_reply":"2022-12-27T14:17:44.693744Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"100%|██████████| 875/875 [1:12:20<00:00,  4.96s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"deltas[deltas>0].to_dict()","metadata":{"execution":{"iopub.status.busy":"2022-12-27T14:25:44.881200Z","iopub.execute_input":"2022-12-27T14:25:44.881557Z","iopub.status.idle":"2022-12-27T14:25:44.908615Z","shell.execute_reply.started":"2022-12-27T14:25:44.881527Z","shell.execute_reply":"2022-12-27T14:25:44.907765Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"{'c-65': 0.000830453615714332,\n 'c-98': 0.00013651671816113792,\n 'cp_type': 0.00012257825485802176,\n 'g-178': 0.00011551029220621331,\n 'g-175': 0.00011403923172112126,\n 'g-100': 9.818649561012943e-05,\n 'g-512': 8.776692989578491e-05,\n 'c-78': 5.9864116332280365e-05,\n 'g-392': 5.828528618574344e-05,\n 'g-75': 5.706459995039395e-05,\n 'g-157': 5.576714383242337e-05,\n 'g-763': 3.7470654855462776e-05,\n 'g-414': 3.702651305615909e-05,\n 'g-300': 3.1518016634746876e-05,\n 'g-91': 2.7966650252835012e-05,\n 'g-231': 2.682532076599825e-05,\n 'g-357': 2.5579478802811167e-05,\n 'g-628': 2.544874871971553e-05,\n 'g-202': 2.4426946446252235e-05,\n 'g-20': 2.3882961607001735e-05,\n 'g-47': 2.2292538744198703e-05,\n 'g-158': 2.080915696544075e-05,\n 'g-22': 1.8461243703402075e-05,\n 'g-635': 1.7415416807196188e-05,\n 'c-30': 1.7189612942999227e-05,\n 'g-332': 1.713347705404114e-05,\n 'g-387': 1.6420359507184068e-05,\n 'g-493': 1.603599642736528e-05,\n 'g-385': 1.5455562984014598e-05,\n 'g-640': 1.5151858036830529e-05,\n 'g-431': 1.509298317266955e-05,\n 'g-639': 1.4459270121933154e-05,\n 'g-570': 1.3641780572049705e-05,\n 'g-117': 1.356610321239654e-05,\n 'g-432': 1.3362127040346394e-05,\n 'g-207': 1.2832676053269737e-05,\n 'g-8': 1.230157576758567e-05,\n 'g-269': 1.1379194347026916e-05,\n 'g-168': 1.1215548591805846e-05,\n 'g-761': 1.1166496447119478e-05,\n 'g-62': 1.105940445621581e-05,\n 'g-122': 1.0888585892494806e-05,\n 'c-51': 1.061140417130818e-05,\n 'g-260': 9.865542941075306e-06,\n 'g-203': 9.382417237378815e-06,\n 'g-167': 8.66700019099692e-06,\n 'g-206': 8.243341467754906e-06,\n 'g-476': 8.140169675232428e-06,\n 'g-443': 7.964273889854434e-06,\n 'g-181': 7.523401139422625e-06,\n 'g-253': 7.376101992553202e-06,\n 'g-28': 7.158045159057214e-06,\n 'g-48': 6.879127785085898e-06,\n 'g-377': 6.84175925151978e-06,\n 'g-156': 6.749778871422557e-06,\n 'g-379': 6.7262846383149555e-06,\n 'g-759': 6.572386364454147e-06,\n 'g-636': 6.561406167636397e-06,\n 'c-53': 6.267643519963784e-06,\n 'g-33': 6.231301576026943e-06,\n 'g-210': 6.200711075644155e-06,\n 'c-1': 6.0329406862402735e-06,\n 'c-26': 5.830041113694118e-06,\n 'g-69': 5.7309724506640625e-06,\n 'g-418': 5.722975869371094e-06,\n 'g-670': 5.718396811696391e-06,\n 'g-272': 5.615976385396626e-06,\n 'g-217': 5.581618749514461e-06,\n 'g-30': 5.512335106597904e-06,\n 'g-441': 5.452972576945603e-06,\n 'g-135': 5.433761607654547e-06,\n 'g-183': 5.4237783494864855e-06,\n 'g-291': 5.37537676047542e-06,\n 'g-248': 5.286634102603938e-06,\n 'g-522': 5.1763305402019455e-06,\n 'c-70': 4.995505165156927e-06,\n 'g-58': 4.822832273664729e-06,\n 'g-569': 4.794995019527515e-06,\n 'g-369': 4.759731469180378e-06,\n 'g-557': 4.685923776413803e-06,\n 'g-235': 4.6289705359255084e-06,\n 'g-597': 4.607866933896421e-06,\n 'g-696': 4.600067639113498e-06,\n 'c-6': 4.566468590407047e-06,\n 'g-533': 4.330740240116038e-06,\n 'g-57': 4.318568418558433e-06,\n 'c-67': 4.297593287252988e-06,\n 'g-51': 4.232340381968985e-06,\n 'g-63': 4.2033625732122715e-06,\n 'g-66': 4.113030394582717e-06,\n 'g-36': 4.044065314529749e-06,\n 'g-152': 4.018221072148165e-06,\n 'g-29': 3.865905525705232e-06,\n 'g-489': 3.7690489333025434e-06,\n 'g-170': 3.706368407994237e-06,\n 'g-208': 3.674033662300763e-06,\n 'g-708': 3.585044800906434e-06,\n 'g-105': 3.5694939415110982e-06,\n 'g-299': 3.5430067440861412e-06,\n 'g-528': 3.494396336758271e-06,\n 'g-41': 3.466028467197929e-06,\n 'g-349': 3.4618558413523903e-06,\n 'c-47': 3.458095111923132e-06,\n 'c-83': 3.443503284281113e-06,\n 'g-128': 3.4251639210482188e-06,\n 'g-254': 3.3561078253523113e-06,\n 'g-705': 3.295172628734605e-06,\n 'g-221': 3.257613826954109e-06,\n 'g-558': 3.1826495295074475e-06,\n 'g-535': 3.173825036017347e-06,\n 'g-37': 3.0805551908819062e-06,\n 'g-699': 3.075687660326204e-06,\n 'g-133': 3.0428073650376475e-06,\n 'c-50': 3.027636538407985e-06,\n 'g-148': 3.010788434183731e-06,\n 'g-162': 3.007031680435368e-06,\n 'g-743': 2.988192256858646e-06,\n 'g-371': 2.967466335983271e-06,\n 'g-689': 2.956495483329852e-06,\n 'g-524': 2.9296702001337105e-06,\n 'g-222': 2.9282610901591732e-06,\n 'g-186': 2.9168840147385244e-06,\n 'g-123': 2.9001610857802562e-06,\n 'g-201': 2.883248978863967e-06,\n 'g-169': 2.8775929041650883e-06,\n 'g-241': 2.8617193134281016e-06,\n 'g-39': 2.8458043920716325e-06,\n 'g-283': 2.793495733725837e-06,\n 'g-344': 2.71872859287578e-06,\n 'g-330': 2.7073830149611233e-06,\n 'g-500': 2.6752917188371805e-06,\n 'g-34': 2.672724956867545e-06,\n 'g-284': 2.6352485361959277e-06,\n 'g-185': 2.559522629723421e-06,\n 'g-422': 2.541668435964567e-06,\n 'g-120': 2.5008921263509898e-06,\n 'g-424': 2.4734633554714114e-06,\n 'g-350': 2.4504822346352317e-06,\n 'g-90': 2.4496371924358218e-06,\n 'g-74': 2.430946046055177e-06,\n 'g-464': 2.3816982587347713e-06,\n 'g-578': 2.365561598115934e-06,\n 'g-130': 2.312021952083354e-06,\n 'g-68': 2.2895696148508837e-06,\n 'g-85': 2.2652833086347912e-06,\n 'g-108': 2.2398459822853267e-06,\n 'g-107': 2.2348095743002228e-06,\n 'g-354': 2.2240328873476822e-06,\n 'c-62': 2.1776864470371415e-06,\n 'g-102': 2.1666702966320617e-06,\n 'g-583': 2.1648899684034717e-06,\n 'g-698': 2.113567797439042e-06,\n 'g-282': 2.1088386528249348e-06,\n 'g-239': 2.0596924432333963e-06,\n 'c-9': 2.033653487356485e-06,\n 'g-395': 2.0098728820312772e-06,\n 'g-398': 1.9940169854668066e-06,\n 'g-402': 1.9930945499048447e-06,\n 'g-634': 1.9794077049835324e-06,\n 'g-336': 1.96072791504992e-06,\n 'g-288': 1.94480829473348e-06,\n 'c-57': 1.932163701992562e-06,\n 'g-760': 1.922187960211319e-06,\n 'g-769': 1.913537321929226e-06,\n 'g-463': 1.9084083596673795e-06,\n 'g-475': 1.9081333164916447e-06,\n 'g-27': 1.902825461919111e-06,\n 'g-656': 1.897437117731604e-06,\n 'g-435': 1.834458254506427e-06,\n 'c-76': 1.8216075735495718e-06,\n 'c-73': 1.8124554370217705e-06,\n 'g-293': 1.8042813555232962e-06,\n 'g-325': 1.7826783690888526e-06,\n 'g-478': 1.7602033240762816e-06,\n 'g-451': 1.7570384571521935e-06,\n 'g-394': 1.753081068506246e-06,\n 'g-50': 1.7462499659753883e-06,\n 'g-324': 1.7450236131678898e-06,\n 'g-691': 1.7289812445468733e-06,\n 'g-621': 1.713508799607022e-06,\n 'g-406': 1.7111851847717574e-06,\n 'g-125': 1.702795466105439e-06,\n 'g-228': 1.6864119270973421e-06,\n 'g-138': 1.668166304855112e-06,\n 'g-150': 1.664204192577956e-06,\n 'c-48': 1.6510481691718593e-06,\n 'g-3': 1.6382041999260522e-06,\n 'g-722': 1.629848063296191e-06,\n 'g-317': 1.627868274085148e-06,\n 'c-59': 1.6177488953300623e-06,\n 'g-642': 1.6092117793645166e-06,\n 'c-49': 1.5994953943554435e-06,\n 'g-348': 1.5779945662028338e-06,\n 'g-724': 1.560896275312057e-06,\n 'g-180': 1.5496245431775857e-06,\n 'g-453': 1.5475082016137842e-06,\n 'g-343': 1.5364580172386177e-06,\n 'g-620': 1.5325495495155383e-06,\n 'g-353': 1.5292802938368655e-06,\n 'g-577': 1.5280309331837383e-06,\n 'g-492': 1.5232885374263927e-06,\n 'g-604': 1.5199438495212825e-06,\n 'g-65': 1.5160301040824387e-06,\n 'g-643': 1.4906263786328144e-06,\n 'g-606': 1.4897209912045906e-06,\n 'g-600': 1.4890871023257035e-06,\n 'c-86': 1.4697427417192932e-06,\n 'g-84': 1.4683520913154335e-06,\n 'g-209': 1.4543180869783234e-06,\n 'g-627': 1.4404189653711985e-06,\n 'g-744': 1.4304767472564373e-06,\n 'g-427': 1.421144863081314e-06,\n 'g-218': 1.4205156534628782e-06,\n 'g-749': 1.4182603473127109e-06,\n 'g-270': 1.4159817915068706e-06,\n 'g-562': 1.4137016312136541e-06,\n 'g-113': 1.3974862455232162e-06,\n 'g-311': 1.390856955826164e-06,\n 'g-411': 1.3891441982896713e-06,\n 'g-137': 1.372229836222455e-06,\n 'g-614': 1.3653096154822575e-06,\n 'g-265': 1.359031752627693e-06,\n 'g-271': 1.3529133313198993e-06,\n 'g-718': 1.340473000439879e-06,\n 'g-410': 1.3276955818618263e-06,\n 'g-110': 1.3228546225235727e-06,\n 'g-116': 1.298881131119728e-06,\n 'c-36': 1.287035065019454e-06,\n 'g-491': 1.2831916221499318e-06,\n 'c-75': 1.281821582536924e-06,\n 'g-98': 1.277486102214953e-06,\n 'g-337': 1.2512500739865484e-06,\n 'g-615': 1.2505988316938166e-06,\n 'g-547': 1.2487571300276445e-06,\n 'c-93': 1.2335151984080306e-06,\n 'g-363': 1.2304627955671277e-06,\n 'c-64': 1.2141048864218462e-06,\n 'g-757': 1.212874706009448e-06,\n 'g-688': 1.2081097161380228e-06,\n 'g-296': 1.2075204035080778e-06,\n 'g-677': 1.1960004442992944e-06,\n 'g-497': 1.1936212840346627e-06,\n 'g-694': 1.1926476845038059e-06,\n 'g-599': 1.1787597902455926e-06,\n 'g-360': 1.1778832557651775e-06,\n 'g-259': 1.1643121309995452e-06,\n 'g-616': 1.160828075340986e-06,\n 'g-433': 1.1414788518016028e-06,\n 'g-351': 1.133860730709313e-06,\n 'g-710': 1.1334031616086482e-06,\n 'g-567': 1.1319880665223991e-06,\n 'g-192': 1.1310770417424787e-06,\n 'g-750': 1.1308758310520084e-06,\n 'g-136': 1.1195101151592612e-06,\n 'c-32': 1.118615955676694e-06,\n 'g-594': 1.1183307936785125e-06,\n 'c-3': 1.1166898347507448e-06,\n 'g-215': 1.1122778721707471e-06,\n 'g-745': 1.107972321060663e-06,\n 'g-134': 1.1002308560464913e-06,\n 'c-18': 1.0910846531334006e-06,\n 'g-608': 1.0873099428772293e-06,\n 'g-720': 1.0850057862790696e-06,\n 'g-365': 1.0727697551046744e-06,\n 'g-711': 1.0606730347396953e-06,\n 'g-459': 1.0597894623820547e-06,\n 'g-249': 1.0588000521026542e-06,\n 'c-54': 1.055082467549956e-06,\n 'g-499': 1.053672543761125e-06,\n 'g-188': 1.0534846985876811e-06,\n 'g-60': 1.050946779837525e-06,\n 'c-10': 1.0427908754100101e-06,\n 'c-0': 1.0354584545034518e-06,\n 'g-675': 1.0299090000935596e-06,\n 'g-143': 1.0204648044145892e-06,\n 'c-33': 1.0131644668995299e-06,\n 'g-548': 1.0125293339914576e-06,\n 'g-118': 1.010705249854249e-06,\n 'g-434': 1.0012634327170866e-06,\n 'c-27': 9.996715013803337e-07,\n 'g-280': 9.918137157255424e-07,\n 'g-681': 9.850810701109158e-07,\n 'g-631': 9.823470907277532e-07,\n 'g-314': 9.792429778929246e-07,\n 'g-224': 9.77628601100955e-07,\n 'g-590': 9.719805939070736e-07,\n 'g-674': 9.712411257606357e-07,\n 'c-31': 9.697541715515678e-07,\n 'g-329': 9.655053282639492e-07,\n 'g-114': 9.644833562014177e-07,\n 'g-289': 9.617866192843105e-07,\n 'g-312': 9.597473090913744e-07,\n 'g-613': 9.497277787644254e-07,\n 'g-405': 9.480869737274467e-07,\n 'g-190': 9.448089625004041e-07,\n 'g-709': 9.440603794194646e-07,\n 'g-439': 9.414010838162956e-07,\n 'g-281': 9.288480966387902e-07,\n 'g-730': 9.213296521745629e-07,\n 'g-731': 9.178160472120278e-07,\n 'g-515': 9.155805106332571e-07,\n 'g-328': 9.15567074519924e-07,\n 'c-34': 9.148604607420174e-07,\n 'g-603': 9.145652390095405e-07,\n 'g-563': 9.115578936610969e-07,\n 'g-692': 9.061188545149423e-07,\n 'g-449': 9.016382900677022e-07,\n 'g-546': 8.99072230619441e-07,\n 'g-683': 8.940141145788449e-07,\n 'g-632': 8.777296207455876e-07,\n 'g-516': 8.773698170795596e-07,\n 'g-297': 8.554442851131661e-07,\n 'g-140': 8.554382761594148e-07,\n 'g-559': 8.532758413101749e-07,\n 'g-458': 8.523835837610794e-07,\n 'g-132': 8.513555042610754e-07,\n 'c-52': 8.464986517159323e-07,\n 'g-456': 8.443130024501921e-07,\n 'g-166': 8.410100712369362e-07,\n 'g-704': 8.397020424440182e-07,\n 'g-99': 8.36851007983419e-07,\n 'g-587': 8.313424774697431e-07,\n 'g-586': 8.269122663898121e-07,\n 'g-503': 8.225928736192711e-07,\n 'g-673': 8.18442947969622e-07,\n 'g-390': 8.162098375126547e-07,\n 'g-147': 8.15807669442975e-07,\n 'g-176': 8.100052745312059e-07,\n 'g-526': 8.095869381066767e-07,\n 'g-274': 8.061259960077949e-07,\n 'g-504': 8.05876951330553e-07,\n 'g-182': 8.021683133539736e-07,\n 'c-44': 7.988033846086096e-07,\n 'g-103': 7.972307461169559e-07,\n 'c-87': 7.952500985994315e-07,\n 'g-429': 7.93601632784191e-07,\n 'g-525': 7.903086405199045e-07,\n 'g-623': 7.882190642891085e-07,\n 'g-14': 7.751755762772095e-07,\n 'c-46': 7.66161281216815e-07,\n 'g-649': 7.540589464019587e-07,\n 'c-29': 7.522926001937713e-07,\n 'c-91': 7.47502973071773e-07,\n 'g-372': 7.442611096621088e-07,\n 'g-287': 7.392429421000657e-07,\n 'g-448': 7.335335599246984e-07,\n 'g-767': 7.245082273461823e-07,\n 'g-83': 7.20830602551209e-07,\n 'g-146': 7.154620971054626e-07,\n 'g-752': 7.096131951801754e-07,\n 'g-486': 7.076229484107022e-07,\n 'g-388': 7.03925284350343e-07,\n 'g-257': 7.026237764801158e-07,\n 'g-593': 6.984390242825711e-07,\n 'c-79': 6.960466548792954e-07,\n 'g-732': 6.88436896250505e-07,\n 'c-14': 6.881557198940014e-07,\n 'g-81': 6.873187078301757e-07,\n 'g-738': 6.855246353562605e-07,\n 'g-335': 6.847791440174611e-07,\n 'g-658': 6.843466337225212e-07,\n 'g-409': 6.837915382564008e-07,\n 'g-566': 6.83553591375452e-07,\n 'g-488': 6.77648133964176e-07,\n 'g-303': 6.75869679438168e-07,\n 'g-200': 6.655153609383402e-07,\n 'g-461': 6.648601065493975e-07,\n 'g-214': 6.609151824085724e-07,\n 'g-187': 6.434145873555897e-07,\n 'g-529': 6.396247874997096e-07,\n 'g-124': 6.358814359300191e-07,\n 'g-35': 6.297186773314423e-07,\n 'g-482': 6.276902063916123e-07,\n 'c-23': 6.261651320782491e-07,\n 'g-308': 6.228363198211573e-07,\n 'g-326': 6.221429937301737e-07,\n 'g-342': 6.208456845915866e-07,\n 'c-21': 6.184011364165909e-07,\n 'g-447': 6.162455626554375e-07,\n 'g-96': 6.134079163777617e-07,\n 'g-626': 6.089897474116646e-07,\n 'g-347': 6.076513206290657e-07,\n 'g-445': 6.063769532535512e-07,\n 'g-334': 6.046218758090638e-07,\n 'g-501': 6.041068997993115e-07,\n 'g-457': 6.006681017890758e-07,\n 'g-327': 5.999482674788614e-07,\n 'g-79': 5.942383812830576e-07,\n 'g-2': 5.920520624834447e-07,\n 'g-322': 5.736654160315435e-07,\n 'g-331': 5.712400892761427e-07,\n 'g-598': 5.705099478496567e-07,\n 'g-232': 5.691627718087955e-07,\n 'g-539': 5.664926915338597e-07,\n 'c-25': 5.664758681231397e-07,\n 'g-151': 5.642336217311084e-07,\n 'c-22': 5.634952290160811e-07,\n 'g-189': 5.629774342784388e-07,\n 'g-321': 5.571088992246953e-07,\n 'g-602': 5.567440878768981e-07,\n 'g-364': 5.548747598477233e-07,\n 'g-268': 5.534153252155505e-07,\n 'g-11': 5.533140657008884e-07,\n 'g-553': 5.517878253064046e-07,\n 'g-428': 5.433805438104233e-07,\n 'c-77': 5.387203927190165e-07,\n 'g-511': 5.375980932399715e-07,\n 'g-701': 5.363843544325952e-07,\n 'g-292': 5.31987488925495e-07,\n 'g-195': 5.304911471250284e-07,\n 'g-669': 5.304909626302479e-07,\n 'g-121': 5.275023862924555e-07,\n 'g-12': 5.268872220499932e-07,\n 'g-768': 5.268353064392939e-07,\n 'g-362': 5.263628920330887e-07,\n 'g-420': 5.230965462107995e-07,\n 'g-298': 5.220005301100061e-07,\n 'g-549': 5.089404169524026e-07,\n 'g-423': 5.080042985994293e-07,\n 'g-684': 5.031300769793579e-07,\n 'g-771': 5.028457429091804e-07,\n 'g-275': 4.852402311211756e-07,\n 'g-106': 4.82519976995055e-07,\n 'g-358': 4.821343669199341e-07,\n 'g-319': 4.807595737725545e-07,\n 'c-94': 4.796027928966051e-07,\n 'g-450': 4.793510656984912e-07,\n 'g-471': 4.791900403110227e-07,\n 'g-545': 4.655641834584723e-07,\n 'g-55': 4.641985971998164e-07,\n 'g-309': 4.5879811752178035e-07,\n 'g-437': 4.572685789357156e-07,\n 'c-43': 4.569454791873706e-07,\n 'g-472': 4.551276576759966e-07,\n 'g-702': 4.54023461111952e-07,\n 'g-495': 4.5185431514493946e-07,\n 'g-345': 4.4938158972468445e-07,\n 'g-714': 4.488534509732023e-07,\n 'g-741': 4.483673090548146e-07,\n 'g-460': 4.4560888723746417e-07,\n 'c-7': 4.4523355793857355e-07,\n 'g-245': 4.415877373521726e-07,\n 'g-393': 4.400767479213752e-07,\n 'g-764': 4.374578532161111e-07,\n 'g-177': 4.335327459872862e-07,\n 'c-84': 4.324335900925125e-07,\n 'g-584': 4.314395716208119e-07,\n 'g-576': 4.309827007636935e-07,\n 'g-155': 4.288871081024881e-07,\n 'g-213': 4.288517409974102e-07,\n 'g-109': 4.2629169370173026e-07,\n 'g-391': 4.2319659839321067e-07,\n 'g-302': 4.182154137027938e-07,\n 'g-664': 4.179495430058078e-07,\n 'c-20': 4.1741306563633507e-07,\n 'c-45': 4.155030926579484e-07,\n 'c-16': 4.148637707465652e-07,\n 'g-352': 4.1447813681552703e-07,\n 'g-163': 4.144452771144558e-07,\n 'c-4': 4.1110037920372156e-07,\n 'g-367': 4.1083894583918146e-07,\n 'g-690': 4.060503650780978e-07,\n 'g-165': 4.054956216463512e-07,\n 'g-219': 4.053770660261691e-07,\n 'g-53': 4.032492235059304e-07,\n 'g-258': 4.0219517528916615e-07,\n 'c-2': 4.007381981079028e-07,\n 'c-19': 4.0069103401235706e-07,\n 'g-276': 4.0050022143828246e-07,\n 'g-262': 3.9983112147534694e-07,\n 'g-541': 3.9873251824612543e-07,\n 'g-142': 3.9826960485794816e-07,\n 'g-454': 3.963762417215355e-07,\n 'c-58': 3.9600974746645257e-07,\n 'g-45': 3.933710367963472e-07,\n 'g-13': 3.915228616625799e-07,\n 'g-101': 3.8905880980766305e-07,\n 'g-697': 3.881182286148699e-07,\n 'g-16': 3.860733170275854e-07,\n 'g-255': 3.856599959904672e-07,\n 'g-304': 3.8519013490292964e-07,\n 'g-573': 3.8464178342412225e-07,\n 'g-294': 3.8140317810270163e-07,\n 'g-252': 3.8123035110346204e-07,\n 'g-611': 3.772582980940409e-07,\n 'g-734': 3.7669865696893234e-07,\n 'g-572': 3.7449933263516977e-07,\n 'g-397': 3.7434889520529535e-07,\n 'g-470': 3.718273162527097e-07,\n 'g-238': 3.716121077662615e-07,\n 'g-173': 3.702135910783888e-07,\n 'g-67': 3.692201452042132e-07,\n 'g-279': 3.6680664711533084e-07,\n 'g-73': 3.659285773179033e-07,\n 'g-278': 3.648811839757071e-07,\n 'c-82': 3.600324221912643e-07,\n 'g-286': 3.5236536510482863e-07,\n 'g-663': 3.5130213373382735e-07,\n 'g-653': 3.4699912122501675e-07,\n 'g-216': 3.4686675776107734e-07,\n 'c-63': 3.458044077059297e-07,\n 'g-659': 3.3979648032811394e-07,\n 'g-346': 3.3866382841141185e-07,\n 'g-38': 3.3809781479696155e-07,\n 'g-726': 3.365412047720562e-07,\n 'g-17': 3.31569387855557e-07,\n 'g-261': 3.3110967394278656e-07,\n 'c-90': 3.304626399955324e-07,\n 'g-164': 3.2950812068471835e-07,\n 'g-383': 3.2713478286330866e-07,\n 'g-144': 3.241482940258278e-07,\n 'g-42': 3.1657226409048667e-07,\n 'g-737': 3.1307259878987637e-07,\n 'g-727': 3.116798901921236e-07,\n 'g-64': 3.116522972884672e-07,\n 'g-263': 3.116060501157092e-07,\n 'g-174': 3.106175274388001e-07,\n 'g-333': 3.1021089379537625e-07,\n 'g-687': 3.101421423151729e-07,\n 'g-498': 3.093675973904475e-07,\n 'g-413': 3.079717688127259e-07,\n 'g-729': 3.070896901213871e-07,\n 'g-5': 3.0473569022707037e-07,\n 'g-660': 3.0322574371549615e-07,\n 'g-389': 3.004551066831951e-07,\n 'c-85': 2.9674474473842527e-07,\n 'c-89': 2.961527487549409e-07,\n 'g-561': 2.9452338144025925e-07,\n 'c-95': 2.940259351616459e-07,\n 'g-619': 2.937927590131173e-07,\n 'c-28': 2.905393450478022e-07,\n 'g-159': 2.8726638705983687e-07,\n 'g-179': 2.868156554376722e-07,\n 'g-400': 2.855396064559146e-07,\n 'cp_time': 2.8384700244063477e-07,\n 'g-230': 2.813739055293474e-07,\n 'g-733': 2.8095594450938677e-07,\n 'g-242': 2.805466036565063e-07,\n 'c-40': 2.779083959382478e-07,\n 'g-596': 2.7472510147341644e-07,\n 'g-725': 2.7438122518486097e-07,\n 'g-765': 2.7337055949483435e-07,\n 'g-625': 2.724480178580013e-07,\n 'g-115': 2.7024493295599306e-07,\n 'g-56': 2.676034609773448e-07,\n 'g-426': 2.67199245271299e-07,\n 'g-240': 2.663370296425438e-07,\n 'g-695': 2.661585922318632e-07,\n 'g-86': 2.657595797352086e-07,\n 'g-204': 2.612285147987614e-07,\n 'g-301': 2.5925159652731566e-07,\n 'g-401': 2.5916646882034877e-07,\n 'g-318': 2.581393232052587e-07,\n 'c-60': 2.5515833989783743e-07,\n 'g-601': 2.5209003566908916e-07,\n 'g-544': 2.5160640695909287e-07,\n 'g-751': 2.500023256572781e-07,\n 'c-24': 2.497452207123052e-07,\n 'c-5': 2.4928770476356776e-07,\n 'g-543': 2.465135125438722e-07,\n 'g-93': 2.44903095650284e-07,\n 'g-723': 2.4382475201367715e-07,\n 'g-153': 2.3850314097295655e-07,\n 'c-13': 2.3833611434431745e-07,\n 'g-758': 2.362180056315888e-07,\n 'g-496': 2.3619145981049194e-07,\n 'g-52': 2.3470226587218868e-07,\n 'g-712': 2.330461880382939e-07,\n 'g-485': 2.3292775972599822e-07,\n 'g-234': 2.3258288014624262e-07,\n 'g-6': 2.3136293483480563e-07,\n 'g-172': 2.3048194189301263e-07,\n 'g-527': 2.303968877521989e-07,\n 'g-444': 2.2998030032098105e-07,\n 'g-111': 2.2755298810170355e-07,\n 'g-618': 2.2521558476137904e-07,\n 'c-99': 2.2458225151442135e-07,\n 'g-440': 2.232435594336224e-07,\n 'g-754': 2.2284971633659834e-07,\n 'g-715': 2.1606116270764253e-07,\n 'c-71': 2.152649634795667e-07,\n 'g-579': 2.1496713966881464e-07,\n 'g-595': 2.148282110259192e-07,\n 'g-355': 2.1454596991349328e-07,\n 'g-560': 2.1440321806148743e-07,\n 'g-662': 2.1316430974874234e-07,\n 'g-277': 2.1113033838240924e-07,\n 'g-638': 2.1070010234117142e-07,\n 'g-510': 2.10281710506105e-07,\n 'g-487': 2.0952430994022886e-07,\n 'g-126': 2.0670170686881706e-07,\n 'g-0': 2.0512602800712432e-07,\n 'g-477': 2.0472820306399742e-07,\n 'g-197': 2.0209300830714705e-07,\n 'g-716': 2.0031367530329303e-07,\n 'g-212': 2.002870379963495e-07,\n 'g-92': 1.9987846147997979e-07,\n 'g-417': 1.9975694961343882e-07,\n 'c-12': 1.9777605186899194e-07,\n 'g-479': 1.9712676063016188e-07,\n 'c-66': 1.9706362597293747e-07,\n 'g-264': 1.9546752959981317e-07,\n 'g-149': 1.9416989572099141e-07,\n 'c-69': 1.93375287924763e-07,\n 'g-647': 1.9285382754294367e-07,\n 'g-251': 1.9115100480249225e-07,\n 'g-246': 1.911214018782137e-07,\n 'g-139': 1.8951571561717784e-07,\n 'g-425': 1.8939107000737199e-07,\n 'g-59': 1.8627836523982255e-07,\n 'g-465': 1.8602140599774386e-07,\n 'g-655': 1.8498359648927432e-07,\n 'g-315': 1.8308838528602323e-07,\n 'g-236': 1.808039185278998e-07,\n 'g-645': 1.7970521492410851e-07,\n 'g-233': 1.7886795760854723e-07,\n 'g-366': 1.776356905285048e-07,\n 'g-678': 1.7758623853739164e-07,\n 'g-316': 1.7740702824495247e-07,\n 'g-728': 1.7316832821856476e-07,\n 'g-717': 1.726054199915894e-07,\n 'g-667': 1.7224132219800792e-07,\n 'g-46': 1.7155292915752018e-07,\n 'g-693': 1.7108658006276256e-07,\n 'g-582': 1.7074411177639415e-07,\n 'g-359': 1.7046536391826517e-07,\n 'g-679': 1.690447127590533e-07,\n 'g-141': 1.6846692398736485e-07,\n 'g-87': 1.6744365984380782e-07,\n 'g-10': 1.6668695562369051e-07,\n 'c-74': 1.6505784482256924e-07,\n 'g-191': 1.6282402852743627e-07,\n 'g-127': 1.6252276188732928e-07,\n 'g-550': 1.610496246570725e-07,\n 'g-25': 1.596529300602889e-07,\n 'g-43': 1.59456408240638e-07,\n 'g-756': 1.560603338987554e-07,\n 'c-42': 1.560449267960784e-07,\n 'c-38': 1.5531691036421713e-07,\n 'g-421': 1.551974695077063e-07,\n 'g-755': 1.551252766240907e-07,\n 'g-542': 1.540613276639069e-07,\n 'g-770': 1.5395633225917527e-07,\n 'g-19': 1.5368369005522298e-07,\n 'g-742': 1.4996967480709888e-07,\n 'g-753': 1.4899928708553278e-07,\n 'g-462': 1.473705561992611e-07,\n 'c-61': 1.4659489514651236e-07,\n 'c-88': 1.4579642795137238e-07,\n 'g-247': 1.4530297989465146e-07,\n 'g-532': 1.4184346193435449e-07,\n 'g-468': 1.40367450562201e-07,\n 'g-637': 1.36662890178052e-07,\n 'g-622': 1.3643287933087778e-07,\n 'g-32': 1.362598951240579e-07,\n 'g-305': 1.3625920126936197e-07,\n 'g-307': 1.3622586518546864e-07,\n 'g-82': 1.3531997610990998e-07,\n 'g-227': 1.3436224403223784e-07,\n 'g-412': 1.3371867414735394e-07,\n 'g-508': 1.3069126213183235e-07,\n 'g-376': 1.2856539155503555e-07,\n 'g-467': 1.2802646669890505e-07,\n 'g-514': 1.2524055853155303e-07,\n 'g-415': 1.2241508523525013e-07,\n 'g-507': 1.2110615257854285e-07,\n 'g-506': 1.209977372532145e-07,\n 'c-15': 1.2028107133216226e-07,\n 'g-72': 1.2009333151888346e-07,\n 'c-80': 1.1986285260862095e-07,\n 'g-196': 1.196054722797657e-07,\n 'g-612': 1.1725191316480643e-07,\n 'g-112': 1.167448059335996e-07,\n 'g-652': 1.158602496234562e-07,\n 'g-31': 1.1419943985249703e-07,\n 'g-484': 1.1122540222868738e-07,\n 'g-624': 1.108824430919031e-07,\n 'g-119': 1.1050364190010042e-07,\n 'g-686': 1.0720032792357892e-07,\n 'g-250': 1.0582993604721369e-07,\n 'g-607': 1.0522678639199312e-07,\n 'g-591': 1.0322353099015413e-07,\n 'g-266': 1.0222916274268967e-07,\n 'g-399': 9.894184556033525e-08,\n 'g-721': 9.829925796261962e-08,\n 'g-556': 9.730271251576217e-08,\n 'g-49': 9.701604902073724e-08,\n 'g-80': 9.173710683804681e-08,\n 'g-88': 9.146694225289909e-08,\n 'c-81': 8.765266911944791e-08,\n 'g-551': 8.691398903359904e-08,\n 'g-671': 8.67308985776083e-08,\n 'g-517': 8.469744796485434e-08,\n 'g-1': 8.210589260912449e-08,\n 'g-237': 7.951851367649088e-08,\n 'g-518': 7.768607905231373e-08,\n 'c-55': 7.762676226472087e-08,\n 'g-161': 7.736429036980796e-08,\n 'g-160': 7.534307514683869e-08,\n 'g-226': 7.460253180491261e-08,\n 'g-104': 7.21596810186742e-08,\n 'g-452': 7.213661491409229e-08,\n 'g-285': 6.778841048910023e-08,\n 'g-766': 6.686120928786621e-08,\n 'g-419': 6.596810874295866e-08,\n 'g-198': 6.387304478036593e-08,\n 'g-320': 6.361480330008273e-08,\n 'g-223': 6.235416094915736e-08,\n 'g-474': 6.034104775565208e-08,\n 'g-361': 5.9406035333997576e-08,\n 'g-44': 5.89212384126403e-08,\n 'g-273': 5.798257280212327e-08,\n 'g-256': 5.764005040972964e-08,\n 'g-384': 5.519646257418076e-08,\n 'g-536': 5.466443499593798e-08,\n 'c-41': 5.448208455563486e-08,\n 'g-403': 5.1525177995753735e-08,\n 'g-131': 5.087896474270659e-08,\n 'c-39': 5.085142784633234e-08,\n 'g-129': 5.049199792300785e-08,\n 'g-494': 4.876590678146164e-08,\n 'g-700': 4.839275753037997e-08,\n 'g-469': 4.77466935329407e-08,\n 'g-644': 4.767061230134817e-08,\n 'g-480': 4.637462669432324e-08,\n 'g-552': 4.384575382279654e-08,\n 'g-589': 4.068309708213369e-08,\n 'g-54': 3.9693197253359846e-08,\n 'g-519': 3.9505208871554176e-08,\n 'g-685': 3.763075605983657e-08,\n 'g-71': 3.715000873247276e-08,\n 'g-740': 3.66265780725461e-08,\n 'g-171': 3.5685037126642616e-08,\n 'g-707': 3.557757894193103e-08,\n 'g-540': 3.4341895376044373e-08,\n 'g-703': 3.334146999808829e-08,\n 'g-382': 3.266638030560709e-08,\n 'g-555': 3.192204792515363e-08,\n 'g-513': 3.061702627885077e-08,\n 'g-225': 2.9537522969191876e-08,\n 'g-374': 2.7524506358150846e-08,\n 'g-585': 2.6830498746355236e-08,\n 'g-665': 2.5173981916398747e-08,\n 'g-706': 2.488123128016606e-08,\n 'g-661': 2.24617777398739e-08,\n 'g-61': 2.1529947463738708e-08,\n 'g-436': 2.0691763864821855e-08,\n 'g-575': 2.015032508559389e-08,\n 'g-375': 1.938262903303234e-08,\n 'g-735': 1.778517531511592e-08,\n 'g-719': 1.7363494191069417e-08,\n 'g-680': 1.5200741127624484e-08,\n 'g-368': 1.2435746422206906e-08,\n 'cp_dose': 1.1880937379749046e-08,\n 'g-509': 1.1176315827671202e-08,\n 'g-323': 9.30378559610423e-09,\n 'g-580': 7.633843803617202e-09,\n 'g-571': 5.102416905655716e-09,\n 'g-341': 4.350571108230339e-09,\n 'g-651': 4.073687689259975e-09,\n 'g-26': 3.4393676448529753e-09,\n 'g-211': 2.0253343446796634e-09,\n 'g-229': 3.847019439118249e-10,\n 'c-72': 2.5645403856078275e-10}"},"metadata":{}}]},{"cell_type":"code","source":"# visualize top10 importances\nimport seaborn as sns\n\nimportance = pd.DataFrame({\n    \"feature\": deltas.index,\n    \"importance\": deltas.values\n})\nimportance = importance.sort_values(by=\"importance\", ascending=False)\nimportance = importance.head(n=10)\n\nplt.figure(figsize=(20, 5))\nsns.barplot(x=importance[\"feature\"], y=importance[\"importance\"], palette=\"viridis\")\nplt.title(\"Top Importance Features for permutation importance\", size=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-27T14:27:21.890341Z","iopub.execute_input":"2022-12-27T14:27:21.890730Z","iopub.status.idle":"2022-12-27T14:27:22.204679Z","shell.execute_reply.started":"2022-12-27T14:27:21.890696Z","shell.execute_reply":"2022-12-27T14:27:22.203774Z"},"trusted":true},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1440x360 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"sns.histplot(x=\"c-65\", data=train_features, kde=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-27T14:29:56.476728Z","iopub.execute_input":"2022-12-27T14:29:56.477190Z","iopub.status.idle":"2022-12-27T14:29:57.191815Z","shell.execute_reply.started":"2022-12-27T14:29:56.477149Z","shell.execute_reply":"2022-12-27T14:29:57.190761Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:xlabel='c-65', ylabel='Count'>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"train_features[\"c-65\"].describe()","metadata":{"execution":{"iopub.status.busy":"2022-12-27T14:30:32.022159Z","iopub.execute_input":"2022-12-27T14:30:32.022636Z","iopub.status.idle":"2022-12-27T14:30:32.036962Z","shell.execute_reply.started":"2022-12-27T14:30:32.022597Z","shell.execute_reply":"2022-12-27T14:30:32.035903Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"count    23814.000000\nmean        -0.619682\nstd          2.225596\nmin        -10.000000\n25%         -0.603200\n50%          0.007650\n75%          0.441250\nmax          3.328000\nName: c-65, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"selected_train = train_features[deltas[deltas>0].index.tolist()].copy(deep=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-27T14:43:36.452841Z","iopub.execute_input":"2022-12-27T14:43:36.453607Z","iopub.status.idle":"2022-12-27T14:43:36.558447Z","shell.execute_reply.started":"2022-12-27T14:43:36.453570Z","shell.execute_reply":"2022-12-27T14:43:36.557423Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"# commented as important information lost\n# for column in selected_train.drop([\"cp_type\", \"cp_dose\"], axis=1).columns:\n#     p_1 = np.percentile(selected_train[column], 0.015)\n#     p_2 = np.percentile(selected_train[column], 0.99)\n#     selected_train[column] = np.clip(selected_train[column], a_min=p_1, a_max=p_2)","metadata":{"execution":{"iopub.status.busy":"2022-12-27T14:37:54.375795Z","iopub.execute_input":"2022-12-27T14:37:54.376202Z","iopub.status.idle":"2022-12-27T14:37:55.916618Z","shell.execute_reply.started":"2022-12-27T14:37:54.376170Z","shell.execute_reply":"2022-12-27T14:37:55.915657Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"k = int(np.sqrt(targets.shape[1]))\n# sketch = RandomProjectionSketch(k)  # use_hess=False - S(R=leaf) = sum(1..d) (sum(gi^2)/(sum(hi) + l2)), d=num_classes\n# gi = d'L/(da=prev_tree_out), hi = d\"L/da, where L = BCE/MSE/XE...\n\"\"\"\nuse matrix P, sampled from N(0, 1/k), so that jacobian Gk[N,K] = Gd[N,D]@P[D,K]\nTrain mean score by each fold:0.01331 +/- 0.00002\nValid mean score by each fold:0.01584 +/- 0.00013\n**************************************************\nOOF-score: 0.01584\n\"\"\"\n# sketch = RandomSamplingSketch(k)  # use_hess=True (add second derivative part - diagonal hessian in tree-split computation)\n# in leaf estimation hessian is always applied\n\"\"\"\ncompute all class-gradients norms and total norms, sample every class with probability of:\npi = ||gi|| / sum(||gj||), i = 1..d, j = 1..d + normalize gi_ = gi / sqrt(k*pi)\nTrain mean score by each fold:0.01239 +/- 0.00002\nValid mean score by each fold:0.01581 +/- 0.00013\n**************************************************\nOOF-score: 0.01581\n\"\"\"\nsketch = TopOutputsSketch(k)  # use_hess=False\n\"\"\"\nselect top-k classes that have highest gradient norms\nTrain mean score by each fold:0.01314 +/- 0.00003\nValid mean score by each fold:0.01631 +/- 0.00012\n**************************************************\nOOF-score: 0.01631\n\"\"\"\n    \nparams = {\"ntrees\": 5000,  # number of trees (base estimators)\n          \"lr\": 0.01, # every tree multiplier\n          \"verbose\": 500, # print loss value after every 500 trees are constructed\n          \"es\": 100,  # stop training, when loss didn't improve during 100 trees\n          \"lambda_l2\": 10, # l2 regularization: sum(grads)*idx / (sum(hess) + lambda)\n          \"gd_steps\": 1, # number of newton iterations\n          \"subsample\": 0.6,  # sample 0.6 part of training data for each tree\n          \"colsample\": 0.8,  # sample 0.8 number of features per tree / tree level\n          \"min_data_in_leaf\": 20,  # min data samples in leaf (prevents overfitting)\n          \"use_hess\": False,  # use second derivative in scoring function when constructing tree, False - to speedup\n          \"max_bin\": 32,  # group feature values into bins to speedup optimal tree structure searching \n          \"max_depth\": 3,  # maximum depth of each tree to prevent overfitting\n          \"multioutput_sketch\": sketch  # type of output dimension reduction: sort, prob sample or projection\n         }  \n\ncv = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nestimators_, encoders_, oof_preds_ = sketch_cross_validation(params, selected_train, targets, cv, categorical=None)","metadata":{"execution":{"iopub.status.busy":"2022-12-27T15:12:29.429110Z","iopub.execute_input":"2022-12-27T15:12:29.430205Z","iopub.status.idle":"2022-12-27T15:22:55.783456Z","shell.execute_reply.started":"2022-12-27T15:12:29.430155Z","shell.execute_reply":"2022-12-27T15:22:55.782499Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"Tue Dec 27 15:12:29 2022, Cross-Validation, 23814 rows, 764 cols\n[15:12:32] Stdout logging level is INFO.\n[15:12:32] GDBT train starts. Max iter 5000, early stopping rounds 100\n[15:12:32] Iter 0; Sample 0, BCE = 0.020731030125675468; \n[15:12:43] Iter 500; Sample 0, BCE = 0.017624854328690407; \n[15:12:54] Iter 1000; Sample 0, BCE = 0.01716590388231179; \n[15:13:06] Iter 1500; Sample 0, BCE = 0.01692762672133184; \n[15:13:17] Iter 2000; Sample 0, BCE = 0.016757973384063606; \n[15:13:28] Iter 2500; Sample 0, BCE = 0.016635080705471685; \n[15:13:39] Iter 3000; Sample 0, BCE = 0.01653241651464982; \n[15:13:50] Iter 3500; Sample 0, BCE = 0.01644582212286762; \n[15:14:01] Iter 4000; Sample 0, BCE = 0.016362825397127703; \n[15:14:12] Iter 4500; Sample 0, BCE = 0.016294192955092435; \n[15:14:23] Iter 4999; Sample 0, BCE = 0.016236177297524673; \nFold 1, Train score = 0.01314, Valid score = 0.01635\n[15:14:37] Stdout logging level is INFO.\n[15:14:37] GDBT train starts. Max iter 5000, early stopping rounds 100\n[15:14:37] Iter 0; Sample 0, BCE = 0.02077626531744805; \n[15:14:48] Iter 500; Sample 0, BCE = 0.01777724562985081; \n[15:14:59] Iter 1000; Sample 0, BCE = 0.01729575258090486; \n[15:15:10] Iter 1500; Sample 0, BCE = 0.017048531511709028; \n[15:15:21] Iter 2000; Sample 0, BCE = 0.016879304521349735; \n[15:15:33] Iter 2500; Sample 0, BCE = 0.016741181635985185; \n[15:15:44] Iter 3000; Sample 0, BCE = 0.016633684946099223; \n[15:15:55] Iter 3500; Sample 0, BCE = 0.016538068046171014; \n[15:16:06] Iter 4000; Sample 0, BCE = 0.01646170115474744; \n[15:16:17] Iter 4500; Sample 0, BCE = 0.016393910771962086; \n[15:16:28] Iter 4999; Sample 0, BCE = 0.016333961976508028; \nFold 2, Train score = 0.01309, Valid score = 0.01646\n[15:16:41] Stdout logging level is INFO.\n[15:16:41] GDBT train starts. Max iter 5000, early stopping rounds 100\n[15:16:41] Iter 0; Sample 0, BCE = 0.020706410045779566; \n[15:16:53] Iter 500; Sample 0, BCE = 0.01767794954993619; \n[15:17:04] Iter 1000; Sample 0, BCE = 0.017201559748712097; \n[15:17:15] Iter 1500; Sample 0, BCE = 0.016945567387219692; \n[15:17:26] Iter 2000; Sample 0, BCE = 0.016765932188068308; \n[15:17:37] Iter 2500; Sample 0, BCE = 0.016629547000497485; \n[15:17:48] Iter 3000; Sample 0, BCE = 0.016525554996304655; \n[15:17:59] Iter 3500; Sample 0, BCE = 0.016433620938970092; \n[15:18:10] Iter 4000; Sample 0, BCE = 0.016356953105258608; \n[15:18:21] Iter 4500; Sample 0, BCE = 0.016288572973273283; \n[15:18:32] Iter 4999; Sample 0, BCE = 0.01622521307861495; \nFold 3, Train score = 0.01314, Valid score = 0.01636\n[15:18:46] Stdout logging level is INFO.\n[15:18:46] GDBT train starts. Max iter 5000, early stopping rounds 100\n[15:18:46] Iter 0; Sample 0, BCE = 0.020593921372172197; \n[15:18:57] Iter 500; Sample 0, BCE = 0.017455077852144844; \n[15:19:08] Iter 1000; Sample 0, BCE = 0.016954971256792836; \n[15:19:19] Iter 1500; Sample 0, BCE = 0.01669301630943046; \n[15:19:30] Iter 2000; Sample 0, BCE = 0.01651861479574035; \n[15:19:42] Iter 2500; Sample 0, BCE = 0.016386143640123075; \n[15:19:53] Iter 3000; Sample 0, BCE = 0.016276588760505688; \n[15:20:04] Iter 3500; Sample 0, BCE = 0.01618375082417231; \n[15:20:15] Iter 4000; Sample 0, BCE = 0.01610032917175071; \n[15:20:26] Iter 4500; Sample 0, BCE = 0.016032380549317164; \n[15:20:37] Iter 4999; Sample 0, BCE = 0.015969582103424022; \nFold 4, Train score = 0.01317, Valid score = 0.01610\n[15:20:51] Stdout logging level is INFO.\n[15:20:51] GDBT train starts. Max iter 5000, early stopping rounds 100\n[15:20:51] Iter 0; Sample 0, BCE = 0.02067225030081376; \n[15:21:02] Iter 500; Sample 0, BCE = 0.0175653692933343; \n[15:21:13] Iter 1000; Sample 0, BCE = 0.017087533473477207; \n[15:21:24] Iter 1500; Sample 0, BCE = 0.01684167632965332; \n[15:21:35] Iter 2000; Sample 0, BCE = 0.016669090493199208; \n[15:21:47] Iter 2500; Sample 0, BCE = 0.016545880220119533; \n[15:21:58] Iter 3000; Sample 0, BCE = 0.01644007156698419; \n[15:22:09] Iter 3500; Sample 0, BCE = 0.016351558956291735; \n[15:22:20] Iter 4000; Sample 0, BCE = 0.01627266463670367; \n[15:22:31] Iter 4500; Sample 0, BCE = 0.016204508831615932; \n[15:22:42] Iter 4999; Sample 0, BCE = 0.01614427172223416; \nFold 5, Train score = 0.01315, Valid score = 0.01627\nTrain score by each fold: [0.01314, 0.01309, 0.01314, 0.01317, 0.01315]\nValid score by each fold: [0.01635, 0.01646, 0.01636, 0.0161, 0.01627]\nTrain mean score by each fold:0.01314 +/- 0.00003\nValid mean score by each fold:0.01631 +/- 0.00012\n**************************************************\nOOF-score: 0.01631\n","output_type":"stream"}]},{"cell_type":"code","source":"# hyper-parameters tuning: optuna\n\ntune_params = {\"ntrees\": 5000,\n               \"verbose\": -1,\n               \"es\": 100, \n               \"lambda_l2\": 10, \n               \"gd_steps\": 1,\n               \"subsample\": 0.6,\n               \"colsample\": 0.8,\n               \"max_bin\": 32,\n               }\nsketches = [\n    RandomProjectionSketch(k),\n    RandomSamplingSketch(k),\n    TopOutputsSketch(k)\n]\n\ndef objective(trial):\n    param_trials = {\n                    'max_depth': trial.suggest_int('max_depth', 3, 10),\n                    'lr': trial.suggest_float('lr', 0.003, 0.1),\n                    'num_leaves': trial.suggest_int('num_leaves', 6, 100),\n                    'use_hess': trial.suggest_categorical('use_hess', [True, False]),\n                    'multioutput_sketch': trial.suggest_categorical(multioutput_sketch, sketches),\n                    'min_data_in_leaf': trial.suggest_int('min_data_in_leaf': 15, 200),\n                    }\n    \n    param_trials.update(tune_params)\n\n    fold = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    \n    _, _, oof_preds = sketch_cross_validation(param_trials, train_features, targets, fold, categorical=None)\n    score = multi_logloss(targets, oof_preds, activation=False)\n    \n    return score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n# optuna.logging.set_verbosity(optuna.logging.FATAL)\n# study = optuna.create_study(sampler=TPESampler(seed=seed), direction=\"minimize\")\n# study.optimize(objective, n_trials=1500, timeout=10000)\n# \n# print(f'Number of completed trials: {len(study.trials)}')\n# print('Best trial')\n# trial = study.best_trial\n# print(f'Best score: {trial.value}')\n# print('Best params')\n# pprint(trial.params)","metadata":{},"execution_count":null,"outputs":[]}]}