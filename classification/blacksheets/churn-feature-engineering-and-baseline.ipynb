{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport time\nfrom tqdm import tqdm\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, roc_auc_score, accuracy_score, f1_score\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nfrom functools import partial\n\nimport catboost\nfrom catboost import CatBoostClassifier, Pool","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-29T10:46:01.003380Z","iopub.execute_input":"2023-01-29T10:46:01.004290Z","iopub.status.idle":"2023-01-29T10:46:02.672776Z","shell.execute_reply.started":"2023-01-29T10:46:01.004161Z","shell.execute_reply":"2023-01-29T10:46:02.671260Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def create_numerical_aggs(data: pd.DataFrame,\n                          groupby_id: str,\n                          aggs: dict,\n                          prefix: str = None,\n                          suffix: str = None,\n                          ) -> pd.DataFrame:\n    if not prefix:\n        prefix = \"\"\n    if not suffix:\n        suffix = \"\"\n\n    data_grouped = data.groupby(groupby_id)\n    stats = data_grouped.agg(aggs)\n    stats.columns = [f\"{prefix}{feature}_{stat}{suffix}\".lower() for feature, stat in stats]\n    stats = stats.reset_index()\n\n    return stats\n\ndef find_max_consequtive(x):\n    if isinstance(x, int):\n        return x\n    max_c, run_m = 1, 1\n    for i in range(1, len(x)):\n        if x[i] - x[i-1] == 1:\n            run_m += 1\n        else:\n            if max_c < run_m:\n                max_c = run_m\n            run_m = 1\n    return max_c\n\ndef find_min_delta(x):\n    min_delta = 365\n    length = len(x)\n    if length < 2:\n        return min_delta\n    \n    for i in range(1, len(x)):\n        delta = x[i] - x[i-1]\n        if delta < min_delta:\n            min_delta = delta\n    return min_delta\n\ndef find_mean_delta(x):\n    mean_delta = 365\n    length = len(x)\n    if length < 2:\n        return mean_delta\n    \n    deltas = []\n    for i in range(1, len(x)):\n        deltas.append(x[i] - x[i-1])\n    return np.mean(deltas)\n\ndef find_max_delta(x):\n    max_delta = 0\n    length = len(x)\n    if length < 2:\n        return max_delta\n    \n    for i in range(1, len(x)):\n        delta = x[i] - x[i-1]\n        if delta > max_delta:\n            max_delta = delta\n    return max_delta","metadata":{"execution":{"iopub.status.busy":"2023-01-29T10:46:18.437688Z","iopub.execute_input":"2023-01-29T10:46:18.438117Z","iopub.status.idle":"2023-01-29T10:46:18.455747Z","shell.execute_reply.started":"2023-01-29T10:46:18.438079Z","shell.execute_reply":"2023-01-29T10:46:18.453568Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"users = pd.read_csv(\"/kaggle/input/onlineretail/OnlineRetail.csv\", parse_dates=[\"InvoiceDate\"], encoding='unicode_escape')\nusers[\"InvoiceDate\"] = pd.to_datetime(users[\"InvoiceDate\"].dt.date)\nusers.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-29T10:51:38.613045Z","iopub.execute_input":"2023-01-29T10:51:38.613535Z","iopub.status.idle":"2023-01-29T10:51:42.992368Z","shell.execute_reply.started":"2023-01-29T10:51:38.613494Z","shell.execute_reply":"2023-01-29T10:51:42.991271Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"  InvoiceNo StockCode                          Description  Quantity  \\\n0    536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n1    536365     71053                  WHITE METAL LANTERN         6   \n2    536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n3    536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n4    536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n\n  InvoiceDate  UnitPrice  CustomerID         Country  \n0  2010-12-01       2.55     17850.0  United Kingdom  \n1  2010-12-01       3.39     17850.0  United Kingdom  \n2  2010-12-01       2.75     17850.0  United Kingdom  \n3  2010-12-01       3.39     17850.0  United Kingdom  \n4  2010-12-01       3.39     17850.0  United Kingdom  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>InvoiceNo</th>\n      <th>StockCode</th>\n      <th>Description</th>\n      <th>Quantity</th>\n      <th>InvoiceDate</th>\n      <th>UnitPrice</th>\n      <th>CustomerID</th>\n      <th>Country</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>536365</td>\n      <td>85123A</td>\n      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n      <td>6</td>\n      <td>2010-12-01</td>\n      <td>2.55</td>\n      <td>17850.0</td>\n      <td>United Kingdom</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>536365</td>\n      <td>71053</td>\n      <td>WHITE METAL LANTERN</td>\n      <td>6</td>\n      <td>2010-12-01</td>\n      <td>3.39</td>\n      <td>17850.0</td>\n      <td>United Kingdom</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>536365</td>\n      <td>84406B</td>\n      <td>CREAM CUPID HEARTS COAT HANGER</td>\n      <td>8</td>\n      <td>2010-12-01</td>\n      <td>2.75</td>\n      <td>17850.0</td>\n      <td>United Kingdom</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>536365</td>\n      <td>84029G</td>\n      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>\n      <td>6</td>\n      <td>2010-12-01</td>\n      <td>3.39</td>\n      <td>17850.0</td>\n      <td>United Kingdom</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>536365</td>\n      <td>84029E</td>\n      <td>RED WOOLLY HOTTIE WHITE HEART.</td>\n      <td>6</td>\n      <td>2010-12-01</td>\n      <td>3.39</td>\n      <td>17850.0</td>\n      <td>United Kingdom</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# taking only UK\nusers_uk = users[users[\"Country\"] == \"United Kingdom\"]\nusers_uk.drop(\"Country\", axis=1, inplace=True)\n\n# split into target and working datasets: better select more months in target, e.g. 3 months!\nusers_nov = users_uk[(users_uk[\"InvoiceDate\"] > \"2011-08-31\") & (users_uk[\"InvoiceDate\"] < \"2011-12-01\")]\nusers_prev = users_uk[users_uk[\"InvoiceDate\"] < \"2011-09-01\"]\n\n# remove unavailable customer ids\nusers_prev = users_prev[users_prev[\"CustomerID\"].notna()]\nusers_nov = users_nov[users_nov[\"CustomerID\"].notna()]\n\n# compute churn target: users who never order anything in november\nusers_prev[\"target\"] = 1 - users_prev[\"CustomerID\"].isin(users_nov[\"CustomerID\"]).astype(int)\n\n# lower description\nusers_prev[\"Description\"] = users_prev[\"Description\"].str.lower()\n\n# revenue\nusers_prev[\"Revenue\"] = users_prev[\"UnitPrice\"] * users_prev[\"Quantity\"]\n\n# add date features\nusers_prev[\"DayMonth\"] = users_prev[\"InvoiceDate\"].dt.day.astype(\"str\")\nusers_prev[\"DayWeek\"] = users_prev[\"InvoiceDate\"].dt.dayofweek.astype(\"str\")\nusers_prev[\"Month\"] = users_prev[\"InvoiceDate\"].dt.month.astype(\"str\")\nusers_prev[\"WeekOfYear\"] = users_prev[\"InvoiceDate\"].dt.week\nusers_prev[\"DayOfYear\"] = users_prev[\"InvoiceDate\"].dt.dayofyear\n\n# calculate total customer's statistics\naggs = {\"Quantity\": [\"sum\", \"max\", \"min\", \"count\"], \"UnitPrice\": [\"mean\", \"max\", \"sum\", \"min\"]}\nstats = create_numerical_aggs(users_prev, groupby_id=\"CustomerID\", aggs=aggs, suffix=\"_by_id\")\nusers_prev = pd.merge(users_prev, stats, on=\"CustomerID\", how=\"left\")\n\n# number of returned orders\nqneg_users = users_prev[users_prev[\"Quantity\"] < 0]\nqneg_users = qneg_users.groupby(\"CustomerID\", as_index=False)[\"Quantity\"].count() # [\"Quantity\"].transform(\"count\")\nqneg_users.columns = [\"CustomerID\", \"return_cnt\"]\n\nusers_prev = users_prev.merge(qneg_users, on=\"CustomerID\", how=\"left\")\nusers_prev.fillna(0, inplace=True)\n\n# number of max consequtive days of ordering\ntmp = pd.concat([users_prev[\"CustomerID\"], users_prev[\"DayOfYear\"]], axis=1).sort_values([\"CustomerID\", \"DayOfYear\"]).sort_values([\"CustomerID\", \"DayOfYear\"])\ntmp.drop_duplicates(inplace=True)\ntmp = tmp.groupby(\"CustomerID\", as_index=True).agg({\"DayOfYear\": lambda x: (x - x.min() + 1)})\ntmp = tmp[\"DayOfYear\"].apply(lambda x: find_max_consequtive(x.tolist()))\ntmp = tmp.reset_index()\ntmp = tmp.rename(columns={\"DayOfYear\": \"conseq\"})\nusers_prev = users_prev.merge(tmp, on=\"CustomerID\", how=\"left\")\n\n# min, max, mean days between orders\ntmp = users_prev.groupby([\"CustomerID\"], as_index=False).agg({\"DayOfYear\": lambda x: sorted(list(set(x)))}).rename(columns={\"DayOfYear\": \"DaysList\"})\ntmp[\"min_delta\"] = tmp[\"DaysList\"].apply(find_min_delta)\ntmp[\"max_delta\"] = tmp[\"DaysList\"].apply(find_max_delta)\ntmp[\"mean_delta\"] = tmp[\"DaysList\"].apply(find_mean_delta)\ntmp.drop(\"DaysList\", axis=1, inplace=True)\n\nusers_prev = users_prev.merge(tmp, on=\"CustomerID\", how=\"left\")\n\n# compute monthly quantity and unit prices\naggs = {\"Quantity\": [\"sum\",],\n        \"Revenue\": [\"sum\",]}\n\nfor month in users_prev[\"Month\"].unique():\n    stats = create_numerical_aggs(users_prev[users_prev[\"Month\"] == month], groupby_id=[\"CustomerID\", \"Month\"], aggs=aggs, suffix=f\"_by_id_month_{month}\")\n    stats.drop(\"Month\", axis=1, inplace=True)\n    users_prev = users_prev.merge(stats, on=[\"CustomerID\"], how=\"left\")\n    \nusers_prev.fillna(0, inplace=True)  # assuming no orders in current month\n\nusers_prev[\"mean_upr_mothly\"] = users_prev[['revenue_sum_by_id_month_12',\n                                             'revenue_sum_by_id_month_1',\n                                             'revenue_sum_by_id_month_2',\n                                             'revenue_sum_by_id_month_3',\n                                             'revenue_sum_by_id_month_4',\n                                             'revenue_sum_by_id_month_5',\n                                             'revenue_sum_by_id_month_6',\n                                             'revenue_sum_by_id_month_7',\n                                             'revenue_sum_by_id_month_8',\n                                            # 'unitprice_sum_by_id_month_9',\n                                            # 'unitprice_sum_by_id_month_10'\n                                           ]].mean(axis=1)\n\nusers_prev[\"stdv_upr_mothly\"] = users_prev[['revenue_sum_by_id_month_12',\n                                             'revenue_sum_by_id_month_1',\n                                             'revenue_sum_by_id_month_2',\n                                             'revenue_sum_by_id_month_3',\n                                             'revenue_sum_by_id_month_4',\n                                             'revenue_sum_by_id_month_5',\n                                             'revenue_sum_by_id_month_6',\n                                             'revenue_sum_by_id_month_7',\n                                             'revenue_sum_by_id_month_8',\n                                            # 'unitprice_sum_by_id_month_9',\n                                            # 'unitprice_sum_by_id_month_10'\n                                           ]].std(axis=1)\n\nusers_prev[\"mean_qnt_mothly\"] = users_prev[['quantity_sum_by_id_month_12',\n                                            'quantity_sum_by_id_month_1',\n                                            'quantity_sum_by_id_month_2',\n                                            'quantity_sum_by_id_month_3',\n                                            'quantity_sum_by_id_month_4',\n                                            'quantity_sum_by_id_month_5',\n                                            'quantity_sum_by_id_month_6',\n                                            'quantity_sum_by_id_month_7',\n                                            'quantity_sum_by_id_month_8',\n                                           # 'quantity_sum_by_id_month_9',\n                                           # 'quantity_sum_by_id_month_10'\n                                           ]].mean(axis=1)\n\nusers_prev[\"stdv_qnt_mothly\"] = users_prev[['quantity_sum_by_id_month_12',\n                                            'quantity_sum_by_id_month_1',\n                                            'quantity_sum_by_id_month_2',\n                                            'quantity_sum_by_id_month_3',\n                                            'quantity_sum_by_id_month_4',\n                                            'quantity_sum_by_id_month_5',\n                                            'quantity_sum_by_id_month_6',\n                                            'quantity_sum_by_id_month_7',\n                                            'quantity_sum_by_id_month_8',\n                                           # 'quantity_sum_by_id_month_9',\n                                           # 'quantity_sum_by_id_month_10'\n                                           ]].std(axis=1)\n\n# ratios of revenue and quantity\nusers_prev[\"rev_678_to_345\"] = users_prev[['revenue_sum_by_id_month_6', 'revenue_sum_by_id_month_7', 'revenue_sum_by_id_month_8']].sum(axis=1) \\\n                                    / users_prev[['revenue_sum_by_id_month_3', 'revenue_sum_by_id_month_4', 'revenue_sum_by_id_month_5',]].sum(axis=1)\n\nusers_prev[\"qnt_678_to_345\"] = users_prev[['quantity_sum_by_id_month_6', 'quantity_sum_by_id_month_7', 'quantity_sum_by_id_month_8',]].sum(axis=1) \\\n                                    / users_prev[['quantity_sum_by_id_month_3', 'quantity_sum_by_id_month_4', 'quantity_sum_by_id_month_5',]].sum(axis=1)\n\n# lifetimes style recency: last date - first date \ntmp = users_prev.groupby(\"CustomerID\", as_index=False).agg({\"InvoiceDate\": lambda x: (x.max() - x.min()).days}).rename(columns={\"InvoiceDate\": \"LTRecency\"})\nusers_prev = users_prev.merge(tmp, on=\"CustomerID\", how=\"left\")\n\n# compute frequency\ntmp = users_prev.groupby(\"CustomerID\", as_index=False)[\"InvoiceNo\"].count().rename(columns={\"InvoiceNo\": \"frequency\"})\nusers_prev = users_prev.merge(tmp, on=\"CustomerID\", how=\"left\")\n\n# compute recency\ndf_recency = users_prev.groupby(\"CustomerID\", as_index=False)[\"InvoiceDate\"].max()\ndf_recency.columns = ['CustomerID', 'LastPurchaseDate']\nrecent_date = df_recency['LastPurchaseDate'].max()\n\ndf_recency['Recency'] = df_recency['LastPurchaseDate'].apply(lambda x: (recent_date - x).days)\ndf_recency.drop(\"LastPurchaseDate\", axis=1, inplace=True)\n\nusers_prev = users_prev.merge(df_recency, on=\"CustomerID\", how=\"left\")\n\nusers_prev['R_rank'] = users_prev['Recency'].rank(ascending=False)\nusers_prev['F_rank'] = users_prev['frequency'].rank(ascending=True)\nusers_prev['M_rank'] = users_prev['Revenue'].rank(ascending=True)\n\n# normalizing rank\nusers_prev['R_rank'] = (users_prev['R_rank']/users_prev['R_rank'].max())*100\nusers_prev['F_rank'] = (users_prev['F_rank']/users_prev['F_rank'].max())*100\nusers_prev['M_rank'] = (users_prev['F_rank']/users_prev['M_rank'].max())*100\n\n# calculating RFM-score\nalpha = beta = gamma = 1\nmult = 0.05\n\nusers_prev['RFM_Score'] = alpha * users_prev['R_rank'] + beta * users_prev['F_rank'] + gamma * users_prev['M_rank']\nusers_prev['RFM_Score'] *= mult\n\n# text features: cut on most recent 1000 tokens\ntmp = users_prev.sort_values(\"InvoiceDate\").groupby([\"CustomerID\"], as_index=False)[\"Description\"].sum().rename(columns={\"Description\": \"Text\"})\ntmp[\"Text\"] = tmp[\"Text\"].apply(lambda x: \" \".join(x.split()[-1000:]))\nusers_prev = users_prev.merge(tmp, on=\"CustomerID\", how=\"left\").drop(\"Description\", axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-01-29T10:51:45.790104Z","iopub.execute_input":"2023-01-29T10:51:45.790516Z","iopub.status.idle":"2023-01-29T10:51:56.358571Z","shell.execute_reply.started":"2023-01-29T10:51:45.790483Z","shell.execute_reply":"2023-01-29T10:51:56.357013Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"users_prev.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-01-29T10:52:01.525449Z","iopub.execute_input":"2023-01-29T10:52:01.526003Z","iopub.status.idle":"2023-01-29T10:52:01.558779Z","shell.execute_reply.started":"2023-01-29T10:52:01.525950Z","shell.execute_reply":"2023-01-29T10:52:01.557498Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"  InvoiceNo StockCode  Quantity InvoiceDate  UnitPrice  CustomerID  target  \\\n0    536365    85123A         6  2010-12-01       2.55     17850.0       1   \n1    536365     71053         6  2010-12-01       3.39     17850.0       1   \n\n   Revenue DayMonth DayWeek  ... rev_678_to_345  qnt_678_to_345  LTRecency  \\\n0    15.30        1       2  ...            NaN             NaN         71   \n1    20.34        1       2  ...            NaN             NaN         71   \n\n   frequency  Recency    R_rank     F_rank    M_rank  RFM_Score  \\\n0        312      202  4.205177  76.582104  0.037329    4.04123   \n1        312      202  4.205177  76.582104  0.037329    4.04123   \n\n                                                Text  \n0  antique white wooden picture frame white finis...  \n1  antique white wooden picture frame white finis...  \n\n[2 rows x 58 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>InvoiceNo</th>\n      <th>StockCode</th>\n      <th>Quantity</th>\n      <th>InvoiceDate</th>\n      <th>UnitPrice</th>\n      <th>CustomerID</th>\n      <th>target</th>\n      <th>Revenue</th>\n      <th>DayMonth</th>\n      <th>DayWeek</th>\n      <th>...</th>\n      <th>rev_678_to_345</th>\n      <th>qnt_678_to_345</th>\n      <th>LTRecency</th>\n      <th>frequency</th>\n      <th>Recency</th>\n      <th>R_rank</th>\n      <th>F_rank</th>\n      <th>M_rank</th>\n      <th>RFM_Score</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>536365</td>\n      <td>85123A</td>\n      <td>6</td>\n      <td>2010-12-01</td>\n      <td>2.55</td>\n      <td>17850.0</td>\n      <td>1</td>\n      <td>15.30</td>\n      <td>1</td>\n      <td>2</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>71</td>\n      <td>312</td>\n      <td>202</td>\n      <td>4.205177</td>\n      <td>76.582104</td>\n      <td>0.037329</td>\n      <td>4.04123</td>\n      <td>antique white wooden picture frame white finis...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>536365</td>\n      <td>71053</td>\n      <td>6</td>\n      <td>2010-12-01</td>\n      <td>3.39</td>\n      <td>17850.0</td>\n      <td>1</td>\n      <td>20.34</td>\n      <td>1</td>\n      <td>2</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>71</td>\n      <td>312</td>\n      <td>202</td>\n      <td>4.205177</td>\n      <td>76.582104</td>\n      <td>0.037329</td>\n      <td>4.04123</td>\n      <td>antique white wooden picture frame white finis...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows × 58 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"users_prev[\"CustomerID\"].nunique()  # small amount of unique users","metadata":{"execution":{"iopub.status.busy":"2023-01-28T22:58:19.461315Z","iopub.execute_input":"2023-01-28T22:58:19.461681Z","iopub.status.idle":"2023-01-28T22:58:19.477508Z","shell.execute_reply.started":"2023-01-28T22:58:19.461651Z","shell.execute_reply":"2023-01-28T22:58:19.476000Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"3027"},"metadata":{}}]},{"cell_type":"code","source":"# aggregate everything\nusers = users_prev.groupby(\"CustomerID\").agg({\n    \"StockCode\": [\"last\", lambda x: x.nunique()],\n    \"target\": \"last\",\n    \"Revenue\": \"last\",\n    \"DayWeek\": lambda x: x.value_counts().index[0],\n    \"Month\": \"last\",\n    \"quantity_sum_by_id\": \"last\",\n    'quantity_max_by_id': \"last\",\n    'quantity_min_by_id': \"last\", \n    'quantity_count_by_id': \"last\",\n    'unitprice_mean_by_id': \"last\",\n    'unitprice_max_by_id': \"last\",\n    'unitprice_sum_by_id': \"last\",\n    'unitprice_min_by_id': \"last\",\n    'return_cnt': \"last\",\n    'conseq': \"last\",\n    'min_delta': \"last\", \n    'max_delta': \"last\", \n    'mean_delta': \"last\",\n    'quantity_sum_by_id_month_12': \"last\", \n    'revenue_sum_by_id_month_12': \"last\",\n    'quantity_sum_by_id_month_1': \"last\", \n    'revenue_sum_by_id_month_1': \"last\",\n    'quantity_sum_by_id_month_2': \"last\", \n    'revenue_sum_by_id_month_2': \"last\",\n    'quantity_sum_by_id_month_3': \"last\", \n    'revenue_sum_by_id_month_3': \"last\",\n    'quantity_sum_by_id_month_4': \"last\", \n    'revenue_sum_by_id_month_4': \"last\",\n    'quantity_sum_by_id_month_5': \"last\", \n    'revenue_sum_by_id_month_5': \"last\",\n    'quantity_sum_by_id_month_6': \"last\", \n    'revenue_sum_by_id_month_6': \"last\",\n    'quantity_sum_by_id_month_7': \"last\", \n    'revenue_sum_by_id_month_7': \"last\",\n    'quantity_sum_by_id_month_8': \"last\", \n    'revenue_sum_by_id_month_8': \"last\",\n    'rev_678_to_345': \"last\",\n    'qnt_678_to_345': \"last\",\n    'mean_upr_mothly': \"last\", \n    'stdv_upr_mothly': \"last\", \n    'mean_qnt_mothly': \"last\",\n    'stdv_qnt_mothly': \"last\", \n    'LTRecency': \"last\", \n    'frequency': \"last\", \n    'Recency': \"last\", \n    'R_rank': \"last\",\n    'F_rank': \"last\", \n    'M_rank': \"last\", \n    'RFM_Score': \"last\", \n    'Text': \"last\"\n})\nusers.columns = [col[0] if col[0] != \"StockCode\" else \"StockCode\" + str(i+1) for i, col in enumerate(users.columns)]","metadata":{"execution":{"iopub.status.busy":"2023-01-29T10:52:37.641990Z","iopub.execute_input":"2023-01-29T10:52:37.642447Z","iopub.status.idle":"2023-01-29T10:52:39.547358Z","shell.execute_reply.started":"2023-01-29T10:52:37.642410Z","shell.execute_reply":"2023-01-29T10:52:39.545297Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"X, y = users.drop(\"target\", axis=1), users[\"target\"]\nX.reset_index(drop=True, inplace=True)\ny.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-29T10:52:44.387139Z","iopub.execute_input":"2023-01-29T10:52:44.387623Z","iopub.status.idle":"2023-01-29T10:52:44.399560Z","shell.execute_reply.started":"2023-01-29T10:52:44.387586Z","shell.execute_reply":"2023-01-29T10:52:44.398132Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def print_scores(folds_scores, train_scores):\n    print(f\"Train score by each fold: {train_scores}\")\n    print(f\"Valid score by each fold: {folds_scores}\")\n    print(f\"Train mean score by each fold:{np.mean(train_scores):.5f} +/- {np.std(train_scores):.5f}\")\n    print(f\"Valid mean score by each fold:{np.mean(folds_scores):.5f} +/- {np.std(folds_scores):.5f}\")\n    print(\"*\" * 50)\n    \ndef create_bootstrap_samples(data: np.array, n_samples: int = 1000) -> np.array:\n    bootstrap_idx = np.random.randint(\n        low=0, high=len(data), size=(n_samples, len(data))\n    )\n    return bootstrap_idx\n\n\ndef create_bootstrap_metrics(y_true: np.array,\n                             y_pred: np.array,\n                             metric: callable,\n                             n_samlpes: int = 1000) -> list:\n    scores = []\n\n    if isinstance(y_true, pd.Series):\n        y_true = y_true.values\n\n    bootstrap_idx = create_bootstrap_samples(y_true)\n    for idx in bootstrap_idx:\n        y_true_bootstrap = y_true[idx]\n        y_pred_bootstrap = y_pred[idx]\n\n        score = metric(y_true_bootstrap, y_pred_bootstrap)\n        scores.append(score)\n\n    return scores\n\n\ndef calculate_confidence_interval(scores: list, conf_interval: float = 0.95) -> tuple:\n    left_bound = np.percentile(\n        scores, ((1 - conf_interval) / 2) * 100\n    )\n    right_bound = np.percentile(\n        scores, (conf_interval + ((1 - conf_interval) / 2)) * 100\n    )\n\n    return left_bound, right_bound\n\ndef _predict(estimator, x_valid, probas=True):\n    if hasattr(estimator, \"predict_proba\") and probas:\n        y_pred = estimator.predict_proba(x_valid)[:, 1]\n    else:\n        y_pred = estimator.predict(x_valid)\n\n    return y_pred\n\ndef calculate_permutation_importance(estimator,\n                                     metric: callable,\n                                     x_valid: pd.DataFrame,\n                                     y_valid: pd.DataFrame,\n                                     maximize: bool = True,\n                                     probas: bool = False\n                                     ) -> pd.Series:\n    y_pred = _predict(estimator, x_valid, probas)\n    base_score = metric(y_valid, y_pred)\n    scores, delta = {}, {}\n\n    for feature in tqdm(x_valid.columns):\n        x_valid_ = x_valid.copy(deep=True)\n        np.random.seed(42)\n        x_valid_[feature] = np.random.permutation(x_valid_[feature])\n\n        y_pred = _predict(estimator, x_valid_, probas)\n        feature_score = metric(y_valid, y_pred)\n\n        if maximize:\n            delta[feature] = base_score - feature_score\n        else:\n            delta[feature] = feature_score - base_score\n\n        scores[feature] = feature_score\n\n    scores, delta = pd.Series(scores), pd.Series(delta)\n    scores = scores.sort_values(ascending=False)\n    delta = delta.sort_values(ascending=False)\n\n    return scores, delta\n    \ndef catboost_cross_validation(X: pd.DataFrame,\n                              y: pd.Series,\n                              params: dict = None,\n                              cv=None,\n                              categorical: list = None,\n                              textual: list = None,\n                              rounds: int = 50,\n                              verbose: bool = True,\n                              preprocess: object = None,\n                              score_fn: callable = roc_auc_score,\n                              calculate_ci: bool = False,\n                              n_samples: int = 1000,\n                              confidence: float = 0.95,\n                              best_iter: str = \"median\",\n                              seed: int = 42):\n\n    minor_class_counts = y.value_counts(normalize=True).values[-1]\n\n    if cv is None:\n        if minor_class_counts >= 0.05:\n            cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n        else:\n            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n\n    if params is None:\n        if len(X) <= 50_000:\n            sub_params = {\n                \"grow_policy\": \"SymmetricTree\",\n                \"boosting_type\": \"Ordered\",\n                \"score_function\": \"Cosine\",\n                \"depth\": 6,\n            }\n        else:\n            sub_params = {\n                \"grow_policy\": \"Lossguide\",\n                \"boosting_type\": \"Plain\",\n                \"score_function\": \"L2\",\n                \"depth\": 16,\n                \"min_data_in_leaf\": 200,\n                \"max_leaves\": 2**16 // 8,\n            }\n        params = {\n            \"iterations\": 1000,\n            \"learning_rate\": 0.01,\n            \"loss_function\": \"Logloss\",\n            \"eval_metric\": \"AUC\",\n            \"task_type\": \"CPU\",\n            \"use_best_model\": True,\n            \"thread_count\": -1,\n            \"silent\": True,\n            \"random_seed\": seed,\n            \"allow_writing_files\": False,\n            \"auto_class_weights\": \"SqrtBalanced\" if minor_class_counts < 0.05 else None,\n            \"bagging_temperature\": 1,\n            \"max_bin\": 255,\n            \"l2_leaf_reg\": 10,\n            \"subsample\": 0.9,\n            \"bootstrap_type\": \"MVS\",\n            \"colsample_bylevel\": 0.9,\n        }\n        params.update(sub_params)\n\n    prediction_type = \"Probability\" if score_fn.__name__ == \"roc_auc_score\" else \"Class\"\n\n    estimators, folds_scores, train_scores = [], [], []\n\n    oof_preds = np.zeros(X.shape[0])\n\n    if verbose:\n        print(f\"{time.ctime()}, Cross-Validation, {X.shape[0]} rows, {X.shape[1]} cols\")\n        print(\"Estimating best number of trees.\")\n\n    best_iterations = []\n\n    for fold, (train_idx, valid_idx) in enumerate(cv.split(X, y)):\n        x_train, x_valid = X.loc[train_idx], X.loc[valid_idx]\n        y_train, y_valid = y[train_idx], y[valid_idx]\n\n        if preprocess is not None:\n            x_train = preprocess.fit_transform(x_train, y_train)\n            x_valid = preprocess.transform(x_valid)\n\n        train_pool = Pool(x_train, y_train, cat_features=categorical, text_features=textual)\n        valid_pool = Pool(x_valid, y_valid, cat_features=categorical, text_features=textual)\n\n        model = CatBoostClassifier(**params).fit(\n            train_pool,\n            eval_set=valid_pool,\n            early_stopping_rounds=rounds\n            )\n\n        best_iterations.append(model.get_best_iteration())\n    \n    if best_iter == \"median\":\n        best_iteration = int(np.median(best_iterations))  \n    elif best_iter == \"mean\":\n        best_iteration = int(np.mean(best_iterations))\n    else:\n        raise NotImplementedError(\"Set best_iter median or mean\")\n        \n    params[\"iterations\"] = best_iteration\n\n    cv.random_state = seed % 3\n    if verbose:\n        print(f\"Evaluating cross validation with {best_iteration} trees.\")\n\n    for fold, (train_idx, valid_idx) in enumerate(cv.split(X, y)):\n\n        x_train, x_valid = X.loc[train_idx], X.loc[valid_idx]\n        y_train, y_valid = y[train_idx], y[valid_idx]\n\n        if preprocess is not None:\n            x_train = preprocess.fit_transform(x_train, y_train)\n            x_valid = preprocess.transform(x_valid)\n\n        train_pool = Pool(x_train, y_train, cat_features=categorical, text_features=textual)\n        valid_pool = Pool(x_valid, y_valid, cat_features=categorical, text_features=textual)\n\n        model = CatBoostClassifier(**params).fit(\n            train_pool,\n            eval_set=valid_pool,\n            )\n\n        train_score = catboost.CatBoost.predict(model, train_pool, prediction_type=prediction_type)\n        if prediction_type == \"Probability\":\n            train_score = train_score[:, 1]\n        train_score = score_fn(y_train, train_score)\n\n        valid_scores = catboost.CatBoost.predict(model, valid_pool, prediction_type=prediction_type)\n        if prediction_type == \"Probability\":\n            valid_scores = valid_scores[:, 1]\n\n        oof_preds[valid_idx] = valid_scores\n        score = score_fn(y_valid, oof_preds[valid_idx])\n\n        folds_scores.append(round(score, 5))\n        train_scores.append(round(train_score, 5))\n\n        if verbose:\n            print(f\"Fold {fold + 1}, Train score = {train_score:.5f}, Valid score = {score:.5f}\")\n        estimators.append(model)\n\n    if verbose:\n        oof_scores = score_fn(y, oof_preds)\n        print_scores(folds_scores, train_scores)\n        print(f\"OOF-score {score_fn.__name__}: {oof_scores:.5f}\")\n        if calculate_ci:\n            bootstrap_scores = create_bootstrap_metrics(y, oof_preds, score_fn, n_samlpes=n_samples)\n            left_bound, right_bound = calculate_confidence_interval(bootstrap_scores, conf_interval=confidence)\n            print(f\"Expected metric value lies between: {left_bound:.5f} and {right_bound:.5f}\",\n                  f\"with confidence of {confidence*100}%\")\n\n    return estimators, oof_preds, np.mean(folds_scores)","metadata":{"execution":{"iopub.status.busy":"2023-01-29T11:16:59.424417Z","iopub.execute_input":"2023-01-29T11:16:59.424861Z","iopub.status.idle":"2023-01-29T11:16:59.477976Z","shell.execute_reply.started":"2023-01-29T11:16:59.424823Z","shell.execute_reply":"2023-01-29T11:16:59.475602Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"y.value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-29T10:52:48.811829Z","iopub.execute_input":"2023-01-29T10:52:48.812801Z","iopub.status.idle":"2023-01-29T10:52:48.823970Z","shell.execute_reply.started":"2023-01-29T10:52:48.812748Z","shell.execute_reply":"2023-01-29T10:52:48.822726Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"0    0.564255\n1    0.435745\nName: target, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"cb_params = {\n            \"grow_policy\": \"SymmetricTree\",\n            \"boosting_type\": \"Ordered\",\n            \"score_function\": \"Cosine\",\n            \"depth\": 3,\n            \"iterations\": 1000,\n            \"learning_rate\": 0.1,\n            \"loss_function\": \"Logloss\",\n            \"eval_metric\": \"AUC\",\n            \"task_type\": \"CPU\",\n            \"use_best_model\": True,\n            \"thread_count\": -1,\n            \"silent\": True,\n            \"random_seed\": 42,\n            \"allow_writing_files\": False,\n            \"auto_class_weights\": None,\n            \"bagging_temperature\": 1,\n            \"max_bin\": 16,  # tune\n            \"l2_leaf_reg\": 69,\n            \"subsample\": 0.8,\n            \"bootstrap_type\": \"MVS\",\n            \"colsample_bylevel\": 0.6,\n            # \"max_ctr_complexity\": 4,\n            # \"random_strength\": 0.8,\n}\n\ncategorical = [col for col in users.columns if users[col].dtype == \"object\" and col != \"Text\"]\ntextual = [\"Text\",]\n# cv = StratifiedKFold(n_splits=6, shuffle=True, random_state=42)\ncv = KFold(n_splits=6, shuffle=True, random_state=42)\n\n_, _, _ = catboost_cross_validation(X, y, \n                                    params=cb_params, \n                                    rounds=50, \n                                    cv=cv, \n                                    calculate_ci=True, \n                                    categorical=categorical,\n                                    textual=textual,\n                                    preprocess=None,\n                                    seed=42)","metadata":{"execution":{"iopub.status.busy":"2023-01-29T10:54:42.383835Z","iopub.execute_input":"2023-01-29T10:54:42.384246Z","iopub.status.idle":"2023-01-29T10:56:27.204041Z","shell.execute_reply.started":"2023-01-29T10:54:42.384213Z","shell.execute_reply":"2023-01-29T10:56:27.202745Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Sun Jan 29 10:54:42 2023, Cross-Validation, 3027 rows, 50 cols\nEstimating best number of trees.\nEvaluating cross validation with 36 trees.\nFold 1, Train score = 0.74921, Valid score = 0.76623\nFold 2, Train score = 0.76728, Valid score = 0.71369\nFold 3, Train score = 0.76141, Valid score = 0.73651\nFold 4, Train score = 0.75472, Valid score = 0.77199\nFold 5, Train score = 0.75983, Valid score = 0.74510\nFold 6, Train score = 0.75464, Valid score = 0.74979\nTrain score by each fold: [0.74921, 0.76728, 0.76141, 0.75472, 0.75983, 0.75464]\nValid score by each fold: [0.76623, 0.71369, 0.73651, 0.77199, 0.7451, 0.74979]\nTrain mean score by each fold:0.75785 +/- 0.00578\nValid mean score by each fold:0.74722 +/- 0.01926\n**************************************************\nOOF-score roc_auc_score: 0.74356\nExpected metric value lies between: 0.72596 and 0.76021 with confidence of 95.0%\n","output_type":"stream"}]},{"cell_type":"code","source":"for fold, (train_idx, valid_idx) in enumerate(KFold(n_splits=6, shuffle=True, random_state=42).split(X, y)):\n    if fold == 3:\n        x_train, x_valid = X.loc[train_idx], X.loc[valid_idx]\n        y_train, y_valid = y[train_idx], y[valid_idx]","metadata":{"execution":{"iopub.status.busy":"2023-01-29T11:02:14.331753Z","iopub.execute_input":"2023-01-29T11:02:14.332206Z","iopub.status.idle":"2023-01-29T11:02:14.344957Z","shell.execute_reply.started":"2023-01-29T11:02:14.332167Z","shell.execute_reply":"2023-01-29T11:02:14.343660Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"scores, deltas = calculate_permutation_importance(CatBoostClassifier(**cb_params).fit(X.loc[train_idx], \n                                                                                      y[train_idx],\n                                                                                      eval_set=[(X.loc[valid_idx], y[valid_idx]),],\n                                                                                      cat_features=categorical, \n                                                                                      text_features=textual),\n                                                 roc_auc_score,\n                                                 X.loc[valid_idx],\n                                                 y[valid_idx],\n                                                 maximize=True,\n                                                 probas=True\n                                                 )","metadata":{"execution":{"iopub.status.busy":"2023-01-29T11:02:36.448583Z","iopub.execute_input":"2023-01-29T11:02:36.449014Z","iopub.status.idle":"2023-01-29T11:02:44.575696Z","shell.execute_reply.started":"2023-01-29T11:02:36.448977Z","shell.execute_reply":"2023-01-29T11:02:44.574480Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"100%|██████████| 50/50 [00:02<00:00, 24.99it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"deltas[:20]","metadata":{"execution":{"iopub.status.busy":"2023-01-29T11:03:02.725652Z","iopub.execute_input":"2023-01-29T11:03:02.726026Z","iopub.status.idle":"2023-01-29T11:03:02.735641Z","shell.execute_reply.started":"2023-01-29T11:03:02.725996Z","shell.execute_reply":"2023-01-29T11:03:02.734564Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"Month                         0.005560\nmax_delta                     0.001392\nLTRecency                     0.001384\nqnt_678_to_345                0.001368\nmean_upr_mothly               0.001183\nRecency                       0.001143\nquantity_sum_by_id_month_5    0.000990\nrevenue_sum_by_id_month_7     0.000797\nquantity_sum_by_id_month_1    0.000644\nunitprice_mean_by_id          0.000378\nRFM_Score                     0.000233\nrevenue_sum_by_id_month_5     0.000121\nrevenue_sum_by_id_month_2     0.000097\nrevenue_sum_by_id_month_8     0.000056\nquantity_count_by_id          0.000040\nquantity_min_by_id            0.000016\nrev_678_to_345                0.000000\nR_rank                        0.000000\nquantity_sum_by_id_month_8    0.000000\nrevenue_sum_by_id_month_4     0.000000\ndtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"deltas = deltas[deltas>0].index.tolist()","metadata":{"execution":{"iopub.status.busy":"2023-01-29T11:03:40.231208Z","iopub.execute_input":"2023-01-29T11:03:40.231642Z","iopub.status.idle":"2023-01-29T11:03:40.237333Z","shell.execute_reply.started":"2023-01-29T11:03:40.231605Z","shell.execute_reply":"2023-01-29T11:03:40.236333Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"cb_params = {\n            \"grow_policy\": \"SymmetricTree\",\n            \"boosting_type\": \"Ordered\",\n            \"score_function\": \"Cosine\",\n            \"depth\": 3,\n            \"iterations\": 1000,\n            \"learning_rate\": 0.01,\n            \"loss_function\": \"Logloss\",\n            \"eval_metric\": \"AUC\",\n            \"task_type\": \"CPU\",\n            \"use_best_model\": True,\n            \"thread_count\": -1,\n            \"silent\": True,\n            \"random_seed\": 42,\n            \"allow_writing_files\": False,\n            #\"auto_class_weights\": None,\n            #\"bagging_temperature\": 0,\n            \"max_bin\": 16,  # tune\n            \"l2_leaf_reg\": 100,\n            \"subsample\": 1,\n            \"bootstrap_type\": \"MVS\",\n            \"colsample_bylevel\": 0.9,\n            # \"max_ctr_complexity\": 4,\n            \"random_strength\": 7,\n}\n\ncategorical = [col for col in deltas if users[col].dtype == \"object\" and col != \"Text\"]\n\n# cv = StratifiedKFold(n_splits=6, shuffle=True, random_state=42)\ncv = KFold(n_splits=6, shuffle=True, random_state=42)\n\n_, _, _ = catboost_cross_validation(X[deltas], y, \n                                    params=cb_params, \n                                    rounds=50, \n                                    cv=cv, \n                                    score_fn=accuracy_score,  # fine balance\n                                    calculate_ci=True, \n                                    categorical=categorical,\n                                    textual=None,\n                                    preprocess=None,\n                                    best_iter=\"median\",\n                                    seed=42)","metadata":{"execution":{"iopub.status.busy":"2023-01-29T11:17:41.707589Z","iopub.execute_input":"2023-01-29T11:17:41.707963Z","iopub.status.idle":"2023-01-29T11:17:47.086353Z","shell.execute_reply.started":"2023-01-29T11:17:41.707934Z","shell.execute_reply":"2023-01-29T11:17:47.084897Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"Sun Jan 29 11:17:41 2023, Cross-Validation, 3027 rows, 16 cols\nEstimating best number of trees.\nEvaluating cross validation with 64 trees.\nFold 1, Train score = 0.68517, Valid score = 0.71683\nFold 2, Train score = 0.69033, Valid score = 0.63762\nFold 3, Train score = 0.68239, Valid score = 0.66931\nFold 4, Train score = 0.68292, Valid score = 0.71230\nFold 5, Train score = 0.69441, Valid score = 0.68056\nFold 6, Train score = 0.69005, Valid score = 0.69444\nTrain score by each fold: [0.68517, 0.69033, 0.68239, 0.68292, 0.69441, 0.69005]\nValid score by each fold: [0.71683, 0.63762, 0.66931, 0.7123, 0.68056, 0.69444]\nTrain mean score by each fold:0.68754 +/- 0.00437\nValid mean score by each fold:0.68518 +/- 0.02695\n**************************************************\nOOF-score accuracy_score: 0.68517\nExpected metric value lies between: 0.66799 and 0.70135 with confidence of 95.0%\n","output_type":"stream"}]},{"cell_type":"code","source":"cb_params = {\n            \"grow_policy\": \"Lossguide\",\n            \"boosting_type\": \"Plain\",\n            \"score_function\": \"L2\",\n            \"depth\": 6,\n            \"iterations\": 1000,\n            \"learning_rate\": 0.01,\n            \"loss_function\": \"Logloss\",\n            \"eval_metric\": \"F1\",\n            \"task_type\": \"CPU\",\n            \"use_best_model\": True,\n            \"thread_count\": -1,\n            \"silent\": True,\n            \"random_seed\": 42,\n            \"allow_writing_files\": False,\n            #\"auto_class_weights\": None,\n            #\"bagging_temperature\": 0,\n            \"max_bin\": 16,  # tune\n            \"l2_leaf_reg\": 100,\n            \"subsample\": 1,\n            \"bootstrap_type\": \"MVS\",\n            \"colsample_bylevel\": 0.9,\n            # \"max_ctr_complexity\": 4,\n            \"random_strength\": 7,\n            \"max_leaves\": 18,\n            \"min_data_in_leaf\": 10,\n}\n\ncategorical = [col for col in deltas if users[col].dtype == \"object\" and col != \"Text\"]\n\n# cv = StratifiedKFold(n_splits=6, shuffle=True, random_state=42)\ncv = KFold(n_splits=6, shuffle=True, random_state=42)\n\n_, _, _ = catboost_cross_validation(X[deltas], y, \n                                    params=cb_params, \n                                    rounds=50, \n                                    cv=cv, \n                                    score_fn=accuracy_score,  # fine balance\n                                    calculate_ci=True, \n                                    categorical=categorical,\n                                    textual=None,\n                                    preprocess=None,\n                                    best_iter=\"mean\",\n                                    seed=42)","metadata":{"execution":{"iopub.status.busy":"2023-01-29T11:17:03.815070Z","iopub.execute_input":"2023-01-29T11:17:03.815491Z","iopub.status.idle":"2023-01-29T11:17:08.346838Z","shell.execute_reply.started":"2023-01-29T11:17:03.815456Z","shell.execute_reply":"2023-01-29T11:17:08.345526Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"Sun Jan 29 11:17:03 2023, Cross-Validation, 3027 rows, 16 cols\nEstimating best number of trees.\nEvaluating cross validation with 43 trees.\nFold 1, Train score = 0.68715, Valid score = 0.69901\nFold 2, Train score = 0.69627, Valid score = 0.65347\nFold 3, Train score = 0.68319, Valid score = 0.67327\nFold 4, Train score = 0.68371, Valid score = 0.71429\nFold 5, Train score = 0.69243, Valid score = 0.70040\nFold 6, Train score = 0.69283, Valid score = 0.67857\nTrain score by each fold: [0.68715, 0.69627, 0.68319, 0.68371, 0.69243, 0.69283]\nValid score by each fold: [0.69901, 0.65347, 0.67327, 0.71429, 0.7004, 0.67857]\nTrain mean score by each fold:0.68926 +/- 0.00490\nValid mean score by each fold:0.68650 +/- 0.02021\n**************************************************\nOOF-score accuracy_score: 0.68649\nExpected metric value lies between: 0.66963 and 0.70301 with confidence of 95.0%\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}