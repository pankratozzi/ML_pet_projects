{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Marketing analysis\nImport all necessary dependences","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nfrom IPython.display import clear_output\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\nfrom sklearn.model_selection import train_test_split, cross_val_score, learning_curve\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\nfrom sklearn.metrics import classification_report, accuracy_score, roc_curve, auc\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\nfrom sklearn.base import BaseEstimator, TransformerMixin, clone\nfrom sklearn.compose import ColumnTransformer\n\nimport category_encoders as ce\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer, SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA, KernelPCA\n\nfrom scipy import stats\nimport statsmodels.stats.power as power\n\nfrom imblearn.pipeline import Pipeline as Pipe\nfrom itertools import combinations\nfrom imblearn.combine import SMOTETomek\n\nfrom lightgbm import LGBMClassifier\nimport optuna\nfrom optuna.samplers import TPESampler\n\nimport category_encoders as ce\nimport shap\n!pip install -q BorutaShap\nfrom BorutaShap import BorutaShap\n\nseed = 123\nnp.random.seed(seed)\nclear_output()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:45:56.828768Z","iopub.execute_input":"2022-04-03T18:45:56.829145Z","iopub.status.idle":"2022-04-03T18:46:13.816268Z","shell.execute_reply.started":"2022-04-03T18:45:56.829054Z","shell.execute_reply":"2022-04-03T18:46:13.815505Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"path = r'../input/customer-personality-analysis/marketing_campaign.csv'\nTARGET_NAME = 'Response'","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:46:13.818265Z","iopub.execute_input":"2022-04-03T18:46:13.818530Z","iopub.status.idle":"2022-04-03T18:46:13.822178Z","shell.execute_reply.started":"2022-04-03T18:46:13.818475Z","shell.execute_reply":"2022-04-03T18:46:13.821624Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(path, sep='\\t')\ndf.sample(5).transpose()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:46:13.822993Z","iopub.execute_input":"2022-04-03T18:46:13.823198Z","iopub.status.idle":"2022-04-03T18:46:13.896918Z","shell.execute_reply.started":"2022-04-03T18:46:13.823171Z","shell.execute_reply":"2022-04-03T18:46:13.896032Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def show_proba_calibration_plots(y_predicted_probs, y_true_labels):\n    preds_with_true_labels = np.array(list(zip(y_predicted_probs, y_true_labels)))\n\n    thresholds = []\n    precisions = []\n    recalls = []\n    f1_scores = []\n\n    for threshold in np.linspace(0.1, 0.9, 9):\n        thresholds.append(threshold)\n        precisions.append(precision_score(y_true_labels, list(map(int, y_predicted_probs > threshold))))\n        recalls.append(recall_score(y_true_labels, list(map(int, y_predicted_probs > threshold))))\n        f1_scores.append(f1_score(y_true_labels, list(map(int, y_predicted_probs > threshold))))\n\n    scores_table = pd.DataFrame({'f1':f1_scores,\n                                 'precision':precisions,\n                                 'recall':recalls,\n                                 'probability':thresholds}).sort_values('f1', ascending=False).round(3)\n  \n    figure = plt.figure(figsize = (15, 5))\n\n    plt1 = figure.add_subplot(121)\n    plt1.plot(thresholds, precisions, label='Precision', linewidth=4)\n    plt1.plot(thresholds, recalls, label='Recall', linewidth=4)\n    plt1.plot(thresholds, f1_scores, label='F1', linewidth=4)\n    plt1.set_ylabel('Scores')\n    plt1.set_xlabel('Probability threshold')\n    plt1.set_title('Probabilities threshold calibration')\n    plt1.legend(bbox_to_anchor=(0.25, 0.25))   \n    plt1.table(cellText = scores_table.values,\n               colLabels = scores_table.columns, \n               colLoc = 'center', cellLoc = 'center', loc = 'bottom', bbox = [0, -1.3, 1, 1])\n    plt2 = figure.add_subplot(122)\n    plt2.hist(preds_with_true_labels[preds_with_true_labels[:, 1] == 0][:, 0], \n              label='Another class', color='royalblue', alpha=1)\n    plt2.hist(preds_with_true_labels[preds_with_true_labels[:, 1] == 1][:, 0], \n              label='Main class', color='darkcyan', alpha=0.8)\n    plt2.set_ylabel('Number of examples')\n    plt2.set_xlabel('Probabilities')\n    plt2.set_title('Probability histogram')\n    plt2.legend(bbox_to_anchor=(1, 1))\n\n    plt.show()\n\ndef report(y_train, y_train_pred, y_test, y_test_pred, y_train_proba=None, y_test_proba=None):\n    print('Train\\n', classification_report(y_train, y_train_pred, digits=3))\n    print('Test\\n', classification_report(y_test, y_test_pred, digits=3))\n    if y_train_proba is not None and y_test_proba is not None:\n        roc_train, roc_test = roc_auc_score(y_train, y_train_proba), roc_auc_score(y_test, y_test_proba)\n        print(f'Train ROC_AUC: {roc_train:.3f}, Test ROC_AUC: {roc_test:.3f}')\n    print('Confusion Matrix', '\\n', pd.crosstab(y_test, y_test_pred))\n\ndef reduce_memory(df, verbose=0):\n    if verbose != 0:\n        start_mem = df.memory_usage().sum() / 1024 ** 2\n        print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    for col in df.columns:\n        col_type = df[col].dtype\n        if col_type != object and str(col_type)[:4] != 'uint' and str(col_type) != 'category':\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        elif str(col_type)[:4] != 'uint':\n            df[col] = df[col].astype('category')\n    if verbose != 0:\n        end_mem = df.memory_usage().sum() / 1024 ** 2\n        print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    return df\n\ndef roc_plot(y_true, probs):\n    fpr, tpr, threshold = roc_curve(y_true, probs)\n    roc_auc = auc(fpr, tpr)\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n\ndef vote(predictions: list, weights: list):\n    predictions = np.asarray(predictions).T\n    maj_vote = np.apply_along_axis(lambda x: np.argmax(np.bincount(x, weights=weights)), axis=1, arr=predictions)\n    return maj_vote\n\ndef cross_validation(clf, X, y, scoring='f1'):\n    scores = cross_val_score(estimator=clf, X=X, y=y, cv=10, scoring=scoring, n_jobs=-1)\n    print(f'Меры правильности перекрекстной оценки: {scores}')\n    print(f'Точность перекретсной оценки: {np.mean(scores):.3f} +/- {np.std(scores):.3f}')\n    return scores\n\ndef effect_size(factor_a, factor_b, cohen=True, desired_power=0.8, alpha=0.05):\n    n1, n2 = len(factor_a), len(factor_b)\n    s1, s2 = factor_a.std(ddof=1), factor_b.std(ddof=1)\n    df = (s1 ** 2 / n1 + s2 ** 2 / n2) ** 2 / \\\n          ((s1 ** 2 / n1) ** 2 / (n1 - 1) + (s2 ** 2 / n2) ** 2 / (n2 - 1))\n    if cohen:\n        sigma_pooled = np.sqrt(((n1 - 1) * s1**2 + (n2 - 1) * s2**2) / (n1 + n2 - 2))\n        return np.abs(factor_a.mean() - factor_b.mean()) / sigma_pooled, df\n    else:\n        return power.tt_ind_solve_power(effect_size=None, nobs1=len(factor_a), alpha=alpha, power=desired_power,\n                                        ratio=len(factor_b)/len(factor_a), alternative='two-sided'), df\n    \ndef statistic_output(*columns, df=df, cat=False, target=TARGET_NAME, alpha=0.05, sample_size=0):\n    data = df.copy()\n    data.drop_duplicates(inplace=True)\n    if sample_size == 0:\n        sample_size = int(0.05 * len(data))\n    if not cat:\n        columns = data.drop(target, axis=1).select_dtypes(exclude=['category', 'object']).columns\n        for column in columns:\n            df_sampled = data[[column, target]].sample(sample_size, random_state=seed)\n            factor_a = df_sampled.loc[df_sampled[target] == 0][column]   \n            factor_b = df_sampled.loc[df_sampled[target] == 1][column]\n            var_a, var_b = factor_a.var(), factor_b.var()   \n            _, pvalue = stats.shapiro(df_sampled[column])\n            if pvalue >= alpha:\n                _, pvalue = stats.ttest_ind(factor_a, factor_b, equal_var=False)\n                test = power.TTestIndPower()\n                eff_size, deg_free = effect_size(factor_a, factor_b, cohen=False)\n                pow = test.power(effect_size=eff_size, nobs1=len(factor_a), alpha=alpha, df=deg_free, \n                                 ratio=len(factor_b)/len(factor_a), alternative='two-sided')\n            else:\n                _, pvalue = stats.mannwhitneyu(factor_a, factor_b)\n                pow, eff_size = None, None\n            if pvalue < alpha:\n                result = f'with effect_size = {eff_size:.4f} and ttest power {pow*100:.2f}%' if pow is not None else ''\n                print(f'Factor \"{column}\" has statistical impact on target (var_a: {var_a:.2f}, var_b: {var_b:.2f}). {result}')\n            else:\n                print(f'Factor \"{column}\" does not affect target.')\n    else:\n        for column in columns:\n            print(column)\n            categories = data[column].unique().tolist()\n            for pair in combinations(categories, r=2):\n                a, b = pair\n                if a != b:\n                    try:\n                        data_ = data.loc[data[column].isin(pair), ['ID', column, target]].sample(sample_size, random_state=seed)\n                    except ValueError:\n                        continue\n                    table = data_.pivot_table(values='ID', index=column, columns=target, aggfunc='count')\n                    try:\n                        _, pvalue, _, _ = stats.chi2_contingency(table, correction=False)\n                    except ValueError:\n                        continue\n                    if pvalue >= alpha:\n                        print(f'Categories {a} and {b} can be united. P-value: {pvalue:.6f}')\n                    else:\n                        pass\n                        #print(f'Categories {a} and {b} have different frequencies with target.')\n                        \ndef categorical_stats(df=df, target=TARGET_NAME, alpha=0.05, sample_size=500):\n    data = df.copy().sample(sample_size)\n    columns_to_analize = data.select_dtypes(include=['category', 'object']).columns\n    weak_list = []\n    for factor in columns_to_analize:\n        if factor == target:\n            continue\n        print(f'{factor}')\n        table = pd.crosstab(data[factor], data[TARGET_NAME])\n        p_value = stats.chi2_contingency(table, correction=False)[1]\n        if p_value < alpha:\n            print(f'Feature {factor} has statistical impact on target. P-value: {p_value:.6f}')\n        else:\n            weak_list.append(factor)\n    if len(weak_list) > 0:\n        print(f'Statistically weak categorical features: ', *weak_list)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:46:19.038652Z","iopub.execute_input":"2022-04-03T18:46:19.038918Z","iopub.status.idle":"2022-04-03T18:46:19.104323Z","shell.execute_reply.started":"2022-04-03T18:46:19.038890Z","shell.execute_reply":"2022-04-03T18:46:19.103340Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df.info()\n# no missing values","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:46:25.395994Z","iopub.execute_input":"2022-04-03T18:46:25.396279Z","iopub.status.idle":"2022-04-03T18:46:25.420519Z","shell.execute_reply.started":"2022-04-03T18:46:25.396249Z","shell.execute_reply":"2022-04-03T18:46:25.419503Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df.describe().transpose()\n# max income = 666666 looks like outliers\n# MntMeatProducts, MntWines - check if there are outliers\n# Z_CostContact, Z_Revenue look like constant features","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:46:28.243811Z","iopub.execute_input":"2022-04-03T18:46:28.244116Z","iopub.status.idle":"2022-04-03T18:46:28.328109Z","shell.execute_reply.started":"2022-04-03T18:46:28.244074Z","shell.execute_reply":"2022-04-03T18:46:28.327105Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df.describe(include=['object'])\n# Dt_Customer - we will transform it\n# others: check if we can unite categories\n# cardinality of others is low, we may apply one-hot encoding ","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:46:31.295672Z","iopub.execute_input":"2022-04-03T18:46:31.295950Z","iopub.status.idle":"2022-04-03T18:46:31.316919Z","shell.execute_reply.started":"2022-04-03T18:46:31.295921Z","shell.execute_reply":"2022-04-03T18:46:31.315974Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df[TARGET_NAME].value_counts(normalize=True)\n# pretty high imbalance, we will apply class weights, sampling techniques","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:46:34.598271Z","iopub.execute_input":"2022-04-03T18:46:34.598708Z","iopub.status.idle":"2022-04-03T18:46:34.609147Z","shell.execute_reply.started":"2022-04-03T18:46:34.598663Z","shell.execute_reply":"2022-04-03T18:46:34.608347Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"Z_Cost = df.loc[df['Z_CostContact'] != 3.0, 'Z_CostContact'].sum()\nprint(f\"Z_CostContact is {'not' if Z_Cost > 0 else ''}constant feature\")\nZ_Rev = df.loc[df['Z_Revenue'] != 11.0, 'Z_Revenue'].sum()\nprint(f\"Z_Revenue is {'not' if Z_Rev > 0 else ''}constant feature\")\nif Z_Cost == 0:\n    df.drop('Z_CostContact', axis=1, inplace=True)\nif Z_Rev == 0:\n    df.drop('Z_Revenue', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:46:36.687428Z","iopub.execute_input":"2022-04-03T18:46:36.688154Z","iopub.status.idle":"2022-04-03T18:46:36.699834Z","shell.execute_reply.started":"2022-04-03T18:46:36.688101Z","shell.execute_reply":"2022-04-03T18:46:36.698875Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"#### Visualizations","metadata":{}},{"cell_type":"code","source":"numerical_cols = df.drop(['ID'], axis=1).select_dtypes(include=[np.int64]).columns.tolist()\nnumerical_cols = [column for column in numerical_cols if not column[:3] in ['Acc', 'Res', 'Tee', 'Kid', 'Com']]\n\nplt.figure(figsize=(22, 20))\nfor idx, column in enumerate(numerical_cols):\n    plt.subplot(4, 4, idx + 1)\n    dist = 'Normal Distribution' if stats.shapiro(df[column].sample(100))[1] > 0.05 else 'Not normal distribution'\n    plt.title(f'{column}: {dist}')\n    sns.histplot(data=df, x=column, hue=TARGET_NAME, bins=50, kde=True)\nplt.subplots_adjust(hspace=0.4, wspace=0.4)\nplt.show()\n# almost all num and mnt features are skewed\n# features look like they may have impact on target\n# maybe we have to deal with outliers - later","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:46:40.133291Z","iopub.execute_input":"2022-04-03T18:46:40.134533Z","iopub.status.idle":"2022-04-03T18:46:46.398727Z","shell.execute_reply.started":"2022-04-03T18:46:40.134487Z","shell.execute_reply":"2022-04-03T18:46:46.397593Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"categorical_cols = df.drop('Dt_Customer', axis=1).select_dtypes(include=['object']).columns\n\nplt.figure(figsize=(15, 6))\nfor idx, column in enumerate(categorical_cols, 1):\n    plt.subplot(1, 2, idx)\n    plt.title(f'{column}')\n    sns.countplot(x=column, hue=TARGET_NAME, data=df)\nplt.subplots_adjust(hspace=0.4, wspace=0.4)\nplt.show()\n# maybe we can unite some categories - let's make statistical tests later","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:46:46.400276Z","iopub.execute_input":"2022-04-03T18:46:46.400517Z","iopub.status.idle":"2022-04-03T18:46:46.844891Z","shell.execute_reply.started":"2022-04-03T18:46:46.400490Z","shell.execute_reply":"2022-04-03T18:46:46.843877Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(18,20))\nfor idx, column in enumerate(numerical_cols, 1):\n    plt.subplot(4, 4, idx)\n    sns.boxplot(y=df[column], x=df[TARGET_NAME], data=df)\n    plt.title(f'{column}')\nplt.subplots_adjust(hspace=0.5, wspace=0.5)\nplt.show()\n# some features may appear as not important (medians are equal in different classes)\n# we have some outliers in here: drop (if there are few) or drop only significant (IsolationForest to find out wich ones)\n# impute some significant outliers or let them stay. We will test on lgbm/logreg base model","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:46:46.846165Z","iopub.execute_input":"2022-04-03T18:46:46.846381Z","iopub.status.idle":"2022-04-03T18:46:48.602088Z","shell.execute_reply.started":"2022-04-03T18:46:46.846354Z","shell.execute_reply":"2022-04-03T18:46:48.601454Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# let's see if there are any meaningful correlations\nplt.figure(figsize = (14,12))\ncorr_matrix = df.corr()\ncorr_matrix = np.round(corr_matrix, 2)\ncorr_matrix[np.abs(corr_matrix) < 0.3] = 0\nsns.heatmap(corr_matrix, annot=True, linewidths=.5, cmap='coolwarm')\nplt.title('Correlation matrix')\nplt.show()\n# no extra high correlations, except meat products and number of catalog purchases\n# lot's of medium high correlations between income and amount of money spent on different goods categories\n# let the BestSet class decide if there is a need of dropping some features","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:46:52.837081Z","iopub.execute_input":"2022-04-03T18:46:52.837354Z","iopub.status.idle":"2022-04-03T18:46:55.678262Z","shell.execute_reply.started":"2022-04-03T18:46:52.837327Z","shell.execute_reply":"2022-04-03T18:46:55.677234Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"#### Some early obvious transforms","metadata":{}},{"cell_type":"code","source":"# make Age feature out of year of birth\ndf['Age'] = 2022 - df['Year_Birth']\ndf.drop('Year_Birth', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:47:00.337380Z","iopub.execute_input":"2022-04-03T18:47:00.337836Z","iopub.status.idle":"2022-04-03T18:47:00.345223Z","shell.execute_reply.started":"2022-04-03T18:47:00.337790Z","shell.execute_reply":"2022-04-03T18:47:00.344392Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# transform date into number of days\ndf[\"Dt_Customer\"] = pd.to_datetime('02-04-2022') - pd.to_datetime(df[\"Dt_Customer\"])\ndf[\"Dt_Customer\"] = df[\"Dt_Customer\"].dt.days","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:47:02.931633Z","iopub.execute_input":"2022-04-03T18:47:02.932003Z","iopub.status.idle":"2022-04-03T18:47:02.947094Z","shell.execute_reply.started":"2022-04-03T18:47:02.931966Z","shell.execute_reply":"2022-04-03T18:47:02.945810Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"df = df[df['Age'] < 90]\ndf = df[df['Income'] < 600000]\nprint(f'After removing outliers: {len(df)}')\n# about removing other possible outliers we will decide later","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:47:04.822583Z","iopub.execute_input":"2022-04-03T18:47:04.822900Z","iopub.status.idle":"2022-04-03T18:47:04.834634Z","shell.execute_reply.started":"2022-04-03T18:47:04.822868Z","shell.execute_reply":"2022-04-03T18:47:04.833495Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"df.duplicated().sum()\n# no duplicates","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:47:08.165416Z","iopub.execute_input":"2022-04-03T18:47:08.166009Z","iopub.status.idle":"2022-04-03T18:47:08.180463Z","shell.execute_reply.started":"2022-04-03T18:47:08.165976Z","shell.execute_reply":"2022-04-03T18:47:08.179837Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"#### Some statistic tests","metadata":{}},{"cell_type":"code","source":"statistic_output(df=df.drop('Complain', axis=1), sample_size=200)\n# as we can see lots of features are not important","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:47:11.749031Z","iopub.execute_input":"2022-04-03T18:47:11.749663Z","iopub.status.idle":"2022-04-03T18:47:11.866149Z","shell.execute_reply.started":"2022-04-03T18:47:11.749624Z","shell.execute_reply":"2022-04-03T18:47:11.865020Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"statistic_output(*categorical_cols, cat=True, sample_size=200)\n# features have small impact ","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:47:15.039225Z","iopub.execute_input":"2022-04-03T18:47:15.039557Z","iopub.status.idle":"2022-04-03T18:47:15.381377Z","shell.execute_reply.started":"2022-04-03T18:47:15.039521Z","shell.execute_reply":"2022-04-03T18:47:15.380492Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"df['Marital_Status'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:47:18.661681Z","iopub.execute_input":"2022-04-03T18:47:18.661991Z","iopub.status.idle":"2022-04-03T18:47:18.670809Z","shell.execute_reply.started":"2022-04-03T18:47:18.661961Z","shell.execute_reply":"2022-04-03T18:47:18.669910Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"categorical_stats(sample_size=220)\n# actually they are both weak\n# unite married and together, others as single\n# Family = together(or)single + children + teens","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:47:27.765671Z","iopub.execute_input":"2022-04-03T18:47:27.766170Z","iopub.status.idle":"2022-04-03T18:47:27.797487Z","shell.execute_reply.started":"2022-04-03T18:47:27.766106Z","shell.execute_reply":"2022-04-03T18:47:27.796555Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# unite categories\ndf['Marital_Status'] = df[\"Marital_Status\"].replace({\"Married\":\"Partner\", \"Together\":\"Partner\", \n                                                     \"Absurd\":\"Single\", \"Widow\":\"Single\", \"YOLO\":\"Single\", \n                                                     \"Divorced\":\"Single\", \"Single\":\"Single\", \"Alone\": \"Single\"})\ndf['Marital_Status'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:47:42.205899Z","iopub.execute_input":"2022-04-03T18:47:42.206742Z","iopub.status.idle":"2022-04-03T18:47:42.220016Z","shell.execute_reply.started":"2022-04-03T18:47:42.206697Z","shell.execute_reply":"2022-04-03T18:47:42.219406Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"df['Marital_Status'] = df['Marital_Status'].map({'Partner': 1, 'Single': 0})","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:47:49.840268Z","iopub.execute_input":"2022-04-03T18:47:49.841241Z","iopub.status.idle":"2022-04-03T18:47:49.848977Z","shell.execute_reply.started":"2022-04-03T18:47:49.841184Z","shell.execute_reply":"2022-04-03T18:47:49.847836Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# one-hot\ndf = pd.get_dummies(df, prefix=['Education'])","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:47:53.343399Z","iopub.execute_input":"2022-04-03T18:47:53.343727Z","iopub.status.idle":"2022-04-03T18:47:53.355364Z","shell.execute_reply.started":"2022-04-03T18:47:53.343695Z","shell.execute_reply":"2022-04-03T18:47:53.354398Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"### Best split","metadata":{}},{"cell_type":"code","source":"base_lr = Pipeline(steps=[('scaler', MinMaxScaler()),\n                          ('imputer', IterativeImputer(n_nearest_features=20, random_state=seed)),\n                          ('lr', LogisticRegression(class_weight='balanced', random_state=seed))])\nbase_lgbm = LGBMClassifier(verbose=-1, is_unbalance=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:47:56.912398Z","iopub.execute_input":"2022-04-03T18:47:56.913184Z","iopub.status.idle":"2022-04-03T18:47:56.918706Z","shell.execute_reply.started":"2022-04-03T18:47:56.913143Z","shell.execute_reply":"2022-04-03T18:47:56.917737Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"X, y = df.drop(TARGET_NAME, axis=1), df[TARGET_NAME]\n\ntrain_sizes, train_scores, test_scores = learning_curve(estimator=base_lr, X=X, y=y, \n                                                        train_sizes=np.linspace(0.1, 1.0, 10), cv=10, scoring='roc_auc', n_jobs=-1)\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nplt.figure(figsize=(10, 8))\nplt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='правильность при обучении')\nplt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\nplt.plot(train_sizes, test_mean, color='green', linestyle='--', marker='s', markersize=5, \n         label='правильность при проверке')\nplt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green')\nplt.axvline(X.shape[0]*0.7, color='orange', linestyle='-.', label='test_size=0.3')\nplt.grid()\nplt.xlabel('Количество обучающих образцов')\nplt.ylabel('Правильность')\nplt.legend(loc='best')\nplt.show()\n# setting test_size=0.3 looks fine, but in case of validation data we will set\n# test_size=0.25, valid_size=0.1","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:48:23.443917Z","iopub.execute_input":"2022-04-03T18:48:23.444232Z","iopub.status.idle":"2022-04-03T18:48:30.358882Z","shell.execute_reply.started":"2022-04-03T18:48:23.444191Z","shell.execute_reply":"2022-04-03T18:48:30.357908Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"X, y = df.drop(TARGET_NAME, axis=1), df[TARGET_NAME]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, shuffle=True, stratify=y, random_state=123)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, shuffle=True, stratify=y_train, random_state=123)\nprint(f'Train size: {X_train.shape[0]}, Validation size: {X_valid.shape[0]}, Test size: {X_test.shape[0]}')","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:48:51.107274Z","iopub.execute_input":"2022-04-03T18:48:51.107986Z","iopub.status.idle":"2022-04-03T18:48:51.126828Z","shell.execute_reply.started":"2022-04-03T18:48:51.107928Z","shell.execute_reply":"2022-04-03T18:48:51.125699Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"#### Outliers","metadata":{}},{"cell_type":"code","source":"detector = IsolationForest(n_estimators=100, contamination=0.005, n_jobs=-1, random_state=seed)\nout_cols = [column for column in X.columns if column.startswith('Num') or column.startswith('Mnt')]\noutliers = detector.fit_predict(X[out_cols])\nout_index = X.loc[outliers == -1, out_cols].index\nX.loc[outliers == -1, out_cols]","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:48:55.397789Z","iopub.execute_input":"2022-04-03T18:48:55.398317Z","iopub.status.idle":"2022-04-03T18:48:56.067040Z","shell.execute_reply.started":"2022-04-03T18:48:55.398270Z","shell.execute_reply":"2022-04-03T18:48:56.066016Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# raw predictions\nbase_lgbm.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False, early_stopping_rounds=5, eval_metric='roc_auc')\nbase_raw_train = base_lgbm.predict(X_train)\nbase_raw_test = base_lgbm.predict(X_test)\nbase_train_proba = base_lgbm.predict_proba(X_train)[:,1]\nbase_test_proba = base_lgbm.predict_proba(X_test)[:,1]\n\nreport(y_train, base_raw_train, y_test, base_raw_test, base_train_proba, base_test_proba)\n\nbase_lr.fit(X_train, y_train)\nbase_raw_train = base_lr.predict(X_train)\nbase_raw_test = base_lr.predict(X_test)\nbase_train_proba = base_lr.predict_proba(X_train)[:,1]\nbase_test_proba = base_lr.predict_proba(X_test)[:,1]\n\nreport(y_train, base_raw_train, y_test, base_raw_test, base_train_proba, base_test_proba)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:49:01.148077Z","iopub.execute_input":"2022-04-03T18:49:01.148400Z","iopub.status.idle":"2022-04-03T18:49:01.806589Z","shell.execute_reply.started":"2022-04-03T18:49:01.148366Z","shell.execute_reply":"2022-04-03T18:49:01.805727Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"#### Dropping possible outliers, IQR adjusting and imputing is removed as no significant improve was acheaved","metadata":{}},{"cell_type":"markdown","source":"### Feature engeneering, selecting and outlier classes","metadata":{}},{"cell_type":"code","source":"class FeatureCompose(BaseEstimator, TransformerMixin):\n    def __init__(self, weights=np.ones(6)):\n        self.weights = weights\n        self.weight_vec = pd.Series(data=self.weights, index=['MntWines', 'MntFruits', 'MntMeatProducts',\n                                                             'MntFishProducts', 'MntSweetProducts', 'MntGoldProds'],\n                                   name='weights_vector')\n        self.age_bin = None\n        self.age = None\n        self.median_income = None\n        self.pipe = None\n        self.cols = None\n        \n    def fit(self, X, y=None):\n        X_ = X.dropna().copy()\n        #X_['Age'] = 2022 - X_['Year_Birth']\n        self.age = X_.groupby(by='Age')['Income'].agg('median').to_dict()\n        self.median_income = X_['Income'].median()\n        self.pipe = Pipeline(steps=[('scaler', MinMaxScaler()),\n                                    ('cluster', KMeans(n_clusters=2, n_init=10, max_iter=300, random_state=seed, tol=1e-04))])\n        self.cols = [column for column in X_.columns if column.startswith('Num') or column.startswith('Mnt') or column in ('Income', 'Recency')]\n        self.pipe.fit(X_[self.cols])\n        del X_\n        return self\n    \n    def transform(self, X):\n        # here it is not supposed to deal with 'isoforest' outliers\n        X_ = X.copy()\n        # Family size\n        X_['Family_size'] = X_['Marital_Status']+1 + X_['Kidhome'] + X_['Teenhome']\n        # Number of children\n        X_['Num_child'] = X_['Kidhome'] + X_['Teenhome']\n        # total number of purchases\n        num_cols = [col for col in X_.columns if col.startswith('Num')]\n        X_['Total_Purch'] = X_[num_cols].sum(axis=1)\n        # total amount of spent money, weighted\n        mnt_cols = [col for col in X_.columns if col.startswith('Mnt')]\n        X_['Total_Mnt'] = (X_[mnt_cols] * self.weight_vec).sum(axis=1)\n        # cuts\n        X_['Med_age_income'] = X_['Age'].map(self.age)\n        X_['Med_age_income'].fillna(self.median_income, inplace=True)\n        # cluster distances\n        X_[['cluster_0', 'cluster_1']] = self.pipe.transform(X_[self.cols])\n        X_ = reduce_memory(X_)\n        \n        return X_\n    \nclass DataFrameR(BaseEstimator, TransformerMixin):\n    def __init__(self, columns, index=None):\n        self.columns = columns\n        self.index = index\n\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        assert len(self.columns) == X.shape[1], f'Number of columns {len(self.columns)} is not equal to data shape {X.shape[1]}'\n        if self.index is not None:\n            X_ = pd.DataFrame(X, columns=self.columns, index=self.index)\n        else:\n            X_ = pd.DataFrame(X, columns=self.columns)\n        return X_\n    \nclass RemoveOutliers(BaseEstimator, TransformerMixin):\n    def __init__(self, columns):\n        \"\"\"Drop possible outliers to further apply IterativeImputer\"\"\"\n        self.columns = columns\n        self.detector = IsolationForest(n_estimators=100, contamination=0.005, n_jobs=-1, random_state=seed)\n        \n    def fit(self, X, y=None):\n        out_cols = [column for column in X.columns if column.startswith('Num') or column.startswith('Mnt')]\n        self.detector.fit(X[self.columns])\n        return self\n    \n    def transform(self, X):\n        outliers = self.detector.predict(X[self.columns])\n        out_index = X[outliers == -1].index\n        X.loc[out_index, self.columns] = None\n        return X\n    \nclass ColumnSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, columns):\n        self.columns = columns\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        if not isinstance(X, pd.DataFrame):\n            return X\n        return X[self.columns]\n\nclass BestSet(BaseEstimator, TransformerMixin):\n    def __init__(self, estimator, k_features=12, scoring='f1', test_size=0.2):\n        self.scoring = scoring\n        self.k_features = k_features\n        self.test_size = test_size\n        self.estimator = clone(estimator)\n        self.fit_params = {}\n        if self.estimator.__class__.__name__ == 'LGBMClassifier':\n            self.fit_params.update({'verbose': False})\n\n    def fit(self, X, y):\n        if isinstance(X, pd.DataFrame):\n            X, y = X.values, y.values\n        X_train, X_test, y_train, y_test = train_test_split(X,\n                                                            y, \n                                                            test_size=self.test_size, \n                                                            stratify=y,\n                                                            random_state=seed)\n        dim = X_train.shape[1]\n        self.indices_ = tuple(range(dim))\n        self.subsets_ = [self.indices_]\n        score = self._calc_score(X_train, y_train, X_test, y_test, self.indices_)\n        self.scores_ = [score]\n\n        while dim > self.k_features:\n            scores, subsets = [], []\n            for p in combinations(self.indices_, r=dim-1):\n                score = self._calc_score(X_train, y_train, X_test, y_test, p)\n                scores.append(score)\n                subsets.append(p)\n            best = np.argmax(scores)\n            self.indices_ = subsets[best]\n            self.subsets_.append(self.indices_)\n            dim -= 1\n            self.scores_.append(scores[best])\n        self.k_score_ = self.scores_[-1]\n        return self\n    def transform(self, X):\n        if isinstance(X, pd.DataFrame):\n            X = X.values\n        best_indices = self.subsets_[np.argmax(self.scores_)]\n        return X[:, best_indices]\n\n    def _calc_score(self, X_train, y_train, X_test, y_test, indices):\n        self.estimator.fit(X_train[:, indices], y_train, **self.fit_params)\n        if self.scoring == 'auc':\n            y_pred = self.estimator.predict_proba(X_test[:, indices])[:,1]\n            score = roc_auc_score(y_test, y_pred)\n        else:\n            y_pred = self.estimator.predict(X_test[:, indices])\n            score = f1_score(y_test, y_pred)\n        return score","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:49:17.611054Z","iopub.execute_input":"2022-04-03T18:49:17.611385Z","iopub.status.idle":"2022-04-03T18:49:17.653065Z","shell.execute_reply.started":"2022-04-03T18:49:17.611350Z","shell.execute_reply":"2022-04-03T18:49:17.651920Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"end_columns = ['Marital_Status', 'Income', 'Kidhome', 'Teenhome', 'Dt_Customer',\n       'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts',\n       'MntFishProducts', 'MntSweetProducts', 'MntGoldProds',\n       'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases',\n       'NumStorePurchases', 'NumWebVisitsMonth', 'AcceptedCmp3',\n       'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2',\n       'Complain', 'Age', 'Education_2n Cycle', 'Education_Basic',\n       'Education_Graduation', 'Education_Master', 'Education_PhD',\n       'Family_size', 'Num_child', 'Total_Purch', 'Total_Mnt',\n       'Med_age_income', 'cluster_0', 'cluster_1']","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:49:23.202722Z","iopub.execute_input":"2022-04-03T18:49:23.203017Z","iopub.status.idle":"2022-04-03T18:49:23.209847Z","shell.execute_reply.started":"2022-04-03T18:49:23.202988Z","shell.execute_reply":"2022-04-03T18:49:23.208725Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"### Construct all from the beginning\nNext we make simple preprocessing steps, split data and define base pipeline","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(path, sep='\\t')\ndf.set_index('ID', inplace=True)\n\"\"\"Common simple transforms\"\"\"\n# drop constant\ndf.drop('Z_CostContact', axis=1, inplace=True)\ndf.drop('Z_Revenue', axis=1, inplace=True)\n# Age\ndf['Age'] = 2022 - df['Year_Birth']\ndf.drop('Year_Birth', axis=1, inplace=True)\n# days client on 02.04.2022\ndf[\"Dt_Customer\"] = pd.to_datetime('02-04-2022') - pd.to_datetime(df[\"Dt_Customer\"])\ndf[\"Dt_Customer\"] = df[\"Dt_Customer\"].dt.days\n# outliers\ndf = df[df['Age'] < 90]\ndf = df[df['Income'] < 600000]\n# status\ndf['Marital_Status'] = df[\"Marital_Status\"].replace({\"Married\":\"Partner\", \"Together\":\"Partner\", \n                                                     \"Absurd\":\"Single\", \"Widow\":\"Single\", \"YOLO\":\"Single\", \n                                                     \"Divorced\":\"Single\", \"Single\":\"Single\", \"Alone\": \"Single\"})\n# map status\ndf['Marital_Status'] = df['Marital_Status'].map({'Partner': 1, 'Single': 0})\n# Education one-hot\ndf = pd.get_dummies(df, prefix=['Education'])\n\n\"\"\"Split\"\"\"\nX, y = df.drop(TARGET_NAME, axis=1), df[TARGET_NAME]\n# columns to drop outliers in pipeline\nout_cols = [column for column in df.columns if column.startswith('Num') or column.startswith('Mnt')]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, shuffle=True, stratify=y, random_state=123)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, shuffle=True, stratify=y_train, random_state=123)\nprint(f'Train size: {X_train.shape[0]}, Validation size: {X_valid.shape[0]}, Test size: {X_test.shape[0]}')\n# preserve indexes?","metadata":{"execution":{"iopub.status.busy":"2022-04-03T19:01:06.667405Z","iopub.execute_input":"2022-04-03T19:01:06.668253Z","iopub.status.idle":"2022-04-03T19:01:06.725842Z","shell.execute_reply.started":"2022-04-03T19:01:06.668212Z","shell.execute_reply":"2022-04-03T19:01:06.724867Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"columns_to_scale = ['Income', 'Dt_Customer', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts',\n                    'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'Kidhome', 'Teenhome',\n                    'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases',\n                    'NumStorePurchases', 'NumWebVisitsMonth', 'Age', 'Med_age_income', 'Family_size', \n                    'Num_child', 'Total_Purch', 'Total_Mnt']\ncolumns_mnt_pca = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\ncolumns_num_pca = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth']\nnot_pca = [column for column in end_columns if column not in columns_mnt_pca and column not in columns_num_pca]\nnot_scale = [column for column in end_columns if column not in columns_to_scale]","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:49:30.882898Z","iopub.execute_input":"2022-04-03T18:49:30.883204Z","iopub.status.idle":"2022-04-03T18:49:30.890960Z","shell.execute_reply.started":"2022-04-03T18:49:30.883167Z","shell.execute_reply":"2022-04-03T18:49:30.889965Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"N = 0 # number of components to drop in PCA\nscaler = Pipeline(steps=[('selector1', ColumnSelector(columns_to_scale)), \n                         ('scaler', MinMaxScaler()), \n                         ('df', DataFrameR(columns_to_scale))])\nnot_scaler = Pipeline(steps=[('selector2', ColumnSelector(columns=not_scale))])\n\nunion = FeatureUnion(transformer_list=[('scale', scaler),\n                                       ('not_scale', not_scaler)])\n\nstarter_pipe = Pipeline(steps=[('preprocessor', FeatureCompose()),\n                               ('union_scaling', union),\n                               ('dframer', DataFrameR(columns_to_scale+not_scale)),                     \n                              ])\npca_mnt = Pipeline(steps=[('selector_mnt', ColumnSelector(columns_mnt_pca)),\n                          ('pca_mnt', PCA(n_components=len(columns_mnt_pca)-N, random_state=seed)),\n                          ('framer_mnt', DataFrameR([f\"mnt_{i+1}\" for i in range(len(columns_mnt_pca)-N)]))])\n\npca_num = Pipeline(steps=[('selector_num', ColumnSelector(columns_num_pca)),\n                          ('pca_mnt', PCA(n_components=len(columns_num_pca)-N, random_state=seed)),\n                          ('framer_mnt', DataFrameR([f\"num_{i+1}\" for i in range(len(columns_num_pca)-N)]))])\n\nunion_2 = FeatureUnion(transformer_list=[('select_no_pca', ColumnSelector(not_pca)), \n                                         ('pca_MNT', pca_mnt),\n                                         ('pca_NUM', pca_num)])\n\nestimator_boost = LGBMClassifier(verbose=-1, is_unbalance=True, max_depth=3)\nestimator = LogisticRegression(class_weight='balanced', random_state=seed, C=10.0)\n\npipe = Pipeline(steps=[('starter', starter_pipe),\n                       ('pca', KernelPCA(n_components=len(end_columns)-7, random_state=seed)),\n                       #('union_', union_2), # no improvement\n                       #('feature_selector', BestSet(estimator=estimator, k_features=25, scoring='auc')), # not helping\n                       ])\npipe_boost = Pipe(steps=[('preprocessor', FeatureCompose()),\n                         ('feature_selector', BestSet(estimator=estimator_boost, k_features=25, scoring='auc')),\n                         ('smote', SMOTETomek(sampling_strategy=.8, random_state=seed)),\n                         ('model', LGBMClassifier(verbose=-1, is_unbalance=True))\n                        ])","metadata":{"execution":{"iopub.status.busy":"2022-04-03T19:01:10.843395Z","iopub.execute_input":"2022-04-03T19:01:10.843713Z","iopub.status.idle":"2022-04-03T19:01:10.861377Z","shell.execute_reply.started":"2022-04-03T19:01:10.843681Z","shell.execute_reply":"2022-04-03T19:01:10.860673Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"selector = BorutaShap(model=LGBMClassifier(verbose=-1, is_unbalance=True), importance_measure='shap', classification=True)\nselector.fit(X_train, y_train, n_trials=50, sample=False, verbose=False)\n# lots of features are decided as unimportant","metadata":{"execution":{"iopub.status.busy":"2022-04-03T08:37:51.194637Z","iopub.execute_input":"2022-04-03T08:37:51.194972Z","iopub.status.idle":"2022-04-03T08:38:50.805832Z","shell.execute_reply.started":"2022-04-03T08:37:51.194937Z","shell.execute_reply":"2022-04-03T08:38:50.80525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"```\n9 attributes confirmed important: ['AcceptedCmp5', 'AcceptedCmp1', 'Recency', 'Marital_Status', 'NumStorePurchases', 'MntMeatProducts', 'Income', 'AcceptedCmp3', 'Dt_Customer']\n25 attributes confirmed unimportant: ['Education_Graduation', 'NumCatalogPurchases', 'Education_PhD', 'Med_age_income', 'MntWines', 'NumDealsPurchases', 'Age', 'MntFruits', 'Complain', 'NumWebVisitsMonth', 'Education_2n Cycle', 'Teenhome', 'Family_size', 'AcceptedCmp2', 'Num_child', 'Total_Mnt', 'AcceptedCmp4', 'cluster_0', 'NumWebPurchases', 'MntGoldProds', 'Kidhome', 'MntFishProducts', 'Education_Basic', 'Education_Master', 'MntSweetProducts']\n2 tentative attributes remains: ['cluster_1', 'Total_Purch']\n```","metadata":{}},{"cell_type":"code","source":"remain = ['AcceptedCmp5', 'AcceptedCmp1', 'Recency', 'Marital_Status', 'NumStorePurchases', 'MntMeatProducts', \n          'Income', 'AcceptedCmp3', 'Dt_Customer', 'cluster_1', 'Total_Purch']\n#X_train = X_train[remain]\n#X_valid = X_valid[remain]\n#X_test = X_test[remain]","metadata":{"execution":{"iopub.status.busy":"2022-04-03T08:43:57.678479Z","iopub.execute_input":"2022-04-03T08:43:57.679151Z","iopub.status.idle":"2022-04-03T08:43:57.689836Z","shell.execute_reply.started":"2022-04-03T08:43:57.679113Z","shell.execute_reply":"2022-04-03T08:43:57.688714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for logistic regression\nX_train = pipe.fit_transform(X_train, y_train)\nX_test = pipe.transform(X_test)\nsmote = SMOTETomek(sampling_strategy=.6, random_state=seed)\nX_train, y_train = smote.fit_resample(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T19:01:14.702898Z","iopub.execute_input":"2022-04-03T19:01:14.703222Z","iopub.status.idle":"2022-04-03T19:01:16.460558Z","shell.execute_reply.started":"2022-04-03T19:01:14.703186Z","shell.execute_reply":"2022-04-03T19:01:16.459732Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"lr = LogisticRegression(class_weight='balanced', random_state=seed)\nlr.fit(X_train, y_train)\npred_train = lr.predict(X_train)\npred_test = lr.predict(X_test)\npred_train_proba = lr.predict_proba(X_train)[:,1]\npred_test_proba = lr.predict_proba(X_test)[:,1]\n\nreport(y_train, pred_train, y_test, pred_test, pred_train_proba, pred_test_proba)\n# with kernel-pca instead of Best set selection we got another results, the bias is even more higher, but FN & FP are better\n# a lot of linear correlations that are present in data and noise were eliminated by kernel-pca transformation","metadata":{"execution":{"iopub.status.busy":"2022-04-03T19:01:16.462227Z","iopub.execute_input":"2022-04-03T19:01:16.462529Z","iopub.status.idle":"2022-04-03T19:01:16.563577Z","shell.execute_reply.started":"2022-04-03T19:01:16.462495Z","shell.execute_reply":"2022-04-03T19:01:16.562539Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"markdown","source":"### LightGBM model","metadata":{}},{"cell_type":"code","source":"X_train, y_train = pipe_boost[:3].fit_resample(X_train, y_train)\nX_valid = pipe_boost[:2].transform(X_valid)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T10:50:17.117178Z","iopub.execute_input":"2022-04-03T10:50:17.117542Z","iopub.status.idle":"2022-04-03T10:50:33.433869Z","shell.execute_reply.started":"2022-04-03T10:50:17.117503Z","shell.execute_reply":"2022-04-03T10:50:33.432821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_params = {\n                'objective': 'binary', # cross_entropy\n                'n_estimators': 800,\n                'n_jobs': -1,\n                'is_unbalance': True,\n                'random_state': 123\n}\nfit_params = {'early_stopping_rounds': 10,  \n              'eval_set': [(X_valid, y_valid)],  \n              'eval_metric': 'auc',\n              'verbose': False\n}","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:50:01.591096Z","iopub.execute_input":"2022-04-03T18:50:01.591893Z","iopub.status.idle":"2022-04-03T18:50:01.597824Z","shell.execute_reply.started":"2022-04-03T18:50:01.591841Z","shell.execute_reply":"2022-04-03T18:50:01.596958Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n    param_trials = {\n                    'max_depth': trial.suggest_int('max_depth', 3, 8),\n                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5),\n                    'reg_alpha': trial.suggest_float('reg_alpha', 1., 5.),\n                    'reg_lambda': trial.suggest_float('reg_lambda', 1., 5.),\n                    'num_leaves': trial.suggest_int('num_leaves', 20, 265),\n                    'subsample': trial.suggest_float('subsample', 0.3, 1.),\n                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 1.),\n                    'max_bin': trial.suggest_int('max_bin', 30, 260),\n                    'min_child_samples': trial.suggest_int('min_child_samples', 80, 260),\n                    'min_child_weight': trial.suggest_float('min_child_weight', 0.001, 0.1),\n                    'boosting_type': trial.suggest_categorical('boosting_type', ['goss', 'gbdt']),\n                    }\n    param_trials.update(model_params)\n    opt_model = LGBMClassifier(**param_trials)\n    opt_model.fit(X_train, y_train, **fit_params)\n    \n    y_pred = opt_model.predict_proba(X_valid)[:,1]\n    score = roc_auc_score(y_valid, y_pred)\n    \n    return score","metadata":{"execution":{"iopub.status.busy":"2022-04-03T10:50:46.207613Z","iopub.execute_input":"2022-04-03T10:50:46.207944Z","iopub.status.idle":"2022-04-03T10:50:46.221909Z","shell.execute_reply.started":"2022-04-03T10:50:46.207911Z","shell.execute_reply":"2022-04-03T10:50:46.221134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.logging.set_verbosity(optuna.logging.FATAL)\nstudy = optuna.create_study(sampler=TPESampler(seed=seed), direction=\"maximize\")\nstudy.optimize(objective, n_trials=350, timeout=6000)\n\nprint(f'Number of completed trials: {len(study.trials)}')\nprint('Best trial')\ntrial = study.best_trial\nprint(f'Best score: {trial.value}')\nprint('Best params')\nfor key, value in trial.params.items():\n    print(f'{key}: {value}')","metadata":{"execution":{"iopub.status.busy":"2022-04-03T10:51:18.36557Z","iopub.execute_input":"2022-04-03T10:51:18.366403Z","iopub.status.idle":"2022-04-03T10:51:51.160832Z","shell.execute_reply.started":"2022-04-03T10:51:18.36636Z","shell.execute_reply":"2022-04-03T10:51:51.159738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_params = {\n                'objective': 'binary', # cross_entropy\n                'n_estimators': 800,\n                'n_jobs': -1,\n                'is_unbalance': True,\n                'random_state': 123,\n                'max_depth': 7,\n                'learning_rate': 0.4327804963936754,\n                'reg_alpha': 4.677471904510624,\n                'reg_lambda': 4.046531932784275,\n                'num_leaves': 177,\n                'subsample': 0.8872682894784513,\n                'colsample_bytree': 0.8791931580090088,\n                'max_bin': 102,\n                'min_child_samples': 236,\n                'min_child_weight': 0.0663351899300188,\n                'boosting_type': 'gbdt',\n}","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:50:05.710912Z","iopub.execute_input":"2022-04-03T18:50:05.711593Z","iopub.status.idle":"2022-04-03T18:50:05.718132Z","shell.execute_reply.started":"2022-04-03T18:50:05.711551Z","shell.execute_reply":"2022-04-03T18:50:05.717384Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# get result probabilities on 10 folds\ntest_results, train_roc, test_pred, train_f1, train_acc = [], [], [], [], []\nskf = StratifiedKFold(n_splits=10)\nX, y = df.drop(TARGET_NAME, axis=1), df[TARGET_NAME]\nX_fold, X_test, y_fold, y_test = train_test_split(X,y, test_size=0.2, shuffle=True, stratify=y, random_state=seed)\n\nfor train_index, valid_index in skf.split(X_fold, y_fold):\n    X_train, X_valid = X_fold.iloc[train_index, :], X_fold.iloc[valid_index, :]\n    y_train, y_valid = y_fold.iloc[train_index], y_fold.iloc[valid_index]\n\n    pipe_boost = Pipe(steps=[('preprocessor', FeatureCompose()),\n                             ('feature_selector', BestSet(estimator=estimator_boost, k_features=28, scoring='auc')),\n                             #('Scaler_for_pca', StandardScaler()),\n                             #('kernel', KernelPCA(n_components=len(end_columns)-5, kernel='rbf', random_state=seed)),\n                             ('smote', SMOTETomek(sampling_strategy=.8, random_state=seed)),\n                             ('model', LGBMClassifier(verbose=-1, **model_params))\n                            ])\n    X_train, y_train = pipe_boost[:3].fit_resample(X_train, y_train)\n    X_valid = pipe_boost[:2].transform(X_valid)\n    X_test_ = pipe_boost[:2].transform(X_test)\n    \n    fit_params = {'early_stopping_rounds': 4,  \n                  'eval_set': [(X_valid, y_valid)],  \n                  'eval_metric': 'auc',\n                  'verbose': False\n              }\n\n    pipe_boost[-1].fit(X_train, y_train, **fit_params)\n    test_labels = pipe_boost[-1].predict(X_test_)\n    test_pred.append(test_labels)\n    \n    train_proba = pipe_boost[-1].predict_proba(X_train)[:,1]\n    train_roc.append(roc_auc_score(y_train, train_proba))\n    \n    train_labels = pipe_boost[-1].predict(X_train)\n    train_f1.append(f1_score(y_train, train_labels))\n    train_acc.append(accuracy_score(y_train, train_labels))\n    \n    pred_test = pipe_boost[-1].predict_proba(X_test_)[:,1]\n    test_results.append(pred_test)\n\nfinal_test = np.array(test_results).mean(axis=0)\ntest_pred = vote(test_pred, weights=np.ones(10))","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:50:08.831488Z","iopub.execute_input":"2022-04-03T18:50:08.832230Z","iopub.status.idle":"2022-04-03T18:52:17.141983Z","shell.execute_reply.started":"2022-04-03T18:50:08.832191Z","shell.execute_reply":"2022-04-03T18:52:17.141203Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"print(f'train mean accuracy: {np.array(train_acc).mean():.4f}, test accuracy: {accuracy_score(y_test, test_pred):.4f}')\nprint(f'train mean f1: {np.array(train_f1).mean():.4f}, test f1: {f1_score(y_test, test_pred):.4f}')\nprint(f'train mean auc: {np.array(train_roc).mean():.4f}, test auc: {roc_auc_score(y_test, final_test):.4f}')\n# model overfitted - high variance","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:52:17.143624Z","iopub.execute_input":"2022-04-03T18:52:17.144055Z","iopub.status.idle":"2022-04-03T18:52:17.159559Z","shell.execute_reply.started":"2022-04-03T18:52:17.144022Z","shell.execute_reply":"2022-04-03T18:52:17.158519Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"print(confusion_matrix(y_test, test_pred))","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:52:17.161488Z","iopub.execute_input":"2022-04-03T18:52:17.161857Z","iopub.status.idle":"2022-04-03T18:52:17.170835Z","shell.execute_reply.started":"2022-04-03T18:52:17.161812Z","shell.execute_reply":"2022-04-03T18:52:17.169649Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"show_proba_calibration_plots(final_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:52:17.173742Z","iopub.execute_input":"2022-04-03T18:52:17.174095Z","iopub.status.idle":"2022-04-03T18:52:17.985200Z","shell.execute_reply.started":"2022-04-03T18:52:17.174048Z","shell.execute_reply":"2022-04-03T18:52:17.984012Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"roc_plot(y_test, final_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:52:17.987027Z","iopub.execute_input":"2022-04-03T18:52:17.987386Z","iopub.status.idle":"2022-04-03T18:52:18.190174Z","shell.execute_reply.started":"2022-04-03T18:52:17.987342Z","shell.execute_reply":"2022-04-03T18:52:18.189209Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\n\nvar = VarianceThreshold(threshold=1000)\nvar_selected = var.fit_transform(X_)\nvar.get_feature_names_out()\nprint('High variance features:')\nprint('-'*30)\nfor x, vr in zip(X_.columns, var.get_support()):\n    if vr:\n        print(f'{x}')","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:52:48.878850Z","iopub.execute_input":"2022-04-03T18:52:48.879626Z","iopub.status.idle":"2022-04-03T18:52:48.894834Z","shell.execute_reply.started":"2022-04-03T18:52:48.879574Z","shell.execute_reply":"2022-04-03T18:52:48.894021Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"import lightgbm\n\nfig, ax = plt.subplots(1, 1, figsize=(8,8))\nlightgbm.plot_importance(pipe_boost[-1], ax=ax)\nax.set_yticklabels(X_.columns[[4,3,0,22,13,7,2,23,17,15,14,12,10,25,11,1,18,16,5]][::-1])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:52:52.669101Z","iopub.execute_input":"2022-04-03T18:52:52.670900Z","iopub.status.idle":"2022-04-03T18:52:53.040173Z","shell.execute_reply.started":"2022-04-03T18:52:52.670773Z","shell.execute_reply":"2022-04-03T18:52:53.039150Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"#### with pca decomposition\n```\ntrain mean accuracy: 0.8884, test accuracy: 0.8533\ntrain mean f1: 0.8773, test f1: 0.5806\ntrain mean auc: 0.9565, test auc: 0.8842\n```","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test, test_pred, digits=3))","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:52:18.447795Z","iopub.status.idle":"2022-04-03T18:52:18.448396Z","shell.execute_reply.started":"2022-04-03T18:52:18.448220Z","shell.execute_reply":"2022-04-03T18:52:18.448240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observe shap values","metadata":{}},{"cell_type":"code","source":"X_ = pipe_boost[0].fit_transform(X_fold)\nX_ = X_.iloc[:, list(pipe_boost[1].subsets_[np.argmax(pipe_boost[1].scores_)])]","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:52:27.141487Z","iopub.execute_input":"2022-04-03T18:52:27.141994Z","iopub.status.idle":"2022-04-03T18:52:28.462737Z","shell.execute_reply.started":"2022-04-03T18:52:27.141962Z","shell.execute_reply":"2022-04-03T18:52:28.461929Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"explainer = shap.TreeExplainer(pipe_boost[-1])\n\ndata = pd.Series(data=X_.iloc[np.random.randint(len(X_)), :].values, index=X_.columns.tolist())\n# Calculate Shap values\nshap_values = explainer.shap_values(data.values.reshape(1,-1))","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:52:31.369828Z","iopub.execute_input":"2022-04-03T18:52:31.370145Z","iopub.status.idle":"2022-04-03T18:52:31.385460Z","shell.execute_reply.started":"2022-04-03T18:52:31.370112Z","shell.execute_reply":"2022-04-03T18:52:31.384415Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"shap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], data)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:52:33.981746Z","iopub.execute_input":"2022-04-03T18:52:33.982045Z","iopub.status.idle":"2022-04-03T18:52:34.003431Z","shell.execute_reply.started":"2022-04-03T18:52:33.982015Z","shell.execute_reply":"2022-04-03T18:52:34.002784Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"shap.initjs()\n\nshap_values = explainer.shap_values(X_)\nshap.summary_plot(shap_values[1], X_)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:52:39.767215Z","iopub.execute_input":"2022-04-03T18:52:39.768358Z","iopub.status.idle":"2022-04-03T18:52:41.034840Z","shell.execute_reply.started":"2022-04-03T18:52:39.768289Z","shell.execute_reply":"2022-04-03T18:52:41.033519Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"### Some thoughts: Logistic Regeression with KernelPCA showed better results than stratified LGBM model. One reason maybe is that the number of samples is too low","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}