{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nfrom torchvision import transforms as T\nfrom torch.utils.data import Dataset, DataLoader\n\nimport cv2\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.autonotebook import tqdm\nfrom glob import glob\n\nfrom sklearn.model_selection import train_test_split\n\n!pip install -qq editdistance torchsummary\nimport editdistance\nfrom torchsummary import summary\n\nseed = 42","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-03T08:13:14.747179Z","iopub.execute_input":"2022-07-03T08:13:14.750358Z","iopub.status.idle":"2022-07-03T08:13:28.594236Z","shell.execute_reply.started":"2022-07-03T08:13:14.750247Z","shell.execute_reply":"2022-07-03T08:13:28.593114Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"#### As another and more classic approach: use CNN-LSTM model with OCR Loss ","metadata":{}},{"cell_type":"code","source":"torch.random.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\nnp.random.seed(seed)\n\nPATH = r'../input/handwritten/synthetic-data/'\nLR = 0.001\nBATCH_SIZE = 32\nHIDDEN = 512\nENC_LAYERS = 2\nDEC_LAYERS = 2\nN_HEADS = 4\nDROPOUT = 0.1\nIMG_WIDTH = 256\nIMG_HEIGHT = 64\nEPOCHS = 100\n\nVOCAB = ['PAD', 'SOS', ' ',] + [char for char in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'] + ['EOS']\n\nchar2idx = {char: idx for idx, char in enumerate(VOCAB)}\nidx2char = {idx: char for idx, char in enumerate(VOCAB)}\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Currenly using \"{device.upper()}\" device.')","metadata":{"execution":{"iopub.status.busy":"2022-07-03T08:13:28.597922Z","iopub.execute_input":"2022-07-03T08:13:28.598544Z","iopub.status.idle":"2022-07-03T08:13:28.692851Z","shell.execute_reply.started":"2022-07-03T08:13:28.598511Z","shell.execute_reply":"2022-07-03T08:13:28.692068Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def text_to_labels(text, char2idx=char2idx):\n    return [char2idx['SOS']] + [char2idx[i.upper()] for i in text if i.upper() in char2idx.keys()] + [char2idx['EOS']]\n\ndef labels_to_text(text, idx2char=idx2char):\n    S = \"\".join([idx2char[i] for i in text])\n    if S.find('EOS') == -1:\n        return S\n    else:\n        return S[:S.find('EOS')]\n\ndef char_error_rate(p_seq1, p_seq2):\n    p_vocab = set(p_seq1 + p_seq2)\n    p2c = dict(zip(p_vocab, range(len(p_vocab))))\n    c_seq1 = [chr(p2c[p]) for p in p_seq1]\n    c_seq2 = [chr(p2c[p]) for p in p_seq2]\n    return editdistance.eval(''.join(c_seq1),\n                             ''.join(c_seq2)) / max(len(c_seq1), len(c_seq2))","metadata":{"execution":{"iopub.status.busy":"2022-07-03T08:13:28.694326Z","iopub.execute_input":"2022-07-03T08:13:28.696937Z","iopub.status.idle":"2022-07-03T08:13:28.726881Z","shell.execute_reply.started":"2022-07-03T08:13:28.696890Z","shell.execute_reply":"2022-07-03T08:13:28.724443Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"transforms = T.Compose([\n                        T.ToPILImage(),\n                        T.Resize((IMG_HEIGHT, IMG_WIDTH)),\n                        T.ColorJitter(contrast=(0.5,1),saturation=(0.5,1)),\n                        T.RandomRotation(degrees=(-9, 9), fill=255),\n                        T.RandomAffine(10 ,None ,[0.6 ,1] ,3 ,fillcolor=255),\n                        T.ToTensor()\n                        ])\nvalid_transforms = T.Compose([\n                              T.ToPILImage(),\n                              T.Resize((IMG_HEIGHT, IMG_WIDTH)),\n                              T.ToTensor()\n                              ])","metadata":{"execution":{"iopub.status.busy":"2022-07-03T08:13:28.729007Z","iopub.execute_input":"2022-07-03T08:13:28.745766Z","iopub.status.idle":"2022-07-03T08:13:28.775955Z","shell.execute_reply.started":"2022-07-03T08:13:28.745727Z","shell.execute_reply":"2022-07-03T08:13:28.775224Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"images_paths = glob(PATH+'*.png')\nimages_paths = sorted([str(path) for path in images_paths])\nimages_labels = [path.split('/')[-1].split('_')[0] for path in images_paths]\ndf = pd.DataFrame(data={'path': images_paths, 'label': images_labels})\ndf.sample(3)","metadata":{"execution":{"iopub.status.busy":"2022-07-03T08:13:28.782177Z","iopub.execute_input":"2022-07-03T08:13:28.793128Z","iopub.status.idle":"2022-07-03T08:13:29.490259Z","shell.execute_reply.started":"2022-07-03T08:13:28.793069Z","shell.execute_reply":"2022-07-03T08:13:29.489563Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df_train, df_valid = train_test_split(df, test_size=0.25, shuffle=True, random_state=seed)\nprint(f'Train size: {df_train.shape[0]}, validation size: {df_valid.shape[0]}')","metadata":{"execution":{"iopub.status.busy":"2022-07-03T08:13:29.494264Z","iopub.execute_input":"2022-07-03T08:13:29.496235Z","iopub.status.idle":"2022-07-03T08:13:29.510638Z","shell.execute_reply.started":"2022-07-03T08:13:29.496196Z","shell.execute_reply":"2022-07-03T08:13:29.509904Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class CharDataset(Dataset):\n    def __init__(self, dataframe, transforms=transforms):\n        self.dataframe = dataframe\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.dataframe)\n    \n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx].squeeze()\n        image = cv2.imread(row['path'])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = transforms(image)\n        label = text_to_labels(row['label'])\n        return torch.FloatTensor(image), torch.LongTensor(label) \n    \n    def collate_fn(self, batch):\n        x_padded = []\n        max_y_len = max([i[1].size(0) for i in batch])\n        y_padded = torch.LongTensor(max_y_len, len(batch))\n        y_padded.zero_()\n\n        for i in range(len(batch)):\n            x_padded.append(batch[i][0].unsqueeze(0))\n            y = batch[i][1]\n            y_padded[:y.size(0), i] = y\n\n        x_padded = torch.cat(x_padded)\n        return x_padded.to(device), y_padded.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-07-03T08:13:31.231871Z","iopub.execute_input":"2022-07-03T08:13:31.232132Z","iopub.status.idle":"2022-07-03T08:13:31.244723Z","shell.execute_reply.started":"2022-07-03T08:13:31.232100Z","shell.execute_reply":"2022-07-03T08:13:31.243625Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class TransformerModel(nn.Module):\n    def __init__(self, bb_name, outtoken, hidden, enc_layers=1, dec_layers=1, nhead=1, dropout=0.1, pretrained=False):\n        super(TransformerModel, self).__init__()\n        self.backbone = torchvision.models.__getattribute__(bb_name)(pretrained=pretrained)\n        self.backbone.fc = nn.Conv2d(2048, int(hidden/2), 1)\n\n        self.pos_encoder = PositionalEncoding(hidden, dropout)\n        self.decoder = nn.Embedding(outtoken, hidden)\n        self.pos_decoder = PositionalEncoding(hidden, dropout)\n        self.transformer = nn.Transformer(d_model=hidden, nhead=nhead, num_encoder_layers=enc_layers,\n                                          num_decoder_layers=dec_layers, dim_feedforward=hidden * 4, dropout=dropout,\n                                          activation='relu')\n\n        self.fc_out = nn.Linear(hidden, outtoken)\n        self.src_mask = None\n        self.trg_mask = None\n        self.memory_mask = None\n\n    def generate_square_subsequent_mask(self, sz):\n        mask = torch.triu(torch.ones(sz, sz), 1)\n        mask = mask.masked_fill(mask == 1, float('-inf'))\n        return mask\n\n    def make_len_mask(self, inp):\n        return (inp == 0).transpose(0, 1)\n\n    def forward(self, src, trg):\n        if self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n            self.trg_mask = self.generate_square_subsequent_mask(len(trg)).to(device) \n        x = self.backbone.conv1(src)\n\n        x = self.backbone.bn1(x)\n        x = self.backbone.relu(x)\n        x = self.backbone.maxpool(x)\n        x = self.backbone.layer1(x)\n        x = self.backbone.layer2(x)\n        x = self.backbone.layer3(x)\n        x = self.backbone.layer4(x) # [64, 2048, 2, 8] : [B,C,H,W]\n            \n        x = self.backbone.fc(x) # [64, 256, 2, 8] : [B,C,H,W]\n        x = x.permute(0, 3, 1, 2) # [64, 8, 256, 2] : [B,W,C,H]\n        x = x.flatten(2) # [64, 8, 512] : [B,W,CH]\n        x = x.permute(1, 0, 2) # [8, 64, 512] : [W,B,CH]\n        src_pad_mask = self.make_len_mask(x[:, :, 0])\n        src = self.pos_encoder(x) # [8, 64, 512]\n        trg_pad_mask = self.make_len_mask(trg)\n        trg = self.decoder(trg)\n        trg = self.pos_decoder(trg)\n\n        output = self.transformer(src, trg, src_mask=self.src_mask, tgt_mask=self.trg_mask,\n                                  memory_mask=self.memory_mask,\n                                  src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=trg_pad_mask,\n                                  memory_key_padding_mask=src_pad_mask) # [13, 64, 512] : [L,B,CH]\n        output = self.fc_out(output) # [13, 64, 92] : [L,B,H]\n\n        return output\n    \nclass PositionalEncoding(nn.Module):  # when having sentences\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        self.scale = nn.Parameter(torch.ones(1))\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.scale * self.pe[:x.size(0), :]\n        return self.dropout(x) ","metadata":{"execution":{"iopub.status.busy":"2022-07-03T08:13:33.654816Z","iopub.execute_input":"2022-07-03T08:13:33.656616Z","iopub.status.idle":"2022-07-03T08:13:33.677070Z","shell.execute_reply.started":"2022-07-03T08:13:33.656577Z","shell.execute_reply":"2022-07-03T08:13:33.676421Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def train_one_batch(model, data, optimizer, criterion):\n    model.train()\n    image, target = data\n    optimizer.zero_grad()\n    output = model(image, target[:-1, :])\n    loss = criterion(output.view(-1, output.shape[-1]), torch.reshape(target[1:, :], (-1,)))\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\n@torch.no_grad()\ndef validate_one_batch(model, data, criterion):\n    model.eval()\n    image, target = data\n    output = model(image, target[:-1, :])\n    loss = criterion(output.view(-1, output.shape[-1]), torch.reshape(target[1:, :], (-1,)))\n    return loss.item()\n\ndef evaluate(model, dataloader, max_len=30):  # assuming dataloader has batch_size=1\n    model.eval()\n    wer_overall = 0\n    cer_overall = 0\n    with torch.no_grad():\n        for src, trg in tqdm(dataloader, leave=False):\n            out_indexes = [char2idx['SOS'], ]\n\n            for i in range(max_len):\n                trg_tensor = torch.LongTensor(out_indexes).unsqueeze(1).to(device)\n                output = model(src, trg_tensor)\n                out_token = output.argmax(2)[-1].item()\n                out_indexes.append(out_token)\n                if out_token == char2idx['EOS']:\n                    break\n                    \n            out_char = labels_to_text(out_indexes[1:])\n            real_char = labels_to_text(trg[1:, 0].detach().cpu().numpy()).lower()\n            wer_overall += int(real_char != out_char)\n            if out_char:\n                cer = char_error_rate(real_char, out_char)\n            else:\n                cer = 1\n            \n            cer_overall += cer\n    \n    return cer_overall / len(dataloader) * 100, wer_overall / len(dataloader) * 100\n\n@torch.no_grad()\ndef prediction(model, filepath='random', max_len=30):\n    label = None\n    if filepath == 'random':\n        idx = np.random.randint(len(df_valid))\n        filepath = df_valid.iloc[idx, 0]\n        label = df_valid.iloc[idx, 1]\n\n    model.eval()\n    img = cv2.imread(filepath)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    image = valid_transforms(img)\n    src = torch.FloatTensor(image).unsqueeze(0).to(device)\n\n    out_indexes = [char2idx['SOS'], ]\n\n    for i in range(max_len):\n                \n        trg_tensor = torch.LongTensor(out_indexes).unsqueeze(1).to(device)\n                \n        output = model(src, trg_tensor)\n        out_token = output.argmax(2)[-1].item()\n        out_indexes.append(out_token)\n        if out_token == char2idx['EOS']:\n            break\n    preds = labels_to_text(out_indexes[1:], idx2char)\n    plt.figure(figsize=(6,4))\n    plt.title(f'Prediction: {preds}, Truth: {label if label is not None else \"NO label\"}')\n    plt.imshow(img)\n    plt.tight_layout()\n    plt.show()\n    plt.pause(0.001)\n    \n    return preds","metadata":{"execution":{"iopub.status.busy":"2022-07-03T08:14:12.308108Z","iopub.execute_input":"2022-07-03T08:14:12.308392Z","iopub.status.idle":"2022-07-03T08:14:12.326862Z","shell.execute_reply.started":"2022-07-03T08:14:12.308349Z","shell.execute_reply":"2022-07-03T08:14:12.326133Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_dataset = CharDataset(df_train, transforms)\nvalid_dataset = CharDataset(df_valid, valid_transforms)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=train_dataset.collate_fn, drop_last=True)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=1, shuffle=False, collate_fn=valid_dataset.collate_fn, drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-03T08:14:14.110935Z","iopub.execute_input":"2022-07-03T08:14:14.111191Z","iopub.status.idle":"2022-07-03T08:14:14.116464Z","shell.execute_reply.started":"2022-07-03T08:14:14.111161Z","shell.execute_reply":"2022-07-03T08:14:14.115764Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model = TransformerModel('resnet50', len(VOCAB), hidden=HIDDEN, enc_layers=ENC_LAYERS, dec_layers=DEC_LAYERS,   \n                         nhead=N_HEADS, dropout=DROPOUT).to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-6)\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, mode='min', patience=10, min_lr=1e-6,)","metadata":{"execution":{"iopub.status.busy":"2022-07-03T08:14:16.537261Z","iopub.execute_input":"2022-07-03T08:14:16.537982Z","iopub.status.idle":"2022-07-03T08:14:20.107446Z","shell.execute_reply.started":"2022-07-03T08:14:16.537927Z","shell.execute_reply":"2022-07-03T08:14:20.106700Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_losses, valid_losses, valid_cers, valid_wers = [], [], [], []\n\nfor epoch in range(EPOCHS):\n    print(f'{epoch+1}/{EPOCHS} epoch.')\n    epoch_train_losses, epoch_valid_losses = [], []\n    for _, batch in enumerate(tqdm(train_dataloader, leave=False)):\n        loss = train_one_batch(model, batch, optimizer, criterion)\n        epoch_train_losses.append(loss)\n        \n    train_epoch_loss = np.array(epoch_train_losses).mean()\n    train_losses.append(train_epoch_loss)\n    \n    for _, batch in enumerate(tqdm(valid_dataloader, leave=False)):\n        loss = validate_one_batch(model, batch, criterion)\n        epoch_valid_losses.append(loss)\n        \n    valid_epoch_loss = np.array(epoch_valid_losses).mean()\n    valid_losses.append(valid_epoch_loss)\n    print(f'Train loss: {train_epoch_loss:.4f}, validation loss: {valid_epoch_loss:.4f}')\n    \n    if (epoch + 1) % 10 == 0:\n        valid_cer, valid_wer = evaluate(model, valid_dataloader)\n        valid_cers.append(valid_cer)\n        valid_wers.append(valid_wer)\n        print(f'Char_error_rate: {valid_cer:.4f}, Word_error_rate: {valid_wer:.4f}')\n    \n    scheduler.step(valid_epoch_loss)\n    \n    if (epoch+1) % 10 == 0:\n        pred = prediction(model)\n        torch.save(model.state_dict(), 'model.pth')","metadata":{"execution":{"iopub.status.busy":"2022-07-03T08:14:23.672291Z","iopub.execute_input":"2022-07-03T08:14:23.672656Z","iopub.status.idle":"2022-07-03T09:08:46.422979Z","shell.execute_reply.started":"2022-07-03T08:14:23.672609Z","shell.execute_reply":"2022-07-03T09:08:46.421289Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"prediction(model)","metadata":{"execution":{"iopub.status.busy":"2022-07-03T09:08:46.423906Z","iopub.status.idle":"2022-07-03T09:08:46.424590Z","shell.execute_reply.started":"2022-07-03T09:08:46.424327Z","shell.execute_reply":"2022-07-03T09:08:46.424353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LSTM version. \n#### Transformer model seems too complex for such task.","metadata":{}},{"cell_type":"code","source":"fname2label = lambda name: str(name).split('_')[0].split('/')[-1]\nimages = glob(PATH+'*.png')","metadata":{"execution":{"iopub.status.busy":"2022-05-04T22:22:47.769555Z","iopub.execute_input":"2022-05-04T22:22:47.770374Z","iopub.status.idle":"2022-05-04T22:22:47.857961Z","shell.execute_reply.started":"2022-05-04T22:22:47.770327Z","shell.execute_reply":"2022-05-04T22:22:47.857257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = 'QWERTYUIOPASDFGHJKLZXCVBNMqwertyuiopasdfghjklzxcvbnm'\nBATCH, TIMESTEP, VOCAB = 64, 32, len(vocab)\nH, W = 32, 128","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:55:39.658903Z","iopub.execute_input":"2022-05-04T21:55:39.659559Z","iopub.status.idle":"2022-05-04T21:55:39.666389Z","shell.execute_reply.started":"2022-05-04T21:55:39.65952Z","shell.execute_reply":"2022-05-04T21:55:39.665682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class OCRDataset(Dataset):\n\n    def __init__(self, items, vocab=vocab, preprocess_shape=(H,W), timesteps=TIMESTEP):\n        super().__init__()\n        self.items = items\n        self.charList = {ix+1:ch for ix,ch in enumerate(vocab)}\n        self.charList.update({0: '`'})\n        self.invCharList = {v:k for k,v in self.charList.items()}\n        self.ts = timesteps\n\n    def __len__(self):\n        return len(self.items)\n\n    def sample(self):\n        return self[np.random.randint(len(self))]\n\n    def __getitem__(self, ix):\n        item = self.items[ix]\n        image = cv2.imread(str(item), 0)\n        label = fname2label(item)\n        return image, label\n\n    def collate_fn(self, batch):\n        images, labels, label_lengths, label_vectors, input_lengths = [], [], [], [], []\n        for image, label in batch:\n            images.append(torch.Tensor(self.preprocess(image))[None,None])\n            label_lengths.append(len(label))\n            labels.append(label)\n            label_vectors.append(self.str2vec(label))\n            input_lengths.append(self.ts)\n        images = torch.cat(images).float().to(device)\n        label_lengths = torch.Tensor(label_lengths).long().to(device)\n        label_vectors = torch.Tensor(label_vectors).long().to(device)\n        input_lengths = torch.Tensor(input_lengths).long().to(device)\n        return images, label_vectors, label_lengths, input_lengths, labels\n\n    def str2vec(self, string, pad=True):\n        string = ''.join([s for s in string if s in self.invCharList])\n        val = list(map(lambda x: self.invCharList[x], string)) \n        if pad:\n            while len(val) < self.ts:\n                val.append(0)\n        return val\n    \n    def preprocess(self, img, shape=(32,128)):\n        target = np.ones(shape)*255\n        try:\n            H, W = shape\n            h, w = img.shape\n            fx = H/h\n            fy = W/w\n            f = min(fx, fy)\n            _h = int(h*f)\n            _w = int(w*f)\n            _img = cv2.resize(img, (_w,_h))\n            target[:_h,:_w] = _img\n        except:\n            pass\n        return (255-target)/255 # add augmentations?\n\n    def decoder_chars(self, pred):\n        decoded = \"\"\n        last = \"\"\n        pred = pred.cpu().detach().numpy()\n        for i in range(len(pred)):\n            k = np.argmax(pred[i])\n            if k > 0 and self.charList[k] != last:\n                last = self.charList[k]\n                decoded = decoded + last\n            elif k > 0 and self.charList[k] == last:\n                continue\n            else:\n                last = \"\"\n        return decoded.replace(\" \",\" \")\n\n    def wer(self, preds, labels):\n        c = 0\n        for p, l in zip(preds, labels):\n            c += p.lower().strip() != l.lower().strip()\n        return round(c/len(preds), 4)\n    \n    def cer(self, preds, labels):\n        c, d = [], []\n        for p, l in zip(preds, labels):\n            c.append(editdistance.eval(p, l) / len(l))\n        return round(np.mean(c), 4)\n\n    def evaluate(self, model, ims, labels, lower=False):\n        model.eval()\n        preds = model(ims).permute(1,0,2) \n        preds = [self.decoder_chars(pred) for pred in preds]\n        return {'char-error-rate': self.cer(preds, labels),\n                'word-error-rate': self.wer(preds, labels),\n                'char-accuracy' : 1 - self.cer(preds, labels),\n                'word-accuracy' : 1 - self.wer(preds, labels)}","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:57:08.937759Z","iopub.execute_input":"2022-05-04T21:57:08.938312Z","iopub.status.idle":"2022-05-04T21:57:08.966948Z","shell.execute_reply.started":"2022-05-04T21:57:08.938276Z","shell.execute_reply":"2022-05-04T21:57:08.966237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_items, valid_items = train_test_split(glob(PATH+'*.png'), test_size=0.2, random_state=1)\ntrain_dataset = OCRDataset(train_items)\nvalid_dataset = OCRDataset(valid_items)\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH, collate_fn=train_dataset.collate_fn, drop_last=True, shuffle=True)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=BATCH, collate_fn=valid_dataset.collate_fn, drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T22:23:25.024887Z","iopub.execute_input":"2022-05-04T22:23:25.027434Z","iopub.status.idle":"2022-05-04T22:23:25.182842Z","shell.execute_reply.started":"2022-05-04T22:23:25.027384Z","shell.execute_reply":"2022-05-04T22:23:25.18211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BasicBlock(nn.Module):\n    def __init__(self, ni, no, ks=3, st=1, padding=1, pool=2, drop=0.2):\n        super().__init__()\n        self.ks = ks\n        self.block = nn.Sequential(\n            nn.Conv2d(ni, no, kernel_size=ks, stride=st, padding=padding),\n            nn.BatchNorm2d(no, momentum=0.3),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(pool),\n            nn.Dropout2d(drop)\n        )\n    def forward(self, x):\n        return self.block(x)\n    \nclass Permute(nn.Module):\n    def forward(self, input):\n        return input.permute(2,0,1)\n\nclass Reshape(nn.Module):\n    def __init__(self, shape=(256, 32)):\n        super().__init__()\n        self.shape = shape\n    def forward(self, input):\n        return input.view(-1, *self.shape)\n\nclass Ocr(nn.Module):\n    def __init__(self, vocab):\n        super().__init__()\n        self.model = nn.Sequential(\n            BasicBlock( 1, 128),\n            BasicBlock(128, 128),\n            BasicBlock(128, 256, pool=(4,2)),\n            Reshape(),\n            Permute()\n        )\n        self.rnn = nn.Sequential(\n            nn.LSTM(256, 256, num_layers=2, dropout=0.2, bidirectional=True),\n        )\n        self.classification = nn.Sequential(\n            nn.Linear(512, vocab+1),\n            nn.LogSoftmax(-1),\n        )\n    def forward(self, x):\n        x = self.model(x)\n        x, lstm_states = self.rnn(x)\n        y = self.classification(x)\n        return y","metadata":{"execution":{"iopub.status.busy":"2022-05-04T21:57:14.853293Z","iopub.execute_input":"2022-05-04T21:57:14.853786Z","iopub.status.idle":"2022-05-04T21:57:14.866675Z","shell.execute_reply.started":"2022-05-04T21:57:14.853718Z","shell.execute_reply":"2022-05-04T21:57:14.865941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ctc(log_probs, target, input_lengths, target_lengths, blank=0):\n    loss = nn.CTCLoss(blank=blank, zero_infinity=True)\n    ctc_loss = loss(log_probs, target, input_lengths, target_lengths)\n    return ctc_loss","metadata":{"execution":{"iopub.status.busy":"2022-05-04T22:20:07.162588Z","iopub.execute_input":"2022-05-04T22:20:07.163474Z","iopub.status.idle":"2022-05-04T22:20:07.169424Z","shell.execute_reply.started":"2022-05-04T22:20:07.163428Z","shell.execute_reply":"2022-05-04T22:20:07.168401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Ocr(len(vocab)).to(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T22:23:17.498564Z","iopub.execute_input":"2022-05-04T22:23:17.498845Z","iopub.status.idle":"2022-05-04T22:23:17.534366Z","shell.execute_reply.started":"2022-05-04T22:23:17.498815Z","shell.execute_reply":"2022-05-04T22:23:17.533699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_batch(data, model, optimizer, criterion):\n    model.train()\n    imgs, targets, label_lens, input_lens, labels = data\n    optimizer.zero_grad()\n    preds = model(imgs)\n    loss = criterion(preds, targets, input_lens, label_lens)\n    loss.backward()\n    optimizer.step()\n    results = train_dataset.evaluate(model, imgs.to(device), labels)\n    return loss.item(), results\n\n@torch.no_grad()\ndef validate_batch(data, model, criterion):\n    model.eval()\n    imgs, targets, label_lens, input_lens, labels = data\n    preds = model(imgs)\n    loss = criterion(preds, targets, input_lens, label_lens)\n    return loss.item(), valid_dataset.evaluate(model, imgs.to(device), labels)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T22:02:10.941582Z","iopub.execute_input":"2022-05-04T22:02:10.941885Z","iopub.status.idle":"2022-05-04T22:02:10.949097Z","shell.execute_reply.started":"2022-05-04T22:02:10.941855Z","shell.execute_reply":"2022-05-04T22:02:10.948327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = ctc\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=3e-6)\nscheduler = scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, mode='min', patience=5, min_lr=1e-6,)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T22:23:20.317365Z","iopub.execute_input":"2022-05-04T22:23:20.319833Z","iopub.status.idle":"2022-05-04T22:23:20.330907Z","shell.execute_reply.started":"2022-05-04T22:23:20.319787Z","shell.execute_reply":"2022-05-04T22:23:20.330137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_losses, valid_losses = [], []\n\nfor epoch in range(EPOCHS):\n    print(f'{epoch+1}/{EPOCHS} epoch.')\n    epoch_train_losses, epoch_valid_losses = [], []\n    for _, batch in enumerate(tqdm(train_dataloader, leave=False)):\n        loss, train_results = train_batch(batch, model, optimizer, criterion)\n        epoch_train_losses.append(loss)\n        \n    train_epoch_loss = np.array(epoch_train_losses).mean()\n    train_losses.append(train_epoch_loss)\n    \n    for _, batch in enumerate(tqdm(valid_dataloader, leave=False)):\n        loss, valid_results = validate_batch(batch, model, criterion)\n        epoch_valid_losses.append(loss)\n        \n    valid_epoch_loss = np.array(epoch_valid_losses).mean()\n    valid_losses.append(valid_epoch_loss)\n    print(f'Train loss: {train_epoch_loss:.4f}, validation loss: {valid_epoch_loss:.4f}')\n    \n    train_cer, train_wer = train_results['char-accuracy'], train_results['word-accuracy']\n    valid_cer, valid_wer = valid_results['char-accuracy'], valid_results['word-accuracy']\n    print(f'Train char accuracy: {train_cer:.4f}, train word accuracy: {train_wer:.4f}')\n    print(f'Validation char accuracy: {valid_cer:.4f}, validation word accuracy: {valid_wer:.4f}')\n    print()\n    \n    scheduler.step(valid_epoch_loss)\n    \n    if (epoch+1) % 10 == 0:\n        torch.save(model.state_dict(), 'model.pth')","metadata":{"execution":{"iopub.status.busy":"2022-05-04T22:23:29.320434Z","iopub.execute_input":"2022-05-04T22:23:29.320705Z","iopub.status.idle":"2022-05-04T22:39:19.60756Z","shell.execute_reply.started":"2022-05-04T22:23:29.320674Z","shell.execute_reply":"2022-05-04T22:39:19.606668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\nimages, label_vectors, label_lengths, input_lengths, labels = next(iter(valid_dataloader))\npreds = model(images).permute(1,0,2) \npreds = [valid_dataset.decoder_chars(pred) for pred in preds]\nimages = 255. - images.detach().cpu().numpy().squeeze() * 255 \nplt.figure(figsize=(15,15))\nfor i in range(BATCH):\n    plt.subplot(16, 4, i+1)\n    plt.title(f'True: {labels[i]}, Predicted: {preds[i]}')\n    plt.imshow(images[i], cmap='gray')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T22:39:43.488504Z","iopub.execute_input":"2022-05-04T22:39:43.488827Z","iopub.status.idle":"2022-05-04T22:39:49.289269Z","shell.execute_reply.started":"2022-05-04T22:39:43.488756Z","shell.execute_reply":"2022-05-04T22:39:49.28864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TrOCR pretrained","metadata":{}},{"cell_type":"code","source":"class CharDataset(Dataset):\n    def __init__(self, df, processor, max_target_length=32):\n        self.df = df\n        self.processor = processor\n        self.max_target_length = max_target_length\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        record = self.df.iloc[idx].squeeze()\n        image = cv2.imread(record['path'])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n\n        labels = self.processor.tokenizer(record['label'], \n                                          padding=\"max_length\", \n                                          max_length=self.max_target_length).input_ids\n\n        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n\n        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n        return encoding","metadata":{"execution":{"iopub.status.busy":"2022-07-03T09:09:01.832286Z","iopub.execute_input":"2022-07-03T09:09:01.832653Z","iopub.status.idle":"2022-07-03T09:09:01.844100Z","shell.execute_reply.started":"2022-07-03T09:09:01.832612Z","shell.execute_reply":"2022-07-03T09:09:01.843457Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from transformers import TrOCRProcessor\n\nprocessor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n\ntrain_dataset = CharDataset(df=df_train, processor=processor)\neval_dataset = CharDataset(df=df_valid, processor=processor)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\neval_dataloader = DataLoader(eval_dataset, batch_size=4)","metadata":{"execution":{"iopub.status.busy":"2022-07-03T09:09:04.582162Z","iopub.execute_input":"2022-07-03T09:09:04.582464Z","iopub.status.idle":"2022-07-03T09:09:17.434630Z","shell.execute_reply.started":"2022-07-03T09:09:04.582432Z","shell.execute_reply":"2022-07-03T09:09:17.433706Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from transformers import VisionEncoderDecoderModel\n\nmodel = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-stage1\").to(device)","metadata":{"execution":{"iopub.status.busy":"2022-07-03T09:09:17.438092Z","iopub.execute_input":"2022-07-03T09:09:17.438298Z","iopub.status.idle":"2022-07-03T09:10:05.200225Z","shell.execute_reply.started":"2022-07-03T09:09:17.438274Z","shell.execute_reply":"2022-07-03T09:10:05.199340Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# set special tokens used for creating the decoder_input_ids from the labels\nmodel.config.decoder_start_token_id = processor.tokenizer.cls_token_id\nmodel.config.pad_token_id = processor.tokenizer.pad_token_id\n# make sure vocab size is set correctly\nmodel.config.vocab_size = model.config.decoder.vocab_size\n\n# set beam search parameters\nmodel.config.eos_token_id = processor.tokenizer.sep_token_id\nmodel.config.max_length = 32\nmodel.config.early_stopping = True\nmodel.config.no_repeat_ngram_size = 3\nmodel.config.length_penalty = 2.0\nmodel.config.num_beams = 4","metadata":{"execution":{"iopub.status.busy":"2022-07-03T09:10:05.201651Z","iopub.execute_input":"2022-07-03T09:10:05.201963Z","iopub.status.idle":"2022-07-03T09:10:05.209004Z","shell.execute_reply.started":"2022-07-03T09:10:05.201903Z","shell.execute_reply":"2022-07-03T09:10:05.208216Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def compute_cer(pred_ids, label_ids):\n    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n    \n    cer = 0.0\n    for pred, label in zip(pred_str, label_str):\n        cer += char_error_rate(pred, label)\n    cer /= len(pred_str)\n\n    return cer","metadata":{"execution":{"iopub.status.busy":"2022-07-03T09:10:05.211206Z","iopub.execute_input":"2022-07-03T09:10:05.211497Z","iopub.status.idle":"2022-07-03T09:10:05.223058Z","shell.execute_reply.started":"2022-07-03T09:10:05.211461Z","shell.execute_reply":"2022-07-03T09:10:05.222215Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW\n\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\nfor epoch in range(4):  # loop over the dataset multiple times\n    # train\n    model.train()\n    train_loss = 0.0\n    for batch in tqdm(train_dataloader, leave=False):\n      # get the inputs\n        for k,v in batch.items():\n            batch[k] = v.to(device)\n\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        train_loss += loss.item()\n\n    print(f\"Loss after epoch {epoch+1}:\", train_loss/len(train_dataloader))\n    \n    # evaluate\n    model.eval()\n    valid_cer = 0.0\n    with torch.no_grad():\n        for batch in tqdm(eval_dataloader, leave=False):\n            # run batch generation\n            outputs = model.generate(batch[\"pixel_values\"].to(device))\n            # compute metrics\n            cer = compute_cer(pred_ids=outputs, label_ids=batch[\"labels\"])  # very slow beacuse of for loop with strings\n            valid_cer += cer \n\n    print(\"Validation CER:\", valid_cer / len(eval_dataloader))","metadata":{"execution":{"iopub.status.busy":"2022-07-03T11:40:55.824743Z","iopub.execute_input":"2022-07-03T11:40:55.825352Z","iopub.status.idle":"2022-07-03T14:10:32.409505Z","shell.execute_reply.started":"2022-07-03T11:40:55.825313Z","shell.execute_reply":"2022-07-03T14:10:32.408784Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"model.eval()\nbatch = next(iter(eval_dataloader))\n\nwith torch.no_grad():\n    preds = model.generate(batch[\"pixel_values\"].to(device))\n    \nlabels = batch[\"labels\"]    \npred_str = processor.batch_decode(preds, skip_special_tokens=True)\nlabels[labels == -100] = processor.tokenizer.pad_token_id\nlabel_str = processor.batch_decode(labels, skip_special_tokens=True)    \n\nplt.figure(figsize=(7,7))\nfor i in range(4):\n    plt.subplot(2, 2, i+1)\n    plt.title(f'True: {label_str[i]}, Predicted: {pred_str[i]}')\n    plt.imshow(batch['pixel_values'][i].cpu().detach().numpy().transpose(1,2,0))\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-03T14:12:48.541302Z","iopub.execute_input":"2022-07-03T14:12:48.541881Z","iopub.status.idle":"2022-07-03T14:12:50.013479Z","shell.execute_reply.started":"2022-07-03T14:12:48.541831Z","shell.execute_reply":"2022-07-03T14:12:50.012746Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}