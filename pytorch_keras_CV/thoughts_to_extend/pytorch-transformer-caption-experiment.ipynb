{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport transformers\nimport torchvision\nimport torch.nn as nn\nfrom torchvision import transforms as T\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pack_padded_sequence\n\nimport cv2\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.autonotebook import tqdm\n\nfrom sklearn.model_selection import train_test_split\n\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n\n!pip install -qq editdistance torchsummary\nimport editdistance\nfrom torchsummary import summary\n\nseed = 42","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-06T21:24:48.813575Z","iopub.execute_input":"2022-05-06T21:24:48.813917Z","iopub.status.idle":"2022-05-06T21:25:08.700132Z","shell.execute_reply.started":"2022-05-06T21:24:48.813816Z","shell.execute_reply":"2022-05-06T21:25:08.699120Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"torch.random.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\nnp.random.seed(seed)\nfrom IPython.display import clear_output\n\ntokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n\nMAX_SEQ_LEN = 120\nLR = 1e-4\nEPOCHS = 50\nIMAGE_SIZE = 256\nBATCH_SIZE = 32\nHIDDEN = 512\nENC_LAYERS = 2\nDEC_LAYERS = 2\nN_HEADS = 4\nDROPOUT = 0.1\nPATH = r'caption_model.pth'\nVOCAB_SIZE = tokenizer.vocab_size\nclear_output()\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Currently using \"{device.upper()}\" device')","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:25:08.702627Z","iopub.execute_input":"2022-05-06T21:25:08.702896Z","iopub.status.idle":"2022-05-06T21:25:18.122522Z","shell.execute_reply.started":"2022-05-06T21:25:08.702856Z","shell.execute_reply":"2022-05-06T21:25:18.121754Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"path = '../input/flickr30k/'\ndata = np.zeros((158916, 3), dtype=np.object)\ni = 0\nfor line in open(path + 'captions.txt', 'r'):\n    data[i, :] = line.replace('\\n', \"\").split('|')\n    i += 1\n    \ndf = pd.DataFrame(data=data[1:, :], columns=data[0, :])\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:25:18.123872Z","iopub.execute_input":"2022-05-06T21:25:18.124636Z","iopub.status.idle":"2022-05-06T21:25:18.542933Z","shell.execute_reply.started":"2022-05-06T21:25:18.124596Z","shell.execute_reply":"2022-05-06T21:25:18.542237Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_transforms = T.Compose([\n                              T.ToPILImage(),\n                              T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n                              T.RandomCrop(IMAGE_SIZE),\n                              T.ColorJitter(brightness=(0.95, 1.05),\n                                            contrast=(0.95, 1.05),\n                                            saturation=(0.98, 1.02),\n                                            hue=0.05),\n                              T.RandomHorizontalFlip(p=0.1),\n                              T.GaussianBlur(kernel_size=(1, 3), sigma=(0.1, 0.5)),\n                              T.RandomAdjustSharpness(sharpness_factor=1.2, p=0.2),\n                              T.RandomRotation(degrees=(-5, 5)),\n                              T.ToTensor(),\n                              T.Normalize(mean=[0.485, 0.456, 0.406], \n                                          std=[0.229, 0.224, 0.225])\n])\n\nvalid_transforms = T.Compose([\n                              T.ToPILImage(),\n                              T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n                              T.ToTensor(),\n                              T.Normalize(mean=[0.485, 0.456, 0.406], \n                                          std=[0.229, 0.224, 0.225])\n])\ninvTrans = T.Compose([T.Normalize(mean = [ 0., 0., 0. ],\n                                      std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n                          T.Normalize(mean = [ -0.485, -0.456, -0.406 ],\n                                      std = [ 1., 1., 1. ]),\n                          T.ToPILImage(),\n                         ])","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:25:18.544932Z","iopub.execute_input":"2022-05-06T21:25:18.546245Z","iopub.status.idle":"2022-05-06T21:25:18.556500Z","shell.execute_reply.started":"2022-05-06T21:25:18.546202Z","shell.execute_reply":"2022-05-06T21:25:18.555779Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"root = '../input/flickr30k/images'\n\nclass CapDataset(Dataset):\n    def __init__(self, df, root, tokenizer, transforms=None):\n        self.df = df\n        self.root = root\n        self.tokenizer = tokenizer\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, ix):\n        row = self.df.iloc[ix].squeeze()\n        id = row.image_name\n        image_path = f'{self.root}/{id}'\n\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        caption = row.caption_text\n\n        target = tokenizer(caption, \n                           return_token_type_ids=False, \n                           return_attention_mask=False, \n                           max_length=MAX_SEQ_LEN, \n                           padding=\"do_not_pad\",\n                           return_tensors=\"pt\")\n        target = target['input_ids'].squeeze()\n        target = torch.LongTensor(target)\n        return image, target, caption\n\n    def collate_fn(self, batch):\n        batch.sort(key=lambda x: len(x[1]), reverse=True)\n        images, targets, captions = zip(*batch)\n        images = torch.stack([self.transforms(image) for image in images], 0)\n        lengths = [len(tar) for tar in targets]\n        _targets = torch.zeros(len(captions), max(lengths)).long()\n        for i, tar in enumerate(targets):\n            end = lengths[i]\n            _targets[i, :end] = tar[:end] \n        _targets = _targets.permute(1,0)\n        return images.to(device), _targets.to(device), torch.tensor(lengths).long().to(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:25:18.557936Z","iopub.execute_input":"2022-05-06T21:25:18.558229Z","iopub.status.idle":"2022-05-06T21:25:18.571827Z","shell.execute_reply.started":"2022-05-06T21:25:18.558190Z","shell.execute_reply":"2022-05-06T21:25:18.570942Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train, test = train_test_split(df, test_size=0.01, shuffle=True, random_state=seed)\ntrain, valid = train_test_split(train, test_size=0.1, shuffle=True, random_state=seed)\ntrain.reset_index(drop=True, inplace=True)\nvalid.reset_index(drop=True, inplace=True)\ntest.reset_index(drop=True, inplace=True)\nprint(f'Train size: {train.shape[0]}, valid size: {valid.shape[0]}, test size: {test.shape[0]}')","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:25:18.573435Z","iopub.execute_input":"2022-05-06T21:25:18.573716Z","iopub.status.idle":"2022-05-06T21:25:18.636633Z","shell.execute_reply.started":"2022-05-06T21:25:18.573678Z","shell.execute_reply":"2022-05-06T21:25:18.635773Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_dataset = CapDataset(train, root, tokenizer, train_transforms)\nvalid_dataset = CapDataset(valid, root, tokenizer, valid_transforms)\ntest_dataset = CapDataset(test, root, tokenizer, valid_transforms)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=train_dataset.collate_fn, drop_last=True)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=valid_dataset.collate_fn, drop_last=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=test_dataset.collate_fn)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:25:18.638045Z","iopub.execute_input":"2022-05-06T21:25:18.638370Z","iopub.status.idle":"2022-05-06T21:25:18.646533Z","shell.execute_reply.started":"2022-05-06T21:25:18.638330Z","shell.execute_reply":"2022-05-06T21:25:18.645740Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, encoded_image_size=8, pretrained=True):\n        super(Encoder, self).__init__()\n        self.enc_image_size = encoded_image_size\n\n        resnet = torchvision.models.resnet101(pretrained=pretrained)\n        modules = list(resnet.children())[:-2]\n        self.resnet = nn.Sequential(*modules)\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n\n        for param in self.resnet.parameters():\n            param.requires_grad = False\n\n        for child in list(self.resnet.children())[5:]:\n            for param in child.parameters():\n                param.requires_grad = True\n                \n        self.fc = nn.Sequential(nn.Conv2d(2048, 512, 1),\n                                nn.ReLU())\n\n    def forward(self, images):\n        out = self.resnet(images)\n        out = self.fc(out)\n        out = self.adaptive_pool(out) # [B, C, H, W]\n        #out = out.permute(0, 3, 1, 2)\n        out = out.flatten(2) # [B, C, HW]\n        out = out.permute(2,0,1) # [HW, B, C] or [W, B, CH]?\n        return out\n    \nclass Encoder_50(nn.Module):\n    def __init__(self, bb_name='resnet50', hidden=HIDDEN, pretrained=False):\n        super(Encoder_50, self).__init__()\n        self.backbone = torchvision.models.__getattribute__(bb_name)(pretrained=pretrained)\n        self.backbone.fc = nn.Conv2d(2048, hidden, 1)\n    \n    def forward(self, src):\n        x = self.backbone.conv1(src)\n\n        x = self.backbone.bn1(x)\n        x = self.backbone.relu(x)\n        x = self.backbone.maxpool(x)\n        x = self.backbone.layer1(x)\n        x = self.backbone.layer2(x)\n        x = self.backbone.layer3(x)\n        x = self.backbone.layer4(x) # [32, 2048, 8, 8] : [B,C,H,W]\n            \n        x = self.backbone.fc(x) # [32, 512, 8, 8] : [B,C,H,W]\n        # x = x.permute(0, 3, 1, 2) # [64, 8, 512, 8] : [B,W,C,H]\n        x = x.flatten(2) # [32, 512, 64] : [B,C,WH]\n        #x = x.permute(1, 0, 2) # [8, 32, 512] : [W,B,CH]\n        x = x.permute(2,0,1)\n        return x\n    \nclass TransformerModel(nn.Module):\n    def __init__(self, outtoken, hidden, enc_layers=1, dec_layers=1, nhead=1, dropout=0.1):\n        super(TransformerModel, self).__init__()\n        self.backbone = Encoder_50()\n\n        self.pos_encoder = PositionalEncoding(hidden, dropout)\n        self.decoder = nn.Embedding(outtoken, hidden, padding_idx=0)\n        self.pos_decoder = PositionalEncoding(hidden, dropout)\n        self.transformer = nn.Transformer(d_model=hidden, nhead=nhead, num_encoder_layers=enc_layers,\n                                          num_decoder_layers=dec_layers, dim_feedforward=hidden * 4, dropout=dropout,\n                                          activation='relu')\n\n        self.fc_out = nn.Linear(hidden, outtoken)\n        self.src_mask = None\n        self.trg_mask = None\n        self.memory_mask = None\n\n    def generate_square_subsequent_mask(self, sz):\n        mask = torch.triu(torch.ones(sz, sz), 1)\n        mask = mask.masked_fill(mask == 1, float('-inf'))\n        return mask\n\n    def make_len_mask(self, inp):\n        return (inp == 0).transpose(0, 1)\n\n    def forward(self, src, trg):\n        if self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n            self.trg_mask = self.generate_square_subsequent_mask(len(trg)).to(device) \n        x = self.backbone(src)\n        \n        src_pad_mask = self.make_len_mask(x[:, :, 0])\n        src = self.pos_encoder(x)\n        trg_pad_mask = self.make_len_mask(trg)\n        trg = self.decoder(trg)\n        trg = self.pos_decoder(trg)\n\n        output = self.transformer(src, trg, src_mask=self.src_mask, tgt_mask=self.trg_mask,\n                                  memory_mask=self.memory_mask,\n                                  src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=trg_pad_mask,\n                                  memory_key_padding_mask=src_pad_mask) #  [HW,B,C]\n        output = self.fc_out(output) # [L,B,H]\n\n        return output\n    \nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        self.scale = nn.Parameter(torch.ones(1))\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.scale * self.pe[:x.size(0), :]\n        return self.dropout(x) ","metadata":{"execution":{"iopub.status.busy":"2022-05-06T22:17:11.245861Z","iopub.execute_input":"2022-05-06T22:17:11.246116Z","iopub.status.idle":"2022-05-06T22:17:11.273531Z","shell.execute_reply.started":"2022-05-06T22:17:11.246086Z","shell.execute_reply":"2022-05-06T22:17:11.272840Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"#### Check","metadata":{}},{"cell_type":"code","source":"ds = CapDataset(df, root, tokenizer, train_transforms)\ndl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=ds.collate_fn, drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T22:16:18.208780Z","iopub.execute_input":"2022-05-06T22:16:18.209049Z","iopub.status.idle":"2022-05-06T22:16:18.213628Z","shell.execute_reply.started":"2022-05-06T22:16:18.209020Z","shell.execute_reply":"2022-05-06T22:16:18.212688Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"p = next(iter(dl))\nimgs, tars, _ = p","metadata":{"execution":{"iopub.status.busy":"2022-05-06T22:16:19.797360Z","iopub.execute_input":"2022-05-06T22:16:19.798059Z","iopub.status.idle":"2022-05-06T22:16:20.649020Z","shell.execute_reply.started":"2022-05-06T22:16:19.798023Z","shell.execute_reply":"2022-05-06T22:16:20.648318Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"TransformerModel(VOCAB_SIZE, hidden=HIDDEN, enc_layers=ENC_LAYERS, dec_layers=DEC_LAYERS,   \n                         nhead=N_HEADS, dropout=DROPOUT).to(device)(imgs, tars[:-1,:]).argmax(2)[-1]","metadata":{"execution":{"iopub.status.busy":"2022-05-06T22:17:17.010447Z","iopub.execute_input":"2022-05-06T22:17:17.010720Z","iopub.status.idle":"2022-05-06T22:17:18.041295Z","shell.execute_reply.started":"2022-05-06T22:17:17.010691Z","shell.execute_reply":"2022-05-06T22:17:18.040552Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"mod = TransformerModel(VOCAB_SIZE, hidden=HIDDEN, enc_layers=ENC_LAYERS, dec_layers=DEC_LAYERS,   \n                         nhead=N_HEADS, dropout=DROPOUT).to(device)\nout = mod(imgs, tars[:-1,:])","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:25:33.401830Z","iopub.execute_input":"2022-05-06T21:25:33.402036Z","iopub.status.idle":"2022-05-06T21:26:01.795566Z","shell.execute_reply.started":"2022-05-06T21:25:33.402010Z","shell.execute_reply":"2022-05-06T21:26:01.794831Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"torch.isnan(mod.backbone(imgs)).sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:26:21.755322Z","iopub.execute_input":"2022-05-06T21:26:21.755591Z","iopub.status.idle":"2022-05-06T21:26:21.878720Z","shell.execute_reply.started":"2022-05-06T21:26:21.755561Z","shell.execute_reply":"2022-05-06T21:26:21.877879Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"(out.argmax(2).flatten() == tars[1:, :].flatten()).float().mean().item()\n# [26,32] == [26,32]\n# sum(0) / 26 = [1,32]","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:22:29.605387Z","iopub.execute_input":"2022-05-06T21:22:29.605682Z","iopub.status.idle":"2022-05-06T21:22:29.613225Z","shell.execute_reply.started":"2022-05-06T21:22:29.605645Z","shell.execute_reply":"2022-05-06T21:22:29.612489Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"a = torch.as_tensor(np.array([[1,2,3,4,5], [1,2,1,1,4]]), dtype=torch.int64).flatten()\nb = torch.as_tensor(np.array([[1,2,5,4,1], [1,1,1,1,2]]), dtype=torch.int64).flatten()\na, b","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:29:53.185512Z","iopub.execute_input":"2022-05-06T19:29:53.185773Z","iopub.status.idle":"2022-05-06T19:29:53.195537Z","shell.execute_reply.started":"2022-05-06T19:29:53.185737Z","shell.execute_reply":"2022-05-06T19:29:53.194625Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"(a == b).float().mean().item()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:29:53.197050Z","iopub.execute_input":"2022-05-06T19:29:53.197574Z","iopub.status.idle":"2022-05-06T19:29:53.203463Z","shell.execute_reply.started":"2022-05-06T19:29:53.197538Z","shell.execute_reply":"2022-05-06T19:29:53.202740Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"#### End check","metadata":{}},{"cell_type":"code","source":"def train_one_batch(model, data, optimizer, criterion):\n    model.train()\n    image, target, _ = data\n    optimizer.zero_grad()\n    output = model(image, target[:-1, :])\n    \n    if torch.any(torch.isnan(output)): # unstable model :(\n        output = torch.nan_to_num(output)\n        \n    loss = criterion(output.view(-1, output.shape[-1]), torch.reshape(target[1:, :], (-1,)))\n    loss.backward()\n    optimizer.step()\n    \n    # torch.clamp(model.parameters(), 0.01, 0.01)\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 10.0)\n    \n    accuracy = (output.argmax(2).flatten() == target[1:, :].flatten()).float().mean().item()\n    return loss.item(), accuracy\n\n@torch.no_grad()\ndef validate_one_batch(model, data, criterion):\n    model.eval()\n    image, target, _ = data\n    output = model(image, target[:-1, :])\n    if torch.any(torch.isnan(output)):\n        output = torch.nan_to_num(output)\n    loss = criterion(output.view(-1, output.shape[-1]), torch.reshape(target[1:, :], (-1,)))\n    accuracy = (output.argmax(2).flatten() == target[1:, :].flatten()).float().mean().item()\n    return loss.item(), accuracy\n\n@torch.no_grad()\ndef prediction(model, filepath='random', max_len=MAX_SEQ_LEN, tokenizer=tokenizer):\n    label = None\n    if filepath == 'random':\n        idx = np.random.randint(len(test))\n        filepath = root + '/' + test.iloc[idx, 0]\n        label = test.iloc[idx, -1]\n\n    model.eval()\n    img = cv2.imread(filepath)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    image = valid_transforms(img)\n    src = torch.FloatTensor(image).unsqueeze(0).to(device)\n\n    out_indexes = [101, ]\n\n    for i in range(max_len):\n                \n        trg_tensor = torch.LongTensor(out_indexes).unsqueeze(1).to(device)\n                \n        output = model(src, trg_tensor)\n        out_token = output.argmax(2)[-1].item()\n        out_indexes.append(out_token)\n        if out_token == 102:\n            break\n    preds = tokenizer.decode(out_indexes[1:])\n    plt.figure(figsize=(6,4))\n    plt.title(\"Predicted / Truth\")\n    plt.imshow(img)\n    plt.tight_layout()\n    plt.show()\n    plt.pause(0.001)\n    print(f'Prediction: {preds}, \\nTruth: {label if label is not None else \"NO label\"}')\n    \n    return preds","metadata":{"execution":{"iopub.status.busy":"2022-05-06T22:17:26.469811Z","iopub.execute_input":"2022-05-06T22:17:26.470071Z","iopub.status.idle":"2022-05-06T22:17:26.486105Z","shell.execute_reply.started":"2022-05-06T22:17:26.470043Z","shell.execute_reply":"2022-05-06T22:17:26.485433Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"model = TransformerModel(VOCAB_SIZE, hidden=HIDDEN, enc_layers=ENC_LAYERS, dec_layers=DEC_LAYERS,   \n                         nhead=N_HEADS, dropout=DROPOUT).to(device)\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-6)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, min_lr=1e-6, factor=0.1)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T22:17:29.571243Z","iopub.execute_input":"2022-05-06T22:17:29.571931Z","iopub.status.idle":"2022-05-06T22:17:30.488206Z","shell.execute_reply.started":"2022-05-06T22:17:29.571892Z","shell.execute_reply":"2022-05-06T22:17:30.487445Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"train_losses, valid_losses, train_accuracies, valid_accuracies = [], [], [], []\n\nfor epoch in range(EPOCHS):\n    print(f'{epoch+1}/{EPOCHS} epoch.')\n    epoch_train_losses, epoch_valid_losses, epoch_train_accuracy, epoch_valid_accuracy = [], [], [], []\n    tk0 = tqdm(train_dataloader, total=len(train_dataloader), leave=True)\n    \n    for _, batch in enumerate(tk0):\n        loss, train_accuracy = train_one_batch(model, batch, optimizer, criterion)\n        epoch_train_losses.append(loss)\n        epoch_train_accuracy.append(train_accuracy)\n        tk0.set_postfix(loss=loss)\n        \n    train_epoch_loss = np.array(epoch_train_losses).mean()\n    train_losses.append(train_epoch_loss)\n    train_accuracy = np.array(epoch_train_accuracy).mean()\n    train_accuracies.append(train_accuracy)\n    \n    tk1 = tqdm(valid_dataloader, total=len(valid_dataloader), leave=True)\n    for _, batch in enumerate(tk1):\n        loss, valid_accuracy = validate_one_batch(model, batch, criterion)\n        epoch_valid_losses.append(loss)\n        epoch_valid_accuracy.append(valid_accuracy)\n        tk1.set_postfix(loss=loss)\n        \n    valid_epoch_loss = np.array(epoch_valid_losses).mean()\n    valid_losses.append(valid_epoch_loss)\n    valid_accuracy = np.array(epoch_valid_accuracy).mean()\n    valid_accuracies.append(valid_accuracy)\n    \n    print(f'Epoch {epoch+1} summary:')\n    print(f'Train loss: {train_epoch_loss:.4f}, validation loss: {valid_epoch_loss:.4f}')\n    print(f'Train accuracy: {train_accuracy:.4f}, validation accuracy: {valid_accuracy:.4f}\\n')\n    \n    scheduler.step(valid_epoch_loss)\n    \n    if (epoch+1) % 2 == 0:\n        pred = prediction(model)\n        torch.save(model.state_dict(), 'model.pth')","metadata":{"execution":{"iopub.status.busy":"2022-05-06T22:17:33.774095Z","iopub.execute_input":"2022-05-06T22:17:33.774360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = prediction(model)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T22:07:58.473422Z","iopub.execute_input":"2022-05-06T22:07:58.473992Z","iopub.status.idle":"2022-05-06T22:08:01.213095Z","shell.execute_reply.started":"2022-05-06T22:07:58.473953Z","shell.execute_reply":"2022-05-06T22:08:01.212262Z"},"trusted":true},"execution_count":22,"outputs":[]}]}