{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport transformers\nimport torchvision\nimport torch.nn as nn\nfrom torchvision import transforms as T\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pack_padded_sequence\n\nimport cv2\nimport time\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.autonotebook import tqdm\n\nfrom sklearn.model_selection import train_test_split\n\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n\n!pip install -qq editdistance torchsummary\nimport editdistance\nfrom torchsummary import summary\n\nseed = 42","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-10T19:25:43.234148Z","iopub.execute_input":"2022-05-10T19:25:43.234798Z","iopub.status.idle":"2022-05-10T19:26:02.718962Z","shell.execute_reply.started":"2022-05-10T19:25:43.234689Z","shell.execute_reply":"2022-05-10T19:26:02.717937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.random.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\nnp.random.seed(seed)\nfrom IPython.display import clear_output\n\ntokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n\nMAX_SEQ_LEN = 120  # 120 is approximately upper IQR of sequences lengths\nLR = 5e-5 # 1e-4\nEPOCHS = 50\nIMAGE_SIZE = 256\nBATCH_SIZE = 32  # decreasing batch_size instead of LR may help \nHIDDEN = 512 \nENC_LAYERS = 6 # 2 init, 6 in original paper, init hyperparameters are more or less give some reasonable results\nDEC_LAYERS = 6 # 2 init, 6 in original paper\nN_HEADS = 8 # 4 init, 8 in original paper\nDROPOUT = 0.1\nPATH = r'caption_model.pth'\nVOCAB_SIZE = tokenizer.vocab_size\nclear_output()\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Currently using \"{device.upper()}\" device')","metadata":{"execution":{"iopub.status.busy":"2022-05-10T19:28:54.439176Z","iopub.execute_input":"2022-05-10T19:28:54.439467Z","iopub.status.idle":"2022-05-10T19:28:56.196777Z","shell.execute_reply.started":"2022-05-10T19:28:54.439436Z","shell.execute_reply":"2022-05-10T19:28:56.196114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '../input/flickr30k/'\ndata = np.zeros((158916, 3), dtype=np.object)\ni = 0\nfor line in open(path + 'captions.txt', 'r'):\n    data[i, :] = line.replace('\\n', \"\").split('|')\n    i += 1\n    \ndf = pd.DataFrame(data=data[1:, :], columns=data[0, :])\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T19:26:11.164407Z","iopub.execute_input":"2022-05-10T19:26:11.164675Z","iopub.status.idle":"2022-05-10T19:26:11.6299Z","shell.execute_reply.started":"2022-05-10T19:26:11.164643Z","shell.execute_reply":"2022-05-10T19:26:11.629228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transforms = T.Compose([\n                              T.ToPILImage(),\n                              T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n                              T.RandomCrop(IMAGE_SIZE),\n                              T.ColorJitter(brightness=(0.95, 1.05),\n                                            contrast=(0.95, 1.05),\n                                            saturation=(0.98, 1.02),\n                                            hue=0.05),\n                              T.RandomHorizontalFlip(p=0.1),\n                              T.GaussianBlur(kernel_size=(1, 3), sigma=(0.1, 0.5)),\n                              T.RandomAdjustSharpness(sharpness_factor=1.2, p=0.2),\n                              T.RandomRotation(degrees=(-5, 5)),\n                              T.ToTensor(),\n                              T.Normalize(mean=[0.485, 0.456, 0.406], \n                                          std=[0.229, 0.224, 0.225])\n])\n\nvalid_transforms = T.Compose([\n                              T.ToPILImage(),\n                              T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n                              T.ToTensor(),\n                              T.Normalize(mean=[0.485, 0.456, 0.406], \n                                          std=[0.229, 0.224, 0.225])\n])\ninvTrans = T.Compose([T.Normalize(mean = [ 0., 0., 0. ],\n                                      std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n                          T.Normalize(mean = [ -0.485, -0.456, -0.406 ],\n                                      std = [ 1., 1., 1. ]),\n                          T.ToPILImage(),\n                         ])","metadata":{"execution":{"iopub.status.busy":"2022-05-10T19:26:14.951072Z","iopub.execute_input":"2022-05-10T19:26:14.95164Z","iopub.status.idle":"2022-05-10T19:26:14.963014Z","shell.execute_reply.started":"2022-05-10T19:26:14.951595Z","shell.execute_reply":"2022-05-10T19:26:14.961535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"root = '../input/flickr30k/images'\n\nclass CapDataset(Dataset):\n    def __init__(self, df, root, tokenizer, transforms=None):\n        self.df = df\n        self.root = root\n        self.tokenizer = tokenizer\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, ix):\n        row = self.df.iloc[ix].squeeze()\n        id = row.image_name\n        image_path = f'{self.root}/{id}'\n\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        caption = row.caption_text\n\n        target = tokenizer(caption, \n                           return_token_type_ids=False, \n                           return_attention_mask=False, \n                           max_length=MAX_SEQ_LEN, \n                           padding=\"do_not_pad\",\n                           return_tensors=\"pt\")\n        target = target['input_ids'].squeeze()\n        target = torch.LongTensor(target)\n        return image, target, caption\n\n    def collate_fn(self, batch):\n        batch.sort(key=lambda x: len(x[1]), reverse=True)\n        images, targets, captions = zip(*batch)\n        images = torch.stack([self.transforms(image) for image in images], 0)\n        lengths = [len(tar) for tar in targets]\n        _targets = torch.zeros(len(captions), max(lengths)).long()\n        for i, tar in enumerate(targets):\n            end = lengths[i]\n            _targets[i, :end] = tar[:end] \n        _targets = _targets.permute(1,0)\n        return images.to(device), _targets.to(device), torch.tensor(lengths).long().to(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T19:26:17.920158Z","iopub.execute_input":"2022-05-10T19:26:17.920873Z","iopub.status.idle":"2022-05-10T19:26:17.934214Z","shell.execute_reply.started":"2022-05-10T19:26:17.920834Z","shell.execute_reply":"2022-05-10T19:26:17.933309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, test = train_test_split(df, test_size=0.01, shuffle=True, random_state=seed)\ntrain, valid = train_test_split(train, test_size=0.1, shuffle=True, random_state=seed)\ntrain.reset_index(drop=True, inplace=True)\nvalid.reset_index(drop=True, inplace=True)\ntest.reset_index(drop=True, inplace=True)\nprint(f'Train size: {train.shape[0]}, valid size: {valid.shape[0]}, test size: {test.shape[0]}')","metadata":{"execution":{"iopub.status.busy":"2022-05-10T19:26:21.23455Z","iopub.execute_input":"2022-05-10T19:26:21.23482Z","iopub.status.idle":"2022-05-10T19:26:21.297318Z","shell.execute_reply.started":"2022-05-10T19:26:21.234786Z","shell.execute_reply":"2022-05-10T19:26:21.296589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = CapDataset(train, root, tokenizer, train_transforms)\nvalid_dataset = CapDataset(valid, root, tokenizer, valid_transforms)\ntest_dataset = CapDataset(test, root, tokenizer, valid_transforms)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=train_dataset.collate_fn, drop_last=True)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=valid_dataset.collate_fn, drop_last=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=test_dataset.collate_fn)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T19:26:24.208851Z","iopub.execute_input":"2022-05-10T19:26:24.209445Z","iopub.status.idle":"2022-05-10T19:26:24.217212Z","shell.execute_reply.started":"2022-05-10T19:26:24.209402Z","shell.execute_reply":"2022-05-10T19:26:24.216363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, encoded_image_size=8, pretrained=True):\n        super(Encoder, self).__init__()\n        self.enc_image_size = encoded_image_size\n\n        resnet = torchvision.models.resnet101(pretrained=pretrained)\n        modules = list(resnet.children())[:-2]\n        self.resnet = nn.Sequential(*modules)\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n\n        for param in self.resnet.parameters():\n            param.requires_grad = False\n\n        for child in list(self.resnet.children())[5:]:\n            for param in child.parameters():\n                param.requires_grad = True\n                \n        self.fc = nn.Sequential(nn.Conv2d(2048, 512, 1),\n                                nn.LeakyReLU(0.01, inplace=True)) # to prevent vanishing grads, BatchNorm also helps.\n        # if the input values became too large or too small - batch norm calcs batch mean and batch std (instance - object \n        # mean and std), learns mu and beta\n        # while LeakyReLU or gelu comes to the rescue when we have lots of negative outputs, that become zeros after ReLU\n        # aka - dying relu - vanishing grads\n    def forward(self, images):\n        out = self.resnet(images)\n        out = self.fc(out)\n        out = self.adaptive_pool(out) # [B, C, H, W]\n        #out = out.permute(0, 3, 1, 2)\n        out = out.flatten(2) # [B, C, HW]\n        out = out.permute(2,0,1) # [HW, B, C]\n        return out\n    \nclass Encoder_50(nn.Module):\n    def __init__(self, bb_name='resnet50', hidden=HIDDEN, pretrained=False):\n        super(Encoder_50, self).__init__()\n        self.backbone = torchvision.models.__getattribute__(bb_name)(pretrained=pretrained)\n        self.backbone.fc = nn.Conv2d(2048, hidden, 1)\n    \n    def forward(self, src):\n        x = self.backbone.conv1(src)\n\n        x = self.backbone.bn1(x)\n        x = self.backbone.relu(x)\n        x = self.backbone.maxpool(x)\n        x = self.backbone.layer1(x)\n        x = self.backbone.layer2(x)\n        x = self.backbone.layer3(x)\n        x = self.backbone.layer4(x) # [32, 2048, 8, 8] : [B,C,H,W]\n            \n        x = self.backbone.fc(x) # [32, 512, 8, 8] : [B,C,H,W]\n        # x = x.permute(0, 3, 1, 2) # [64, 8, 512, 8] : [B,W,C,H]\n        x = x.flatten(2) # [32, 512, 64] : [B,C,WH]\n        #x = x.permute(1, 0, 2) # [64, 32, 512] : [W,B,CH]\n        x = x.permute(2,0,1)\n        return x\n    \nclass TransformerModel(nn.Module):\n    def __init__(self, outtoken, hidden, enc_layers=1, dec_layers=1, nhead=1, dropout=0.1, pretrained=False):\n        super(TransformerModel, self).__init__()\n        self.backbone = Encoder_50(pretrained=pretrained)\n        \n        self.pos_emb = nn.Embedding.from_pretrained(self.get_position_embedding_table(), freeze=True) ##\n        self.pos_encoder = PositionalEncoding(hidden, dropout)\n        self.decoder = nn.Embedding(outtoken, hidden, padding_idx=0)\n        self.pos_decoder = PositionalEncoding(hidden, dropout)\n        self.transformer = nn.Transformer(d_model=hidden, nhead=nhead, num_encoder_layers=enc_layers,\n                                          num_decoder_layers=dec_layers, dim_feedforward=hidden * 4, dropout=dropout,\n                                          activation='gelu')  # 'relu' or torch.nn.functional.leaky_relu\n\n        self.fc_out = nn.Linear(hidden, outtoken)\n        self.src_mask = None\n        self.trg_mask = None\n        self.memory_mask = None\n        \n    def get_position_embedding_table(self, num_pix=8, ch_pos=256): ##\n        def cal_angle(position, hid_idx):\n            x = position % num_pix\n            y = position // num_pix\n            x_enc = x / np.power(10000, hid_idx / ch_pos)\n            y_enc = y / np.power(10000, hid_idx / ch_pos)\n            return np.sin(x_enc), np.sin(y_enc)\n        def get_posi_angle_vec(position):\n            return [cal_angle(position, hid_idx)[0] for hid_idx in range(ch_pos)] + [cal_angle(position, hid_idx)[1] for hid_idx in range(ch_pos)]\n\n        embedding_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(int(num_pix**2))])\n        return torch.FloatTensor(embedding_table).to(device)\n\n    def generate_square_subsequent_mask(self, sz):\n        mask = torch.triu(torch.ones(sz, sz), 1)\n        mask = mask.masked_fill(mask == 1, float('-inf'))\n        return mask\n\n    def make_len_mask(self, inp):\n        return (inp == 0).transpose(0, 1)\n\n    def forward(self, src, trg):\n        if self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n            self.trg_mask = self.generate_square_subsequent_mask(len(trg)).to(device) \n        x = self.backbone(src)\n        \n        batch_size = x.size(1) ##\n        positions = x.size(0) ##\n        \n        src_pad_mask = self.make_len_mask(x[:, :, 0])\n        #src = self.pos_encoder(x)\n        pos_emb = self.pos_emb(torch.LongTensor([list(range(positions))]*batch_size).to(device)) # [32,64,512]\n        src = x + pos_emb.permute(1,0,2) #\n        \n        trg_pad_mask = self.make_len_mask(trg)\n        trg = self.decoder(trg)\n        trg = self.pos_decoder(trg)\n\n        output = self.transformer(src, trg, src_mask=self.src_mask, tgt_mask=self.trg_mask,\n                                  memory_mask=self.memory_mask,\n                                  src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=trg_pad_mask,\n                                  memory_key_padding_mask=src_pad_mask) #  [HW,B,C]\n        output = self.fc_out(output) # [L,B,C]\n\n        return output\n    \nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        self.scale = nn.Parameter(torch.ones(1))\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.scale * self.pe[:x.size(0), :]\n        return self.dropout(x) ","metadata":{"execution":{"iopub.status.busy":"2022-05-10T19:29:28.35743Z","iopub.execute_input":"2022-05-10T19:29:28.357713Z","iopub.status.idle":"2022-05-10T19:29:28.397081Z","shell.execute_reply.started":"2022-05-10T19:29:28.357679Z","shell.execute_reply":"2022-05-10T19:29:28.396014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_one_batch(model, data, optimizer, criterion):\n    model.train()\n    image, target, _ = data\n    optimizer.zero_grad()\n    output = model(image, target[:-1, :])\n    \n    if torch.any(torch.isnan(output)): # unstable model :(\n        output = torch.nan_to_num(output)\n        \n    loss = criterion(output.view(-1, output.shape[-1]), torch.reshape(target[1:, :], (-1,)))\n    loss.backward()\n    optimizer.step()\n    \n    # torch.clamp(model.parameters(), 0.01, 0.01)\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 10.0) # 1.0\n    \n    accuracy = (output.argmax(2).flatten() == target[1:, :].flatten()).float().mean().item()\n    return loss.item(), accuracy\n\n@torch.no_grad()\ndef validate_one_batch(model, data, criterion):\n    model.eval()\n    image, target, _ = data\n    output = model(image, target[:-1, :])\n    if torch.any(torch.isnan(output)):\n        output = torch.nan_to_num(output)\n    loss = criterion(output.view(-1, output.shape[-1]), torch.reshape(target[1:, :], (-1,)))\n    accuracy = (output.argmax(2).flatten() == target[1:, :].flatten()).float().mean().item()\n    return loss.item(), accuracy\n\n@torch.no_grad()\ndef prediction(model, filepath='random', max_len=MAX_SEQ_LEN, tokenizer=tokenizer, plot=True):\n    label = None\n    if filepath == 'random':\n        idx = np.random.randint(len(test))\n        filepath = root + '/' + test.iloc[idx, 0]\n        label = test.iloc[idx, -1]\n\n    model.eval()\n    img = cv2.imread(filepath)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    image = valid_transforms(img)\n    src = torch.FloatTensor(image).unsqueeze(0).to(device)\n\n    out_indexes = [101, ]\n\n    for i in range(max_len):\n                \n        trg_tensor = torch.LongTensor(out_indexes).unsqueeze(1).to(device)\n                \n        output = model(src, trg_tensor)\n        out_token = output.argmax(2)[-1].item()\n        if out_token == 102:\n            break\n        out_indexes.append(out_token)\n\n    preds = tokenizer.decode(out_indexes[1:])\n    if plot:\n        plt.figure(figsize=(6,4))\n        plt.title(\"Predicted / Truth\")\n        plt.imshow(img)\n        plt.tight_layout()\n        plt.show()\n        plt.pause(0.001)\n        print(f'Prediction: {preds}, \\nTruth: {label if label is not None else \"NO label\"}')\n    \n    return preds","metadata":{"execution":{"iopub.status.busy":"2022-05-10T19:29:38.449325Z","iopub.execute_input":"2022-05-10T19:29:38.449865Z","iopub.status.idle":"2022-05-10T19:29:38.466958Z","shell.execute_reply.started":"2022-05-10T19:29:38.449829Z","shell.execute_reply":"2022-05-10T19:29:38.466174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bleu_score_fn(method_no: int = 4, ref_type='corpus'):\n    smoothing_method = getattr(SmoothingFunction(), f'method{method_no}')\n    \n    def bleu_score_corpus(reference_corpus: list, candidate_corpus: list, n: int = 4):\n        weights = [1 / n] * n\n        return corpus_bleu(reference_corpus, candidate_corpus,\n                           smoothing_function=smoothing_method, weights=weights)\n\n    def bleu_score_sentence(reference_sentences: list, candidate_sentence: list, n: int = 4):\n        weights = [1 / n] * n\n        return sentence_bleu(reference_sentences, candidate_sentence,\n                             smoothing_function=smoothing_method, weights=weights)\n    if ref_type == 'corpus':\n        return bleu_score_corpus\n    elif ref_type == 'sentence':\n        return bleu_score_sentence\n    \n@torch.no_grad()\ndef evaluate_model(dataloader, model, bleu_score_fn, tokenizer):\n    running_bleu = [0.0] * 5\n    model.eval()\n    for batch_idx, batch in enumerate(tqdm(dataloader, leave=False)):\n        images, captions, _ = batch\n        outputs = []\n        for j in range(images.size(0)):\n            src = images[j, ...].unsqueeze(0)\n            out_indexes = [101, ]\n            for i in range(MAX_SEQ_LEN):\n                trg_tensor = torch.LongTensor(out_indexes).unsqueeze(1).to(device)      \n                output = model(src, trg_tensor)\n                out_token = output.argmax(2)[-1].item()\n                if out_token == 102:  # [SEP]\n                    break\n                out_indexes.append(out_token)\n            preds = tokenizer.decode(out_indexes[1:]).replace('[PAD]', '').split()\n            outputs.append(preds)\n            \n        captions = [tokenizer.decode(captions[:,i]).replace('[PAD]', '').split()[1:-1] for i in range(captions.size(1))]\n        for i in (1, 2, 3, 4):\n            running_bleu[i] += bleu_score_fn(reference_corpus=captions, candidate_corpus=outputs, n=i)\n    for i in (1, 2, 3, 4):\n        running_bleu[i] /= len(dataloader)\n    return running_bleu","metadata":{"execution":{"iopub.status.busy":"2022-05-10T19:54:54.084986Z","iopub.execute_input":"2022-05-10T19:54:54.085673Z","iopub.status.idle":"2022-05-10T19:54:54.099087Z","shell.execute_reply.started":"2022-05-10T19:54:54.08564Z","shell.execute_reply":"2022-05-10T19:54:54.098243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = TransformerModel(VOCAB_SIZE, hidden=HIDDEN, enc_layers=ENC_LAYERS, dec_layers=DEC_LAYERS,   \n                         nhead=N_HEADS, dropout=DROPOUT, pretrained=True).to(device)  # pretrained False initially\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-6)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, min_lr=1e-6, factor=0.1)\ncorpus_bleu_score_fn = bleu_score_fn(4, 'corpus')","metadata":{"execution":{"iopub.status.busy":"2022-05-10T19:56:37.305984Z","iopub.execute_input":"2022-05-10T19:56:37.306344Z","iopub.status.idle":"2022-05-10T19:56:39.101372Z","shell.execute_reply.started":"2022-05-10T19:56:37.306309Z","shell.execute_reply":"2022-05-10T19:56:39.100604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_losses, valid_losses, train_accuracies, valid_accuracies = [], [], [], []\ntimeout = 360.  # minutes before stop\nstart_time = time.time()\n\nfor epoch in range(EPOCHS):\n    print(f'{epoch+1}/{EPOCHS} epoch.')\n    epoch_train_losses, epoch_valid_losses, epoch_train_accuracy, epoch_valid_accuracy = [], [], [], []\n    tk0 = tqdm(train_dataloader, total=len(train_dataloader), leave=True)\n    \n    for i, batch in enumerate(tk0):\n        loss, train_accuracy = train_one_batch(model, batch, optimizer, criterion)\n        epoch_train_losses.append(loss)\n        epoch_train_accuracy.append(train_accuracy)\n        tk0.set_postfix(loss=loss)\n        if (i+1) % 100 == 0:\n            pred = prediction(model)\n        \n    train_epoch_loss = np.array(epoch_train_losses).mean()\n    train_losses.append(train_epoch_loss)\n    train_accuracy = np.array(epoch_train_accuracy).mean()\n    train_accuracies.append(train_accuracy)\n    \n    tk1 = tqdm(valid_dataloader, total=len(valid_dataloader), leave=True)\n    for _, batch in enumerate(tk1):\n        loss, valid_accuracy = validate_one_batch(model, batch, criterion)\n        epoch_valid_losses.append(loss)\n        epoch_valid_accuracy.append(valid_accuracy)\n        tk1.set_postfix(loss=loss)\n        \n    valid_epoch_loss = np.array(epoch_valid_losses).mean()\n    valid_losses.append(valid_epoch_loss)\n    valid_accuracy = np.array(epoch_valid_accuracy).mean()\n    valid_accuracies.append(valid_accuracy)\n    \n    print(f'Epoch {epoch+1} summary:')\n    print(f'Train loss: {train_epoch_loss:.4f}, validation loss: {valid_epoch_loss:.4f}')\n    print(f'Train accuracy: {train_accuracy:.4f}, validation accuracy: {valid_accuracy:.4f}')\n    print(f'Time per {epoch+1} epoch: {(time.time() - start_time)//60} minutes')\n    \n    scheduler.step(valid_epoch_loss)\n    \n    if (epoch+1) % 2 == 0:\n        print(f'Prediction after {epoch+1} epoch.')\n        pred = prediction(model)\n        torch.save(model.state_dict(), 'model.pth')\n    \n    if (epoch+1) % 5 == 0:\n        test_bleu = evaluate_model(test_dataloader, model, corpus_bleu_score_fn, tokenizer)\n        print(''.join([f'test_bleu{i}: {test_bleu[i]:.4f} ' for i in (1, 4)]))\n        \n    if (time.time() - start_time)//60 > timeout:\n        print(f'Timeout stop.')\n        torch.save(model.state_dict(), 'model.pth')\n        break","metadata":{"execution":{"iopub.status.busy":"2022-05-10T19:57:38.75189Z","iopub.execute_input":"2022-05-10T19:57:38.752172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = prediction(model)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T22:07:58.473422Z","iopub.execute_input":"2022-05-06T22:07:58.473992Z","iopub.status.idle":"2022-05-06T22:08:01.213095Z","shell.execute_reply.started":"2022-05-06T22:07:58.473953Z","shell.execute_reply":"2022-05-06T22:08:01.212262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Link to the evaluated model outputs\n[https://www.kaggle.com/code/pankratozzi/pytorch-transformer-caption-experiment/notebook](http://)","metadata":{}}]}