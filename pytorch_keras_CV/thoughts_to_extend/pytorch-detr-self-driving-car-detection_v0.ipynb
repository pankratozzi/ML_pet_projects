{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone -q https://github.com/facebookresearch/detr.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-31T19:14:44.883108Z","iopub.execute_input":"2022-03-31T19:14:44.883720Z","iopub.status.idle":"2022-03-31T19:14:47.218657Z","shell.execute_reply.started":"2022-03-31T19:14:44.883679Z","shell.execute_reply":"2022-03-31T19:14:47.217743Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"[https://www.kaggle.com/code/tanulsingh077/end-to-end-object-detection-with-transformers-detr/notebook](http://)","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport numpy as np \nimport pandas as pd \nfrom datetime import datetime\nimport time\nimport random\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.model_selection import StratifiedKFold\n\nimport cv2\n\nimport sys\nsys.path.append('./detr/')\n\nfrom detr.models.matcher import HungarianMatcher\nfrom detr.models.detr import SetCriterion\n\nimport albumentations as A # advanced augmentation framework with PyTorch interface\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nfrom glob import glob","metadata":{"execution":{"iopub.status.busy":"2022-03-31T19:14:48.771921Z","iopub.execute_input":"2022-03-31T19:14:48.772535Z","iopub.status.idle":"2022-03-31T19:14:52.619497Z","shell.execute_reply.started":"2022-03-31T19:14:48.772495Z","shell.execute_reply":"2022-03-31T19:14:52.618781Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Currently using \"{device}\" device')","metadata":{"execution":{"iopub.status.busy":"2022-03-31T19:14:53.563135Z","iopub.execute_input":"2022-03-31T19:14:53.564063Z","iopub.status.idle":"2022-03-31T19:14:53.615550Z","shell.execute_reply.started":"2022-03-31T19:14:53.564005Z","shell.execute_reply":"2022-03-31T19:14:53.614817Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2022-03-31T19:14:57.091331Z","iopub.execute_input":"2022-03-31T19:14:57.092122Z","iopub.status.idle":"2022-03-31T19:14:57.098738Z","shell.execute_reply.started":"2022-03-31T19:14:57.092082Z","shell.execute_reply":"2022-03-31T19:14:57.098078Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# load validation dataframe just to inspect some images, bboxes and classes\ndf = pd.read_csv('../input/self-driving-cars/labels_val.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T19:15:00.455920Z","iopub.execute_input":"2022-03-31T19:15:00.456584Z","iopub.status.idle":"2022-03-31T19:15:00.525402Z","shell.execute_reply.started":"2022-03-31T19:15:00.456546Z","shell.execute_reply":"2022-03-31T19:15:00.524708Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"n_folds = 5\nseed = 42\nnum_classes = 6 # 5 unique classes + background class\nnum_queries = 50 # max number of objects to detect per one image, default in detr = 100, strictly recommended by \n# developers to change this parameter only when training from scratch\nnull_class_coef = 0.1  # used as default in original repository. Set 0.5 if detecting 2 classes: object and background\nBATCH_SIZE = 16\nIMAGE_SIZE = 224 # 512\nLR = 1e-3 # 2e-5 \nEPOCHS = 4","metadata":{"execution":{"iopub.status.busy":"2022-03-31T20:11:02.144872Z","iopub.execute_input":"2022-03-31T20:11:02.145147Z","iopub.status.idle":"2022-03-31T20:11:02.152623Z","shell.execute_reply.started":"2022-03-31T20:11:02.145119Z","shell.execute_reply":"2022-03-31T20:11:02.151945Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"labels_to_ids = {'car': 1, 'truck': 2, 'pedestrian': 3, 'bicyclist': 4, 'light': 5}\nids_to_labels = {1: 'car', 2: 'truck', 3: 'pedestrian', 4: 'bicyclist', 5: 'light'}","metadata":{"execution":{"iopub.status.busy":"2022-03-31T19:15:17.475698Z","iopub.execute_input":"2022-03-31T19:15:17.476282Z","iopub.status.idle":"2022-03-31T19:15:17.481199Z","shell.execute_reply.started":"2022-03-31T19:15:17.476239Z","shell.execute_reply":"2022-03-31T19:15:17.480298Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything(seed)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T19:15:20.671622Z","iopub.execute_input":"2022-03-31T19:15:20.671887Z","iopub.status.idle":"2022-03-31T19:15:20.680463Z","shell.execute_reply.started":"2022-03-31T19:15:20.671850Z","shell.execute_reply":"2022-03-31T19:15:20.679673Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# base bbox format\nrandom_image = df['frame'].sample(1).iloc[0]\ndf_random = df[df['frame'] == random_image]\nsample_image = cv2.imread('../input/self-driving-cars/images/' + random_image, cv2.IMREAD_COLOR)\nsample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(8,8))\nplt.imshow(sample_image)\nax = plt.gca()\n\nfor idx, row in df_random.iterrows():\n    xmin, xmax, ymin, ymax = row[['xmin', 'xmax', 'ymin', 'ymax']]\n    ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n                                fill=False, color='red', linewidth=3))\n    text = f'Class_id: {row[\"class_id\"]}'\n    ax.text(xmin, ymin, text, fontsize=15,\n            bbox=dict(facecolor='yellow', alpha=0.5))\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T19:15:24.738626Z","iopub.execute_input":"2022-03-31T19:15:24.739323Z","iopub.status.idle":"2022-03-31T19:15:25.023568Z","shell.execute_reply.started":"2022-03-31T19:15:24.739281Z","shell.execute_reply":"2022-03-31T19:15:25.022873Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def xyxy_to_xywh(xyxy):\n    \"\"\"Convert [x1 y1 x2 y2] box format to [x1 y1 w h] format.\"\"\"\n    if isinstance(xyxy, (list, tuple)):\n        # Single box given as a list of coordinates\n        assert len(xyxy) == 4\n        x1, y1 = xyxy[0], xyxy[1]\n        w = xyxy[2] - x1 + 1\n        h = xyxy[3] - y1 + 1\n        return (x1, y1, w, h)\n    elif isinstance(xyxy, np.ndarray):\n        # Multiple boxes given as a 2D ndarray\n        return np.hstack((xyxy[:, 0:2], xyxy[:, 2:4] - xyxy[:, 0:2] + 1))\n    else:\n        raise TypeError('Argument xyxy must be a list, tuple, or numpy array.')","metadata":{"execution":{"iopub.status.busy":"2022-03-31T19:15:29.603738Z","iopub.execute_input":"2022-03-31T19:15:29.604517Z","iopub.status.idle":"2022-03-31T19:15:29.612171Z","shell.execute_reply.started":"2022-03-31T19:15:29.604472Z","shell.execute_reply":"2022-03-31T19:15:29.611268Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/self-driving-cars/labels_train.csv')\ntest = pd.read_csv('../input/self-driving-cars/labels_val.csv')\n\noutliers = train[train['ymax'] == 0].index # outliers\ntrain.drop(outliers, inplace=True)\n\n#train['h'] = train['ymax'] - train['ymin'] + 1\n#train['w'] = train['xmax'] - train['xmin'] + 1\ntrain[['x', 'y', 'w', 'h']] = xyxy_to_xywh(train[['xmin', 'ymin', 'xmax', 'ymax']].values)\n\n#test['h'] = test['ymax'] - test['ymin'] + 1\n#test['w'] = test['xmax'] - test['xmin'] + 1\ntest[['x', 'y', 'w', 'h']] = xyxy_to_xywh(test[['xmin', 'ymin', 'xmax', 'ymax']].values)\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T19:15:32.653645Z","iopub.execute_input":"2022-03-31T19:15:32.654011Z","iopub.status.idle":"2022-03-31T19:15:32.859405Z","shell.execute_reply.started":"2022-03-31T19:15:32.653973Z","shell.execute_reply":"2022-03-31T19:15:32.858616Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"In coco, a bounding box is defined by four values in pixels [x_min, y_min, width, height]. They are coordinates of the top-left corner along with the width and height of the bounding box.","metadata":{}},{"cell_type":"code","source":"# coco bbox format\nrandom_image = train['frame'].sample(1).iloc[0]\ndf_random = train[train['frame'] == random_image]\nsample_image = cv2.imread('../input/self-driving-cars/images/' + random_image, cv2.IMREAD_COLOR)\nsample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(8,8))\nplt.imshow(sample_image)\nax = plt.gca()\n\nfor idx, row in df_random.iterrows():\n    x, y, w, h = row[['x', 'y', 'w', 'h']]\n    ax.add_patch(plt.Rectangle((x, y), w, h,\n                                fill=False, color='red', linewidth=3))\n    text = f'Class_id: {row[\"class_id\"]}'\n    ax.text(x, y, text, fontsize=15,\n            bbox=dict(facecolor='yellow', alpha=0.5))\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T06:42:03.491641Z","iopub.execute_input":"2022-03-31T06:42:03.492392Z","iopub.status.idle":"2022-03-31T06:42:03.865341Z","shell.execute_reply.started":"2022-03-31T06:42:03.492357Z","shell.execute_reply":"2022-03-31T06:42:03.864276Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# get n_folded dataframe, stratified by number of bboxes, truing to preserve target-value counts\nskf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n\ndf_folds = train[['frame']].copy()\ndf_folds.loc[:, 'bbox_count'] = 1\ndf_folds = df_folds.groupby('frame').count()\ndf_folds.loc[:, 'class_id'] = train[['frame', 'class_id']].groupby('frame').max()['class_id']  # min\ndf_folds.loc[:, 'stratify_group'] = np.char.add(\n    df_folds['class_id'].values.astype(str),\n    df_folds['bbox_count'].apply(lambda x: f'_{x // ((num_classes-1)*2 + 1)}').values.astype(str)\n)\ndf_folds.loc[:, 'fold'] = 0\n\nfor fold_number, (train_index, val_index) in enumerate(skf.split(X=df_folds.index, y=df_folds['stratify_group'])):\n    df_folds.loc[df_folds.iloc[val_index].index, 'fold'] = fold_number","metadata":{"execution":{"iopub.status.busy":"2022-03-31T19:15:55.642809Z","iopub.execute_input":"2022-03-31T19:15:55.643179Z","iopub.status.idle":"2022-03-31T19:15:55.825905Z","shell.execute_reply.started":"2022-03-31T19:15:55.643140Z","shell.execute_reply":"2022-03-31T19:15:55.825187Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# check class balances in each fold\nfor i in  range(5):\n    print(df_folds[df_folds['fold'] != i].class_id.value_counts(normalize=False))\n    print(df_folds[df_folds['fold'] != i].index.nunique())\n    print(df_folds[df_folds['fold'] == i].index.nunique())","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:45:54.288955Z","iopub.execute_input":"2022-03-31T11:45:54.289203Z","iopub.status.idle":"2022-03-31T11:45:54.334672Z","shell.execute_reply.started":"2022-03-31T11:45:54.289175Z","shell.execute_reply":"2022-03-31T11:45:54.333903Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train['class_id'].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T06:54:44.403395Z","iopub.execute_input":"2022-03-31T06:54:44.403867Z","iopub.status.idle":"2022-03-31T06:54:44.412828Z","shell.execute_reply.started":"2022-03-31T06:54:44.403828Z","shell.execute_reply":"2022-03-31T06:54:44.411952Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# albumentations transforms for PyTorch\ndef get_train_transforms():\n    return A.Compose([A.OneOf([A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5), # 0.9      \n                      A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.9)],p=0.5), # 0.9\n                      A.ToGray(p=0.01),\n                      A.HorizontalFlip(p=0.1), # 0.5\n                      A.VerticalFlip(p=0.1), # 0.5\n                      A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE, p=1),\n                      A.Cutout(num_holes=4, max_h_size=32, max_w_size=32, fill_value=0, p=0.1), # 8, 64, 64, 0.5\n                      ToTensorV2(p=1.0)],p=1.0,\n                      bbox_params=A.BboxParams(format='coco', min_area=0, min_visibility=0, label_fields=['labels'])\n                      )\n\ndef get_valid_transforms():\n    return A.Compose([A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE, p=1.0),\n                      ToTensorV2(p=1.0)], \n                      p=1.0, \n                      bbox_params=A.BboxParams(format='coco', min_area=0, min_visibility=0, label_fields=['labels'])\n                      )","metadata":{"execution":{"iopub.status.busy":"2022-03-31T19:16:00.574806Z","iopub.execute_input":"2022-03-31T19:16:00.575311Z","iopub.status.idle":"2022-03-31T19:16:00.584978Z","shell.execute_reply.started":"2022-03-31T19:16:00.575271Z","shell.execute_reply":"2022-03-31T19:16:00.584145Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"As classes are extremely imbalanced, it is good practice to sample the data in dataloader. Below is an example \nof WeightedRandomSampler, that shows a way of dealing with imbalanced classes using weighted oversampling technique.\n```\n# alt.: gives weights > 1\nclass_counts = y_train.value_counts().to_list()\nnum_samples = sum(class_counts)\nlabels = y_train.map(labels_to_int).values\n\nclass_weights = [num_samples/class_counts[i] for i in range(len(class_counts))]\nweights = [class_weights[labels[i]] for i in range(int(num_samples))]\nsampler = torch.utils.data.WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))\n```","metadata":{}},{"cell_type":"code","source":"# example of sampler, which we can add when defining train dataloader\ntarget_labels = df_folds.loc[df_folds['fold'] == 0, 'class_id']\n\ndef get_sampler(target_labels):\n    class_sample_count = np.unique(target_labels, return_counts=True)[1]\n    weight = 1./class_sample_count\n    samples_weight = weight[target_labels.values-1]\n    samples_weight = torch.from_numpy(samples_weight)\n    sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight))\n    return sampler\n\nsampler = get_sampler(target_labels)  # pay attention at the background class","metadata":{"execution":{"iopub.status.busy":"2022-03-31T19:16:09.731671Z","iopub.execute_input":"2022-03-31T19:16:09.732371Z","iopub.status.idle":"2022-03-31T19:16:09.745446Z","shell.execute_reply.started":"2022-03-31T19:16:09.732330Z","shell.execute_reply":"2022-03-31T19:16:09.744649Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_path = '../input/self-driving-cars/images'\n\nclass CarDataset(Dataset):\n    \"\"\" Define custom dataset class that returns an image tensor with corresponded target and image name\"\"\"\n    def __init__(self, image_ids, df, transforms=None):\n        self.image_ids = image_ids\n        self.df = df\n        self.transforms = transforms\n        \n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n    \n    def __getitem__(self, index):\n        image_id = self.image_ids[index]\n        records = self.df[self.df['frame'] == image_id]\n        \n        image = cv2.imread(f'{train_path}/{image_id}', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n        boxes = records[['x', 'y', 'w', 'h']].values\n        \n        boxes[:, 2] = np.clip(boxes[:, 2], a_min=5, a_max=None) # clip small boxes for better convergence\n        boxes[:, 3] = np.clip(boxes[:, 3], a_min=5, a_max=None)\n        \n        area = boxes[:,2] * boxes[:,3]\n        area = torch.as_tensor(area, dtype=torch.float32)\n        \n        # set all labels to 0 if our task is only to detect every object on image without label\n        #labels =  np.zeros(len(boxes), dtype=np.int32)\n        labels = records['class_id'].values.astype(np.int32) - 1\n        \n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': boxes,\n                'labels': labels\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            boxes = sample['bboxes']\n            labels = sample['labels']\n                        \n        _,h,w = image.shape\n        boxes = A.augmentations.bbox_utils.normalize_bboxes(sample['bboxes'],rows=h,cols=w)\n        target = {}\n        target['boxes'] = torch.as_tensor(boxes,dtype=torch.float32)\n        target['labels'] = torch.as_tensor(labels,dtype=torch.long)\n        target['image_id'] = torch.tensor([index])\n        target['area'] = area\n        \n        return image, target, image_id\n    \n    def collate_fn(self, batch):\n        return tuple(zip(*batch))","metadata":{"execution":{"iopub.status.busy":"2022-03-31T20:11:15.800175Z","iopub.execute_input":"2022-03-31T20:11:15.800490Z","iopub.status.idle":"2022-03-31T20:11:15.814377Z","shell.execute_reply.started":"2022-03-31T20:11:15.800455Z","shell.execute_reply":"2022-03-31T20:11:15.813412Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"#### Define DETR model taken from facebook research","metadata":{}},{"cell_type":"code","source":"from detr.models.detr import MLP\n\nclass DETRModel(nn.Module):\n    def __init__(self, num_classes, num_queries):\n        super(DETRModel,self).__init__()\n        self.num_classes = num_classes\n        self.num_queries = num_queries\n        \n        self.model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)\n        self.in_features = self.model.class_embed.in_features\n        \n        #for param in self.model.parameters():#\n        #    param.requires_grad = False#\n        \n        self.model.class_embed = nn.Linear(in_features=self.in_features, out_features=self.num_classes)\n        self.model.num_queries = self.num_queries\n        #self.model.query_embed = nn.Embedding(self.num_queries, 256)\n        #self.model.bbox_embed = MLP(256, 256, 4, 3)\n        \n    def forward(self,images):\n        return self.model(images)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T20:36:15.787366Z","iopub.execute_input":"2022-03-31T20:36:15.787681Z","iopub.status.idle":"2022-03-31T20:36:15.794126Z","shell.execute_reply.started":"2022-03-31T20:36:15.787651Z","shell.execute_reply":"2022-03-31T20:36:15.793207Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# cross entropy loss for classification, bbox-loss for regression, IoU loss for background\nmatcher = HungarianMatcher()\n\nweight_dict = weight_dict = {'loss_ce': 1, 'loss_bbox': 1 , 'loss_giou': 1}\n\nlosses = ['labels', 'boxes', 'cardinality']","metadata":{"execution":{"iopub.status.busy":"2022-03-31T21:56:04.386508Z","iopub.execute_input":"2022-03-31T21:56:04.387068Z","iopub.status.idle":"2022-03-31T21:56:04.392367Z","shell.execute_reply.started":"2022-03-31T21:56:04.387027Z","shell.execute_reply":"2022-03-31T21:56:04.391726Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"#### Train and eval functions. We train criterion also.\nMAP was commented due to issue in importing torchmetrics in kaggle","metadata":{}},{"cell_type":"code","source":"def train_fn(dataloader, model, criterion, optimizer, scheduler, epoch):\n    model.train()\n    criterion.train()\n    \n    summary_loss = AverageMeter()\n    \n    tk0 = tqdm(dataloader, total=len(dataloader), leave=True)\n    \n    for step, (images, targets, image_ids) in enumerate(tk0):\n        \n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        \n\n        output = model(images)\n\n        loss_dict = criterion(output, targets)\n        weight_dict = criterion.weight_dict\n        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n        \n        optimizer.zero_grad()\n\n        losses.backward()\n        optimizer.step()\n        if scheduler is not None:\n            scheduler.step()    \n        \n        summary_loss.update(losses.item(), BATCH_SIZE)\n        tk0.set_postfix(loss=summary_loss.avg) # print out average losses after each epoch\n    \n    return summary_loss","metadata":{"execution":{"iopub.status.busy":"2022-03-31T19:16:26.048057Z","iopub.execute_input":"2022-03-31T19:16:26.048361Z","iopub.status.idle":"2022-03-31T19:16:26.061648Z","shell.execute_reply.started":"2022-03-31T19:16:26.048328Z","shell.execute_reply":"2022-03-31T19:16:26.059203Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef eval_fn(dataloader, model, criterion):\n    model.eval()\n    criterion.eval()\n    summary_loss = AverageMeter()\n            \n    tk0 = tqdm(dataloader, total=len(dataloader), leave=True)\n    for step, (images, targets, image_ids) in enumerate(tk0):\n            \n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        output = model(images)\n\n        loss_dict = criterion(output, targets)\n        weight_dict = criterion.weight_dict\n        \n        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n        summary_loss.update(losses.item(),BATCH_SIZE)\n        tk0.set_postfix(loss=summary_loss.avg)\n        \n    return summary_loss","metadata":{"execution":{"iopub.status.busy":"2022-03-31T19:16:48.392403Z","iopub.execute_input":"2022-03-31T19:16:48.392681Z","iopub.status.idle":"2022-03-31T19:16:48.403472Z","shell.execute_reply.started":"2022-03-31T19:16:48.392650Z","shell.execute_reply":"2022-03-31T19:16:48.402575Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### Run learning process on n_folds","metadata":{}},{"cell_type":"code","source":"def run(fold, sample=False):\n    \n    df_train = df_folds[df_folds['fold'] != fold]\n    df_valid = df_folds[df_folds['fold'] == fold]\n    \n    sampler = get_sampler(df_train['class_id']) if sample else None\n    \n    train_dataset = CarDataset(\n                               image_ids=df_train.index.values,\n                               df=train,\n                               transforms=get_train_transforms())\n\n    valid_dataset = CarDataset(\n                               image_ids=df_valid.index.values,\n                               df=train,\n                               transforms=get_valid_transforms())\n    \n    train_data_loader = DataLoader(\n                                   train_dataset,\n                                   batch_size=BATCH_SIZE,\n                                   shuffle=False,\n                                   num_workers=4,\n                                   sampler=sampler,\n                                   collate_fn=train_dataset.collate_fn)\n\n    valid_data_loader = DataLoader(\n                                   valid_dataset,\n                                   batch_size=BATCH_SIZE,\n                                   shuffle=False,\n                                   num_workers=4,\n                                   collate_fn=valid_dataset.collate_fn)\n    \n    model = DETRModel(num_classes=num_classes, num_queries=num_queries).to(device)\n    criterion = SetCriterion(num_classes-1, matcher, weight_dict, eos_coef=1/num_classes, losses=losses).to(device)    \n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n    scheduler=torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n    \n    best_loss = 10**5\n    for epoch in range(EPOCHS):\n        train_loss = train_fn(train_data_loader, model, criterion, optimizer,scheduler=scheduler, epoch=epoch)\n        valid_loss = eval_fn(valid_data_loader, model, criterion)\n        \n        print('|EPOCH {}| TRAIN_LOSS {}| VALID_LOSS {}|'.format(epoch+1, train_loss.avg, valid_loss.avg))\n        if valid_loss.avg < best_loss:\n            best_loss = valid_loss.avg\n            print('Best model found for Fold {} in Epoch {}........Saving Model'.format(fold, epoch+1))\n            torch.save(model.state_dict(), f'detr_best_{fold}.pth')","metadata":{"execution":{"iopub.status.busy":"2022-03-31T19:29:07.682464Z","iopub.execute_input":"2022-03-31T19:29:07.683013Z","iopub.status.idle":"2022-03-31T19:29:07.695173Z","shell.execute_reply.started":"2022-03-31T19:29:07.682972Z","shell.execute_reply":"2022-03-31T19:29:07.694398Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# run\nfor fold in range(5):\n    run(fold, sample=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T21:56:10.106591Z","iopub.execute_input":"2022-03-31T21:56:10.107337Z","iopub.status.idle":"2022-03-31T23:46:27.692301Z","shell.execute_reply.started":"2022-03-31T21:56:10.107285Z","shell.execute_reply":"2022-03-31T23:46:27.690776Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"* fine-tune learning rate, number of queries, augmentations, losses weights\n* try only binary labels with eos_coef=0.5: solve seaprate detection tasks: 1 model to detect cars, 1 model to detect buses, etc. this would be more accurate, but will increase common model size significantly","metadata":{}},{"cell_type":"code","source":"def view_sample(test, model, device, threshold=0.7):\n\n    test_dataset = CarDataset(image_ids=test.frame.values,\n                              df=test,\n                              transforms=get_valid_transforms())\n     \n    test_data_loader = DataLoader(test_dataset,\n                                  batch_size=BATCH_SIZE,\n                                  shuffle=True,\n                                  num_workers=4,\n                                  collate_fn=test_dataset.collate_fn)\n    \n    images, targets, image_ids = next(iter(test_data_loader))\n    _,h,w = images[0].shape\n    \n    images = list(img.to(device) for img in images)\n    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n    \n    boxes = targets[0]['boxes'].cpu().numpy()\n    boxes = [np.array(box).astype(np.int32) for box in A.augmentations.bbox_utils.denormalize_bboxes(boxes,h,w)]\n    sample = images[0].permute(1,2,0).cpu().numpy()\n    \n    model.eval()\n    model.to(device)\n    cpu_device = torch.device(\"cpu\")\n    \n    with torch.no_grad():\n        outputs = model(images)\n        \n    outputs = [{k: v.to(cpu_device) for k, v in outputs.items()}]\n    \n    plt.figure(figsize=(16,8))\n    ax = plt.gca()\n\n    for box in boxes:\n        ax.add_patch(plt.Rectangle((box[0], box[1]), box[2], box[3], fill=False, color='red', linewidth=2))\n        \n    probs = outputs[0]['pred_logits'].softmax(-1).detach().cpu().numpy()[0, :, :-1] # discard background class\n    keep = probs.max(-1) > threshold\n    probs = probs[keep]\n\n    oboxes = outputs[0]['pred_boxes'].detach().cpu().numpy()[0, keep]\n    oboxes = [np.array(box).astype(np.int32) for box in A.augmentations.bbox_utils.denormalize_bboxes(oboxes,h,w)]\n    labels = outputs[0]['pred_logits'][...,:-1].max(-1)[1].cpu().numpy()[0, keep]\n\n    for box, prob, label in zip(oboxes, probs, labels):\n        ax.add_patch(plt.Rectangle((box[0], box[1]), box[2], box[3], fill=False, color='blue', linewidth=2))\n        text = f'Class_id: {ids_to_labels.get(label+1)}'\n        ax.text(box[0], box[1], text, fontsize=10, bbox=dict(facecolor='yellow', alpha=0.5))\n\n    ax.set_axis_off()\n    ax.imshow(sample)\n    return outputs","metadata":{"execution":{"iopub.status.busy":"2022-03-31T23:49:26.249911Z","iopub.execute_input":"2022-03-31T23:49:26.250620Z","iopub.status.idle":"2022-03-31T23:49:26.270264Z","shell.execute_reply.started":"2022-03-31T23:49:26.250580Z","shell.execute_reply":"2022-03-31T23:49:26.269486Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"model = DETRModel(num_classes=num_classes,num_queries=num_queries)\nmodel.load_state_dict(torch.load(\"./detr_best_2.pth\"))","metadata":{"execution":{"iopub.status.busy":"2022-03-31T23:46:50.847067Z","iopub.execute_input":"2022-03-31T23:46:50.847402Z","iopub.status.idle":"2022-03-31T23:46:52.075376Z","shell.execute_reply.started":"2022-03-31T23:46:50.847360Z","shell.execute_reply":"2022-03-31T23:46:52.074724Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"view = view_sample(test, model, device, threshold=0.7)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T23:49:36.287832Z","iopub.execute_input":"2022-03-31T23:49:36.288389Z","iopub.status.idle":"2022-03-31T23:49:38.408943Z","shell.execute_reply.started":"2022-03-31T23:49:36.288347Z","shell.execute_reply":"2022-03-31T23:49:38.408155Z"},"trusted":true},"execution_count":73,"outputs":[]}]}