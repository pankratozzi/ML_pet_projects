{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone -q https://github.com/facebookresearch/detr.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-01T21:30:24.440227Z","iopub.execute_input":"2022-04-01T21:30:24.440908Z","iopub.status.idle":"2022-04-01T21:30:26.705565Z","shell.execute_reply.started":"2022-04-01T21:30:24.440820Z","shell.execute_reply":"2022-04-01T21:30:26.704648Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"[https://www.kaggle.com/code/tanulsingh077/end-to-end-object-detection-with-transformers-detr/notebook](http://)","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport numpy as np \nimport pandas as pd \nfrom datetime import datetime\nimport time\nimport random\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.model_selection import StratifiedKFold\n\nimport cv2\n\nimport sys\nsys.path.append('./detr/')\n\nfrom detr.models.matcher import HungarianMatcher\nfrom detr.models.detr import SetCriterion\n\nimport albumentations as A # advanced augmentation framework with PyTorch interface\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nfrom glob import glob","metadata":{"execution":{"iopub.status.busy":"2022-04-01T21:30:28.228061Z","iopub.execute_input":"2022-04-01T21:30:28.228671Z","iopub.status.idle":"2022-04-01T21:30:31.835629Z","shell.execute_reply.started":"2022-04-01T21:30:28.228631Z","shell.execute_reply":"2022-04-01T21:30:31.834812Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Currently using \"{device}\" device')","metadata":{"execution":{"iopub.status.busy":"2022-04-01T21:30:33.195519Z","iopub.execute_input":"2022-04-01T21:30:33.196118Z","iopub.status.idle":"2022-04-01T21:30:33.200655Z","shell.execute_reply.started":"2022-04-01T21:30:33.196080Z","shell.execute_reply":"2022-04-01T21:30:33.199923Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2022-04-01T21:30:36.843650Z","iopub.execute_input":"2022-04-01T21:30:36.844327Z","iopub.status.idle":"2022-04-01T21:30:36.850993Z","shell.execute_reply.started":"2022-04-01T21:30:36.844276Z","shell.execute_reply":"2022-04-01T21:30:36.850213Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# load validation dataframe just to inspect some images, bboxes and classes\ndf = pd.read_csv('../input/self-driving-cars/labels_val.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T09:02:24.815397Z","iopub.execute_input":"2022-04-01T09:02:24.815799Z","iopub.status.idle":"2022-04-01T09:02:24.896831Z","shell.execute_reply.started":"2022-04-01T09:02:24.815743Z","shell.execute_reply":"2022-04-01T09:02:24.896153Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"n_folds = 5\nseed = 42\nnum_classes = 6 # 5 unique classes + background class\nnum_queries = 50 # max number of objects to detect per one image, default in detr = 100, strictly recommended by \n# developers to change this parameter only when training from scratch\nnull_class_coef = 0.5  # used as default in original repository. Set 0.5 if detecting 2 classes: object and background\nBATCH_SIZE = 16\nIMAGE_SIZE = 224 # 512\nLR = 1e-3 # 2e-5 \nEPOCHS = 4","metadata":{"execution":{"iopub.status.busy":"2022-04-01T21:35:25.588470Z","iopub.execute_input":"2022-04-01T21:35:25.589138Z","iopub.status.idle":"2022-04-01T21:35:25.594192Z","shell.execute_reply.started":"2022-04-01T21:35:25.589096Z","shell.execute_reply":"2022-04-01T21:35:25.593382Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"labels_to_ids = {'car': 1, 'truck': 2, 'pedestrian': 3, 'bicyclist': 4, 'light': 5}\nids_to_labels = {1: 'car', 2: 'truck', 3: 'pedestrian', 4: 'bicyclist', 5: 'light'}","metadata":{"execution":{"iopub.status.busy":"2022-04-01T21:35:28.125760Z","iopub.execute_input":"2022-04-01T21:35:28.126581Z","iopub.status.idle":"2022-04-01T21:35:28.131333Z","shell.execute_reply.started":"2022-04-01T21:35:28.126527Z","shell.execute_reply":"2022-04-01T21:35:28.130637Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything(seed)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T21:35:30.990199Z","iopub.execute_input":"2022-04-01T21:35:30.990994Z","iopub.status.idle":"2022-04-01T21:35:30.997059Z","shell.execute_reply.started":"2022-04-01T21:35:30.990951Z","shell.execute_reply":"2022-04-01T21:35:30.996341Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# base bbox format\nrandom_image = df['frame'].sample(1).iloc[0]\ndf_random = df[df['frame'] == random_image]\nsample_image = cv2.imread('../input/self-driving-cars/images/' + random_image, cv2.IMREAD_COLOR)\nsample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(8,8))\nplt.imshow(sample_image)\nax = plt.gca()\n\nfor idx, row in df_random.iterrows():\n    xmin, xmax, ymin, ymax = row[['xmin', 'xmax', 'ymin', 'ymax']]\n    ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n                                fill=False, color='red', linewidth=3))\n    text = f'Class_id: {row[\"class_id\"]}'\n    ax.text(xmin, ymin, text, fontsize=15,\n            bbox=dict(facecolor='yellow', alpha=0.5))\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T19:15:24.738626Z","iopub.execute_input":"2022-03-31T19:15:24.739323Z","iopub.status.idle":"2022-03-31T19:15:25.023568Z","shell.execute_reply.started":"2022-03-31T19:15:24.739281Z","shell.execute_reply":"2022-03-31T19:15:25.022873Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def xyxy_to_xywh(xyxy):\n    \"\"\"Convert [x1 y1 x2 y2] box format to [x1 y1 w h] format.\"\"\"\n    if isinstance(xyxy, (list, tuple)):\n        # Single box given as a list of coordinates\n        assert len(xyxy) == 4\n        x1, y1 = xyxy[0], xyxy[1]\n        w = xyxy[2] - x1 + 1\n        h = xyxy[3] - y1 + 1\n        return (x1, y1, w, h)\n    elif isinstance(xyxy, np.ndarray):\n        # Multiple boxes given as a 2D ndarray\n        return np.hstack((xyxy[:, 0:2], xyxy[:, 2:4] - xyxy[:, 0:2] + 1))\n    else:\n        raise TypeError('Argument xyxy must be a list, tuple, or numpy array.')\n        \ndef xyxy_to_xcycwh(xyxy):\n    if isinstance(xyxy, (list, tuple)):\n        assert len(xyxy) == 4\n        x1, y1 = (xyxy[0] + xyxy[2]) / 2, (xyxy[1] + xyxy[3]) / 2\n        w, h = xyxy[2] - xyxy[0], xyxy[3] - xyxy[1]\n        return (x1, y1, w, h)\n    elif isinstance(xyxy, np.ndarray):\n        return np.hstack(((xyxy[:, 0:1] + xyxy[:, 2:3]) / 2, (xyxy[:, 1:2] + xyxy[:, -1:]) / 2, xyxy[:, 2:3] - xyxy[:, 0:1], xyxy[:, -1:] - xyxy[:, 1:2]))\n    else:\n        raise TypeError('Argument xyxy must be a list, tuple, or numpy array.')","metadata":{"execution":{"iopub.status.busy":"2022-04-01T21:35:35.628301Z","iopub.execute_input":"2022-04-01T21:35:35.628867Z","iopub.status.idle":"2022-04-01T21:35:35.640980Z","shell.execute_reply.started":"2022-04-01T21:35:35.628828Z","shell.execute_reply":"2022-04-01T21:35:35.640293Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/self-driving-cars/labels_train.csv')\ntest = pd.read_csv('../input/self-driving-cars/labels_val.csv')\n\noutliers = train[train['ymax'] == 0].index # outliers\ntrain.drop(outliers, inplace=True)\n\n#train['h'] = train['ymax'] - train['ymin'] + 1\n#train['w'] = train['xmax'] - train['xmin'] + 1\n#train[['xc', 'yc', 'w', 'h']] = xyxy_to_xcycwh(train[['xmin', 'ymin', 'xmax', 'ymax']].values)\n\n#test['h'] = test['ymax'] - test['ymin'] + 1\n#test['w'] = test['xmax'] - test['xmin'] + 1\n#test[['xc', 'yc', 'w', 'h']] = xyxy_to_xcycwh(test[['xmin', 'ymin', 'xmax', 'ymax']].values)\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T21:35:44.773443Z","iopub.execute_input":"2022-04-01T21:35:44.773708Z","iopub.status.idle":"2022-04-01T21:35:44.994925Z","shell.execute_reply.started":"2022-04-01T21:35:44.773680Z","shell.execute_reply":"2022-04-01T21:35:44.994158Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"In coco, a bounding box is defined by four values in pixels [x_min, y_min, width, height]. They are coordinates of the top-left corner along with the width and height of the bounding box. Yolo format [x_center, y_center, width, height]","metadata":{}},{"cell_type":"code","source":"# coco bbox format\nrandom_image = train['frame'].sample(1).iloc[0]\ndf_random = train[train['frame'] == random_image]\nsample_image = cv2.imread('../input/self-driving-cars/images/' + random_image, cv2.IMREAD_COLOR)\nsample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(8,8))\nplt.imshow(sample_image)\nax = plt.gca()\n\nfor idx, row in df_random.iterrows():\n    x, y, w, h = row[['x', 'y', 'w', 'h']]\n    ax.add_patch(plt.Rectangle((x, y), w, h,\n                                fill=False, color='red', linewidth=3))\n    text = f'Class_id: {row[\"class_id\"]}'\n    ax.text(x, y, text, fontsize=15,\n            bbox=dict(facecolor='yellow', alpha=0.5))\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T06:42:03.491641Z","iopub.execute_input":"2022-03-31T06:42:03.492392Z","iopub.status.idle":"2022-03-31T06:42:03.865341Z","shell.execute_reply.started":"2022-03-31T06:42:03.492357Z","shell.execute_reply":"2022-03-31T06:42:03.864276Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# get n_folded dataframe, stratified by number of bboxes, truing to preserve target-value counts\nskf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n\ndf_folds = train[['frame']].copy()\ndf_folds.loc[:, 'bbox_count'] = 1\ndf_folds = df_folds.groupby('frame').count()\ndf_folds.loc[:, 'class_id'] = train[['frame', 'class_id']].groupby('frame').max()['class_id']  # min\ndf_folds.loc[:, 'stratify_group'] = np.char.add(\n    df_folds['class_id'].values.astype(str),\n    df_folds['bbox_count'].apply(lambda x: f'_{x // ((num_classes-1)*2 + 1)}').values.astype(str)\n)\ndf_folds.loc[:, 'fold'] = 0\n\nfor fold_number, (train_index, val_index) in enumerate(skf.split(X=df_folds.index, y=df_folds['stratify_group'])):\n    df_folds.loc[df_folds.iloc[val_index].index, 'fold'] = fold_number","metadata":{"execution":{"iopub.status.busy":"2022-04-01T21:35:50.906153Z","iopub.execute_input":"2022-04-01T21:35:50.907047Z","iopub.status.idle":"2022-04-01T21:35:51.160195Z","shell.execute_reply.started":"2022-04-01T21:35:50.907009Z","shell.execute_reply":"2022-04-01T21:35:51.159254Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# check class balances in each fold\nfor i in  range(5):\n    print(df_folds[df_folds['fold'] != i].class_id.value_counts(normalize=False))\n    print(df_folds[df_folds['fold'] != i].index.nunique())\n    print(df_folds[df_folds['fold'] == i].index.nunique())","metadata":{"execution":{"iopub.status.busy":"2022-04-01T19:14:36.035837Z","iopub.execute_input":"2022-04-01T19:14:36.036085Z","iopub.status.idle":"2022-04-01T19:14:36.082852Z","shell.execute_reply.started":"2022-04-01T19:14:36.036059Z","shell.execute_reply":"2022-04-01T19:14:36.082162Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train['class_id'].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T06:54:44.403395Z","iopub.execute_input":"2022-03-31T06:54:44.403867Z","iopub.status.idle":"2022-03-31T06:54:44.412828Z","shell.execute_reply.started":"2022-03-31T06:54:44.403828Z","shell.execute_reply":"2022-03-31T06:54:44.411952Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# albumentations transforms for PyTorch\ndef get_train_transforms():\n    return A.Compose([A.OneOf([A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5), # 0.9      \n                      A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.9)],p=0.5), # 0.9\n                      A.ToGray(p=0.01),\n                      A.HorizontalFlip(p=0.1), # 0.5\n                      A.VerticalFlip(p=0.1), # 0.5\n                      A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE, p=1),\n                      A.Cutout(num_holes=4, max_h_size=32, max_w_size=32, fill_value=0, p=0.1), # 8, 64, 64, 0.5\n                      A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=1.0),\n                      ToTensorV2(p=1.0),\n                      ],p=1.0,\n                      bbox_params=A.BboxParams(format='yolo', min_area=0, min_visibility=0, label_fields=['labels']), # coco\n                      )\n\ndef get_valid_transforms():\n    return A.Compose([A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE, p=1.0),\n                      A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=1.0),\n                      ToTensorV2(p=1.0),\n                     ], \n                      p=1.0, \n                      bbox_params=A.BboxParams(format='yolo', min_area=0, min_visibility=0, label_fields=['labels']), # coco\n                      )","metadata":{"execution":{"iopub.status.busy":"2022-04-01T22:07:11.260046Z","iopub.execute_input":"2022-04-01T22:07:11.260338Z","iopub.status.idle":"2022-04-01T22:07:11.270743Z","shell.execute_reply.started":"2022-04-01T22:07:11.260305Z","shell.execute_reply":"2022-04-01T22:07:11.269965Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"As classes are extremely imbalanced, it is good practice to sample the data in dataloader. Below is an example \nof WeightedRandomSampler, that shows a way of dealing with imbalanced classes using weighted oversampling technique.\n```\n# alt.: gives weights > 1\nclass_counts = y_train.value_counts().to_list()\nnum_samples = sum(class_counts)\nlabels = y_train.map(labels_to_int).values\n\nclass_weights = [num_samples/class_counts[i] for i in range(len(class_counts))]\nweights = [class_weights[labels[i]] for i in range(int(num_samples))]\nsampler = torch.utils.data.WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))\n```","metadata":{}},{"cell_type":"code","source":"# example of sampler, which we can add when defining train dataloader\ntarget_labels = df_folds.loc[df_folds['fold'] == 0, 'class_id']\n\ndef get_sampler(target_labels):\n    class_sample_count = np.unique(target_labels, return_counts=True)[1]\n    weight = 1./class_sample_count\n    samples_weight = weight[target_labels.values-1]\n    samples_weight = torch.from_numpy(samples_weight)\n    sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight))\n    return sampler\n\nsampler = get_sampler(target_labels)  # pay attention at the background class","metadata":{"execution":{"iopub.status.busy":"2022-04-01T21:36:01.287648Z","iopub.execute_input":"2022-04-01T21:36:01.287899Z","iopub.status.idle":"2022-04-01T21:36:01.301272Z","shell.execute_reply.started":"2022-04-01T21:36:01.287872Z","shell.execute_reply":"2022-04-01T21:36:01.300489Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_path = '../input/self-driving-cars/images'\n\nclass CarDataset(Dataset):\n    \"\"\" Define custom dataset class that returns an image tensor with corresponded target and image name\"\"\"\n    def __init__(self, image_ids, df, transforms=None):\n        self.image_ids = image_ids\n        self.df = df\n        self.transforms = transforms\n        \n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n    \n    def __getitem__(self, index):\n        image_id = self.image_ids[index]\n        records = self.df[self.df['frame'] == image_id]\n        \n        image = cv2.imread(f'{train_path}/{image_id}', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        h,w,_ = image.shape\n        image /= 255.0\n        \n        boxes = records[['xmin', 'ymin', 'xmax', 'ymax']].values\n        boxes = A.augmentations.bbox_utils.normalize_bboxes(boxes, h, w)\n        boxes = np.array([xyxy_to_xcycwh(box) for box in boxes]) # yolo\n        \n        area = boxes[:,2] * boxes[:,3]\n        area = torch.as_tensor(area, dtype=torch.float32)\n        \n        # set all labels to 0 if our task is only to detect every object on image without label\n        #labels =  np.zeros(len(boxes), dtype=np.int32)\n        labels = records['class_id'].values.astype(np.int32) - 1\n\n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': boxes,\n                'labels': labels\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            boxes = sample['bboxes']\n            labels = sample['labels']\n                                \n        target = {}\n        target['boxes'] = torch.as_tensor(boxes,dtype=torch.float32)\n        target['labels'] = torch.as_tensor(labels,dtype=torch.long)\n        target['image_id'] = torch.tensor([index])\n        target['area'] = area\n        \n        return image, target, image_id\n    \n    def collate_fn(self, batch):\n        return tuple(zip(*batch))","metadata":{"execution":{"iopub.status.busy":"2022-04-01T22:51:20.363352Z","iopub.execute_input":"2022-04-01T22:51:20.363626Z","iopub.status.idle":"2022-04-01T22:51:20.380978Z","shell.execute_reply.started":"2022-04-01T22:51:20.363593Z","shell.execute_reply":"2022-04-01T22:51:20.380311Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"markdown","source":"#### Define DETR model taken from facebook research","metadata":{}},{"cell_type":"code","source":"from detr.models.detr import MLP\n\n# continue training model weights\nclass DETRModel(nn.Module):\n    def __init__(self, num_classes, num_queries):\n        super(DETRModel,self).__init__()\n        self.num_classes = num_classes\n        self.num_queries = num_queries\n        \n        self.model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)\n        self.in_features = self.model.class_embed.in_features\n        \n        #for param in self.model.parameters():#\n        #    param.requires_grad = False#\n        \n        self.model.class_embed = nn.Linear(in_features=self.in_features, out_features=self.num_classes)\n        self.model.num_queries = self.num_queries\n        \n        #self.model.query_embed = nn.Embedding(self.num_queries, 256)\n        #self.model.bbox_embed = MLP(256, 256, 4, 3)\n        \n    def forward(self,images):\n        return self.model(images)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T19:16:29.722887Z","iopub.execute_input":"2022-04-01T19:16:29.723577Z","iopub.status.idle":"2022-04-01T19:16:29.731327Z","shell.execute_reply.started":"2022-04-01T19:16:29.723522Z","shell.execute_reply":"2022-04-01T19:16:29.730434Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# second way\nfrom detr.models.detr import MLP\n\nclass DETRModel(nn.Module):\n    def __init__(self, num_classes, num_queries):\n        super(DETRModel,self).__init__()\n        self.num_classes = num_classes\n        self.num_queries = num_queries\n        \n        self.model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=False, num_classes=50)\n        checkpoint = torch.hub.load_state_dict_from_url(\n                                    url='https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth',\n                                    map_location=device,\n                                    check_hash=True)\n        del checkpoint[\"model\"][\"class_embed.weight\"]\n        del checkpoint[\"model\"][\"class_embed.bias\"]\n        self.model.load_state_dict(checkpoint[\"model\"], strict=False)\n        for param in self.model.parameters():\n            param.requires_grad = False\n        \n        self.in_features = self.model.class_embed.in_features\n        self.model.class_embed = nn.Linear(in_features=self.in_features, out_features=self.num_classes)\n        self.model.bbox_embed = MLP(256, 256, 4, 3) # multilayer perceptron\n        \n    def forward(self,images):\n        return self.model(images)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T22:51:25.751113Z","iopub.execute_input":"2022-04-01T22:51:25.751793Z","iopub.status.idle":"2022-04-01T22:51:25.762034Z","shell.execute_reply.started":"2022-04-01T22:51:25.751755Z","shell.execute_reply":"2022-04-01T22:51:25.761332Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"# cross entropy loss for classification, bbox-loss for regression, IoU loss for background\nmatcher = HungarianMatcher()\n\nweight_dict = weight_dict = {'loss_ce': 1, 'loss_bbox': 1 , 'loss_giou': 1}\n\nlosses = ['labels', 'boxes', 'cardinality']","metadata":{"execution":{"iopub.status.busy":"2022-04-01T21:36:23.319749Z","iopub.execute_input":"2022-04-01T21:36:23.320003Z","iopub.status.idle":"2022-04-01T21:36:23.324148Z","shell.execute_reply.started":"2022-04-01T21:36:23.319974Z","shell.execute_reply":"2022-04-01T21:36:23.323438Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"#### Train and eval functions. We train criterion also.\nMAP was commented due to issue in importing torchmetrics in kaggle","metadata":{}},{"cell_type":"code","source":"def train_fn(dataloader, model, criterion, optimizer, scheduler, epoch):\n    model.train()\n    criterion.train()\n    \n    summary_loss = AverageMeter()\n    \n    tk0 = tqdm(dataloader, total=len(dataloader), leave=True)\n    \n    for step, (images, targets, image_ids) in enumerate(tk0):\n        \n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        \n\n        output = model(images)\n\n        loss_dict = criterion(output, targets)\n        weight_dict = criterion.weight_dict\n        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n        \n        optimizer.zero_grad()\n\n        losses.backward()\n        optimizer.step()\n        if scheduler is not None:\n            scheduler.step()    \n        \n        summary_loss.update(losses.item(), BATCH_SIZE)\n        tk0.set_postfix(loss=summary_loss.avg) # print out average losses after each epoch\n    \n    return summary_loss","metadata":{"execution":{"iopub.status.busy":"2022-04-01T21:36:25.417284Z","iopub.execute_input":"2022-04-01T21:36:25.417982Z","iopub.status.idle":"2022-04-01T21:36:25.426729Z","shell.execute_reply.started":"2022-04-01T21:36:25.417940Z","shell.execute_reply":"2022-04-01T21:36:25.425694Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef eval_fn(dataloader, model, criterion):\n    model.eval()\n    criterion.eval()\n    summary_loss = AverageMeter()\n            \n    tk0 = tqdm(dataloader, total=len(dataloader), leave=True)\n    for step, (images, targets, image_ids) in enumerate(tk0):\n            \n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        output = model(images)\n\n        loss_dict = criterion(output, targets)\n        weight_dict = criterion.weight_dict\n        \n        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n        summary_loss.update(losses.item(),BATCH_SIZE)\n        tk0.set_postfix(loss=summary_loss.avg)\n        \n    return summary_loss","metadata":{"execution":{"iopub.status.busy":"2022-04-01T21:36:28.166609Z","iopub.execute_input":"2022-04-01T21:36:28.167360Z","iopub.status.idle":"2022-04-01T21:36:28.176912Z","shell.execute_reply.started":"2022-04-01T21:36:28.167309Z","shell.execute_reply":"2022-04-01T21:36:28.176008Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### Run learning process on n_folds","metadata":{}},{"cell_type":"code","source":"def run(fold, sample=False):\n    \n    df_train = df_folds[df_folds['fold'] != fold]\n    df_valid = df_folds[df_folds['fold'] == fold]\n    \n    sampler = get_sampler(df_train['class_id']) if sample else None\n    \n    train_dataset = CarDataset(\n                               image_ids=df_train.index.values,\n                               df=train,\n                               transforms=get_train_transforms())\n\n    valid_dataset = CarDataset(\n                               image_ids=df_valid.index.values,\n                               df=train,\n                               transforms=get_valid_transforms())\n    \n    train_data_loader = DataLoader(\n                                   train_dataset,\n                                   batch_size=BATCH_SIZE,\n                                   shuffle=False,\n                                   num_workers=4,\n                                   sampler=sampler,\n                                   collate_fn=train_dataset.collate_fn)\n\n    valid_data_loader = DataLoader(\n                                   valid_dataset,\n                                   batch_size=BATCH_SIZE,\n                                   shuffle=False,\n                                   num_workers=4,\n                                   collate_fn=valid_dataset.collate_fn)\n    \n    model = DETRModel(num_classes=num_classes, num_queries=num_queries).to(device)\n    criterion = SetCriterion(num_classes-1, matcher, weight_dict, eos_coef=1/num_classes, losses=losses).to(device)    \n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n    scheduler=torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n    \n    best_loss = 10**5\n    for epoch in range(EPOCHS):\n        train_loss = train_fn(train_data_loader, model, criterion, optimizer,scheduler=scheduler, epoch=epoch)\n        valid_loss = eval_fn(valid_data_loader, model, criterion)\n        \n        print('|EPOCH {}| TRAIN_LOSS {}| VALID_LOSS {}|'.format(epoch+1, train_loss.avg, valid_loss.avg))\n        if valid_loss.avg < best_loss:\n            best_loss = valid_loss.avg\n            print('Best model found for Fold {} in Epoch {}........Saving Model'.format(fold, epoch+1))\n            torch.save(model.state_dict(), f'detr_best_{fold}.pth')","metadata":{"execution":{"iopub.status.busy":"2022-04-01T21:56:02.820602Z","iopub.execute_input":"2022-04-01T21:56:02.820867Z","iopub.status.idle":"2022-04-01T21:56:02.832216Z","shell.execute_reply.started":"2022-04-01T21:56:02.820838Z","shell.execute_reply":"2022-04-01T21:56:02.831525Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# run\nfor fold in range(5):\n    run(fold, sample=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T22:57:16.259375Z","iopub.execute_input":"2022-04-01T22:57:16.260345Z","iopub.status.idle":"2022-04-01T22:57:23.340705Z","shell.execute_reply.started":"2022-04-01T22:57:16.260292Z","shell.execute_reply":"2022-04-01T22:57:23.339329Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"markdown","source":"* fine-tune learning rate, number of queries, augmentations, losses weights\n* try only binary labels with eos_coef=0.5: solve seaprate detection tasks: 1 model to detect cars, 1 model to detect buses, etc. this would be more accurate, but will increase common model size significantly","metadata":{}},{"cell_type":"code","source":"def _concat(x, y):\n    \"\"\" Concat by the last dimension \"\"\"\n    if isinstance(x, np.ndarray):\n        return np.concatenate((x, y), axis=-1)\n    elif isinstance(x, torch.Tensor):\n        return torch.cat([x, y], dim=-1)\n    else:\n        raise TypeError(\"unknown type '{}'\".format(type(x)))\n\n\ndef xcycwh_to_xywh(xcycwh):\n    \"\"\"Convert [x_c y_c w h] box format to [x1, y1, w, h] format.\"\"\"\n    if isinstance(xcycwh, (list, tuple)):\n        # Single box given as a list of coordinates\n        assert not isinstance(xcycwh[0], (list, tuple))\n        xc, yc = xcycwh[0], xcycwh[1]\n        w = xcycwh[2]\n        h = xcycwh[3]\n        x1 = xc - w / 2.\n        y1 = yc - h / 2.\n        return [x1, y1, w, h]\n    elif isinstance(xcycwh, (np.ndarray, torch.Tensor)):\n        wh = xcycwh[..., 2:4]\n        x1y1 = xcycwh[..., 0:2] - wh / 2.\n        return _concat(x1y1, wh)\n    else:\n        raise TypeError('Argument xcycwh must be a list, tuple, or numpy array.')\n\ndef xcycwh_to_xyxy(xcycwh):\n    \"\"\"Convert [x_c y_c w h] box format to [x1, y1, x2, y2] format.\"\"\"\n    if isinstance(xcycwh, (list, tuple)):\n        # Single box given as a list of coordinates\n        assert not isinstance(xcycwh[0], (list, tuple))\n        xc, yc = xcycwh[0], xcycwh[1]\n        w = xcycwh[2]\n        h = xcycwh[3]\n        x1 = xc - w / 2.\n        y1 = yc - h / 2.\n        x2 = xc + w / 2.\n        y2 = yc + h / 2.\n        return [x1, y1, x2, y2]\n    elif isinstance(xcycwh, (np.ndarray, torch.Tensor)):\n        wh = xcycwh[..., 2:4]\n        x1y1 = xcycwh[..., 0:2] - wh / 2.\n        x2y2 = xcycwh[..., 0:2] + wh / 2.\n        return _concat(x1y1, x2y2)\n    else:\n        raise TypeError('Argument xcycwh must be a list, tuple, or numpy array.')\n        \ndef xywh_to_xyxy(xywh):\n    \"\"\"Convert [x1 y1 w h] box format to [x1 y1 x2 y2] format.\"\"\"\n    if isinstance(xywh, (list, tuple)):\n        # Single box given as a list of coordinates\n        assert len(xywh) == 4\n        x1, y1 = xywh[0], xywh[1]\n        x2 = x1 + np.maximum(0., xywh[2] - 1.)\n        y2 = y1 + np.maximum(0., xywh[3] - 1.)\n        return (x1, y1, x2, y2)\n    elif isinstance(xywh, np.ndarray):\n        # Multiple boxes given as a 2D ndarray\n        return np.hstack(\n            (xywh[:, 0:2], xywh[:, 0:2] + np.maximum(0, xywh[:, 2:4] - 1))\n        )\n    else:\n        raise TypeError('Argument xywh must be a list, tuple, or numpy array.')","metadata":{"execution":{"iopub.status.busy":"2022-04-01T21:49:24.562069Z","iopub.execute_input":"2022-04-01T21:49:24.562338Z","iopub.status.idle":"2022-04-01T21:49:24.579299Z","shell.execute_reply.started":"2022-04-01T21:49:24.562309Z","shell.execute_reply":"2022-04-01T21:49:24.578338Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def view_sample(test, model, device, threshold=0.7):\n\n    test_dataset = CarDataset(image_ids=test.frame.values,\n                              df=test,\n                              transforms=get_valid_transforms())\n    \n    test_data_loader = DataLoader(test_dataset,\n                                  batch_size=BATCH_SIZE,\n                                  shuffle=True,\n                                  num_workers=4,\n                                  collate_fn=test_dataset.collate_fn)\n    \n    images, targets, image_ids = next(iter(test_data_loader))\n    \n    img_to_show = cv2.imread(train_path + '/' + image_ids[0], cv2.IMREAD_COLOR)\n    img_to_show = cv2.cvtColor(img_to_show, cv2.COLOR_BGR2RGB)\n    h,w,_ = img_to_show.shape\n    \n    images = list(img.to(device) for img in images)\n    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n    \n    boxes = targets[0]['boxes'].cpu().numpy()\n    boxes = xcycwh_to_xyxy(boxes)\n    boxes = [np.array(box).astype(np.int32) for box in A.augmentations.bbox_utils.denormalize_bboxes(boxes,h,w)]\n    \n    model.eval()\n    model.to(device)\n    cpu_device = torch.device(\"cpu\")\n    \n    with torch.no_grad():\n        outputs = model(images)\n        \n    outputs = [{k: v.to(cpu_device) for k, v in outputs.items()}]\n    \n    plt.figure(figsize=(16,8))\n    ax = plt.gca()\n\n    for box in boxes:\n        ax.add_patch(plt.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1], fill=False, color='red', linewidth=2))\n        \n    probs = outputs[0]['pred_logits'].softmax(-1).detach().cpu().numpy()[0, :, :-1] # discard background class\n    keep = probs.max(-1) > threshold\n    probs = probs[keep]\n\n    oboxes = outputs[0]['pred_boxes'].detach().cpu().numpy()[0, keep]\n    oboxes = xcycwh_to_xyxy(oboxes)\n    oboxes = [np.array(box).astype(np.int32) for box in A.augmentations.bbox_utils.denormalize_bboxes(oboxes,h,w)]\n\n    labels = outputs[0]['pred_logits'][...,:-1].max(-1)[1].cpu().numpy()[0, keep]\n\n    for box, prob, label in zip(oboxes, probs, labels):\n        ax.add_patch(plt.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1], fill=False, color='blue', linewidth=2))\n        text = f'Class_id: {ids_to_labels.get(label+1)}'\n        ax.text(box[0], box[1], text, fontsize=10, bbox=dict(facecolor='yellow', alpha=0.5))\n\n    ax.set_axis_off()\n    ax.imshow(img_to_show)\n    return outputs","metadata":{"execution":{"iopub.status.busy":"2022-04-01T22:18:56.444082Z","iopub.execute_input":"2022-04-01T22:18:56.444357Z","iopub.status.idle":"2022-04-01T22:18:56.460986Z","shell.execute_reply.started":"2022-04-01T22:18:56.444323Z","shell.execute_reply":"2022-04-01T22:18:56.460332Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"model = DETRModel(num_classes=num_classes,num_queries=num_queries)\nmodel.load_state_dict(torch.load(\"./detr_best_0.pth\"))","metadata":{"execution":{"iopub.status.busy":"2022-04-01T22:43:52.199329Z","iopub.execute_input":"2022-04-01T22:43:52.199675Z","iopub.status.idle":"2022-04-01T22:43:53.376106Z","shell.execute_reply.started":"2022-04-01T22:43:52.199636Z","shell.execute_reply":"2022-04-01T22:43:53.375462Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"view = view_sample(test, model, device, threshold=0.3)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T22:44:07.255784Z","iopub.execute_input":"2022-04-01T22:44:07.256556Z","iopub.status.idle":"2022-04-01T22:44:09.528915Z","shell.execute_reply.started":"2022-04-01T22:44:07.256491Z","shell.execute_reply":"2022-04-01T22:44:09.528137Z"},"trusted":true},"execution_count":74,"outputs":[]}]}