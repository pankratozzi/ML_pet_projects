{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Xs30K1omgO0"
      },
      "outputs": [],
      "source": [
        "!wget https://www.dropbox.com/s/l2ul3upj7dkv4ou/synthetic-data.zip\n",
        "!unzip -qq synthetic-data.zip\n",
        "!pip install torch_snippets torch_summary editdistance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "484OB8NSO4iH"
      },
      "outputs": [],
      "source": [
        "from torch_snippets import *\n",
        "from torchsummary import summary\n",
        "import editdistance\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtTal5QvPKbw",
        "outputId": "75831141-4afb-4720-f625-539c6db20070"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-10-07 19:03:34.014 | INFO     | torch_snippets.paths:inner:24 - 25132 files found at synthetic-data\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "fname2label = lambda name: str(name).split('@')[0].split('/')[1]\n",
        "images = Glob('synthetic-data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjOerHQFPUS2"
      },
      "outputs": [],
      "source": [
        "vocab = 'QWERTYUIOPASDFGHJKLZXCVBNMqwertyuiopasdfghjklzxcvbnm'\n",
        "BATCH, TIMESTEP, VOCAB = 64, 32, len(vocab)\n",
        "H, W = 32, 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRri9h7BQcwX"
      },
      "outputs": [],
      "source": [
        "class OCRDataset(Dataset):\n",
        "\n",
        "    def __init__(self, items, vocab=vocab, preprocess_shape=(H,W), timesteps=TIMESTEP):\n",
        "        super().__init__()\n",
        "        self.items = items\n",
        "        self.charList = {ix+1:ch for ix,ch in enumerate(vocab)}\n",
        "        self.charList.update({0: '`'})\n",
        "        self.invCharList = {v:k for k,v in self.charList.items()}\n",
        "        self.ts = timesteps\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def sample(self):\n",
        "        return self[randint(len(self))]\n",
        "\n",
        "    def __getitem__(self, ix):\n",
        "        item = self.items[ix]\n",
        "        image = cv2.imread(str(item), 0)\n",
        "        label = fname2label(item)\n",
        "        return image, label\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        images, labels, label_lengths, label_vectors, input_lengths = [], [], [], [], []\n",
        "        for image, label in batch:\n",
        "            images.append(torch.Tensor(self.preprocess(image))[None,None])\n",
        "            label_lengths.append(len(label))\n",
        "            labels.append(label)\n",
        "            label_vectors.append(self.str2vec(label))\n",
        "            input_lengths.append(self.ts)\n",
        "        images = torch.cat(images).float().to(device)\n",
        "        label_lengths = torch.Tensor(label_lengths).long().to(device)\n",
        "        label_vectors = torch.Tensor(label_vectors).long().to(device)\n",
        "        input_lengths = torch.Tensor(input_lengths).long().to(device)\n",
        "        return images, label_vectors, label_lengths, input_lengths, labels\n",
        "\n",
        "    def str2vec(self, string, pad=True):\n",
        "        string = ''.join([s for s in string if s in self.invCharList])\n",
        "        val = list(map(lambda x: self.invCharList[x], string)) \n",
        "        if pad:\n",
        "            while len(val) < self.ts:\n",
        "                val.append(0)\n",
        "        return val\n",
        "    \n",
        "    def preprocess(self, img, shape=(32,128)):\n",
        "        target = np.ones(shape)*255\n",
        "        try:\n",
        "            H, W = shape\n",
        "            h, w = img.shape\n",
        "            fx = H/h\n",
        "            fy = W/w\n",
        "            f = min(fx, fy)\n",
        "            _h = int(h*f)\n",
        "            _w = int(w*f)\n",
        "            _img = cv2.resize(img, (_w,_h))\n",
        "            target[:_h,:_w] = _img\n",
        "        except:\n",
        "            pass\n",
        "        return (255-target)/255\n",
        "\n",
        "    def decoder_chars(self, pred):\n",
        "        decoded = \"\"\n",
        "        last = \"\"\n",
        "        pred = pred.cpu().detach().numpy()\n",
        "        for i in range(len(pred)):\n",
        "            k = np.argmax(pred[i])\n",
        "            if k > 0 and self.charList[k] != last:\n",
        "                last = self.charList[k]\n",
        "                decoded = decoded + last\n",
        "            elif k > 0 and self.charList[k] == last:\n",
        "                continue\n",
        "            else:\n",
        "                last = \"\"\n",
        "        return decoded.replace(\" \",\" \")\n",
        "\n",
        "    def wer(self, preds, labels):\n",
        "        c = 0\n",
        "        for p, l in zip(preds, labels):\n",
        "            c += p.lower().strip() != l.lower().strip()\n",
        "        return round(c/len(preds), 4)\n",
        "    \n",
        "    def cer(self, preds, labels):\n",
        "        c, d = [], []\n",
        "        for p, l in zip(preds, labels):\n",
        "            c.append(editdistance.eval(p, l) / len(l))\n",
        "        return round(np.mean(c), 4)\n",
        "\n",
        "    def evaluate(self, model, ims, labels, lower=False):\n",
        "        model.eval()\n",
        "        preds = model(ims).permute(1,0,2) \n",
        "        preds = [self.decoder_chars(pred) for pred in preds]\n",
        "        return {'char-error-rate': self.cer(preds, labels),\n",
        "                'word-error-rate': self.wer(preds, labels),\n",
        "                'char-accuracy' : 1 - self.cer(preds, labels),\n",
        "                'word-accuracy' : 1 - self.wer(preds, labels)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU92XmXzX7FU",
        "outputId": "fb55c54a-e1c1-4008-e41d-1c411e05f8eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-10-07 19:09:26.364 | INFO     | torch_snippets.paths:inner:24 - 25132 files found at synthetic-data\n"
          ]
        }
      ],
      "source": [
        "train_items, valid_items = train_test_split(Glob('synthetic-data'), test_size=0.2, random_state=1)\n",
        "train_dataset = OCRDataset(train_items)\n",
        "valid_dataset = OCRDataset(valid_items)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH, collate_fn=train_dataset.collate_fn, \n",
        "                              drop_last=True, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH, collate_fn=valid_dataset.collate_fn, \n",
        "                              drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "id": "UPDaUz65aa7I",
        "outputId": "f889880a-c77c-43fd-a407-2d4c44f3ca7d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 144x144 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "img, _ = valid_dataset[0]\n",
        "show(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnHsCy6Ga5T4"
      },
      "outputs": [],
      "source": [
        "from torch_snippets import Reshape, Permute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1TEzFk_a_sd"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, ni, no, ks=3, st=1, padding=1, pool=2, drop=0.2):\n",
        "        super().__init__()\n",
        "        self.ks = ks\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(ni, no, kernel_size=ks, stride=st, padding=padding),\n",
        "            nn.BatchNorm2d(no, momentum=0.3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(pool),\n",
        "            nn.Dropout2d(drop)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class Ocr(nn.Module):\n",
        "    def __init__(self, vocab):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            BasicBlock( 1, 128),\n",
        "            BasicBlock(128, 128),\n",
        "            BasicBlock(128, 256, pool=(4,2)),\n",
        "            Reshape(-1, 256, 32),\n",
        "            Permute(2, 0, 1)\n",
        "        )\n",
        "        self.rnn = nn.Sequential(\n",
        "            nn.LSTM(256, 256, num_layers=2, dropout=0.2, bidirectional=True),\n",
        "        )\n",
        "        self.classification = nn.Sequential(\n",
        "            nn.Linear(512, vocab+1),\n",
        "            nn.LogSoftmax(-1),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x, lstm_states = self.rnn(x)\n",
        "        y = self.classification(x)\n",
        "        return y\n",
        "\n",
        "def ctc(log_probs, target, input_lengths, target_lengths, blank=0):\n",
        "    loss = nn.CTCLoss(blank=blank, zero_infinity=True)\n",
        "    ctc_loss = loss(log_probs, target, input_lengths, target_lengths)\n",
        "    return ctc_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goDMfXL29yf9",
        "outputId": "4181f4d1-36be-45ce-d2a5-3efce2c1fda4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "├─Sequential: 1-1                        [-1, 1, 256]              --\n",
            "|    └─BasicBlock: 2-1                   [-1, 128, 16, 64]         --\n",
            "|    |    └─Sequential: 3-1              [-1, 128, 16, 64]         1,536\n",
            "|    └─BasicBlock: 2-2                   [-1, 128, 8, 32]          --\n",
            "|    |    └─Sequential: 3-2              [-1, 128, 8, 32]          147,840\n",
            "|    └─BasicBlock: 2-3                   [-1, 256, 2, 16]          --\n",
            "|    |    └─Sequential: 3-3              [-1, 256, 2, 16]          295,680\n",
            "|    └─Reshape: 2-4                      [-1, 256, 32]             --\n",
            "|    └─Permute: 2-5                      [-1, 1, 256]              --\n",
            "├─Sequential: 1-2                        [-1, 1, 512]              --\n",
            "|    └─LSTM: 2-6                         [-1, 1, 512]              2,629,632\n",
            "├─Sequential: 1-3                        [-1, 1, 53]               --\n",
            "|    └─Linear: 2-7                       [-1, 1, 53]               27,189\n",
            "|    └─LogSoftmax: 2-8                   [-1, 1, 53]               --\n",
            "==========================================================================================\n",
            "Total params: 3,101,877\n",
            "Trainable params: 3,101,877\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 237.84\n",
            "==========================================================================================\n",
            "Input size (MB): 0.02\n",
            "Forward/backward pass size (MB): 11.00\n",
            "Params size (MB): 11.83\n",
            "Estimated Total Size (MB): 22.85\n",
            "==========================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "├─Sequential: 1-1                        [-1, 1, 256]              --\n",
              "|    └─BasicBlock: 2-1                   [-1, 128, 16, 64]         --\n",
              "|    |    └─Sequential: 3-1              [-1, 128, 16, 64]         1,536\n",
              "|    └─BasicBlock: 2-2                   [-1, 128, 8, 32]          --\n",
              "|    |    └─Sequential: 3-2              [-1, 128, 8, 32]          147,840\n",
              "|    └─BasicBlock: 2-3                   [-1, 256, 2, 16]          --\n",
              "|    |    └─Sequential: 3-3              [-1, 256, 2, 16]          295,680\n",
              "|    └─Reshape: 2-4                      [-1, 256, 32]             --\n",
              "|    └─Permute: 2-5                      [-1, 1, 256]              --\n",
              "├─Sequential: 1-2                        [-1, 1, 512]              --\n",
              "|    └─LSTM: 2-6                         [-1, 1, 512]              2,629,632\n",
              "├─Sequential: 1-3                        [-1, 1, 53]               --\n",
              "|    └─Linear: 2-7                       [-1, 1, 53]               27,189\n",
              "|    └─LogSoftmax: 2-8                   [-1, 1, 53]               --\n",
              "==========================================================================================\n",
              "Total params: 3,101,877\n",
              "Trainable params: 3,101,877\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 237.84\n",
              "==========================================================================================\n",
              "Input size (MB): 0.02\n",
              "Forward/backward pass size (MB): 11.00\n",
              "Params size (MB): 11.83\n",
              "Estimated Total Size (MB): 22.85\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = Ocr(len(vocab)).to(device)\n",
        "summary(model, torch.zeros((1,1,32,128)).to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQ_T25lgDFZs"
      },
      "outputs": [],
      "source": [
        "def train_batch(data, model, optimizer, criterion):\n",
        "    model.train()\n",
        "    imgs, targets, label_lens, input_lens, labels = data\n",
        "    optimizer.zero_grad()\n",
        "    preds = model(imgs)\n",
        "    loss = criterion(preds, targets, input_lens, label_lens)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    results = train_dataset.evaluate(model, imgs.to(device), labels)\n",
        "    return loss, results\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate_batch(data, model):\n",
        "    model.eval()\n",
        "    imgs, targets, label_lens, input_lens, labels = data\n",
        "    preds = model(imgs)\n",
        "    loss = criterion(preds, targets, input_lens, label_lens)\n",
        "    return loss, valid_dataset.evaluate(model, imgs.to(device), labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ije7a3QGhkC"
      },
      "outputs": [],
      "source": [
        "criterion = ctc\n",
        "optimizer = optim.AdamW(model.parameters(), lr=3e-3)\n",
        "# scheduler?\n",
        "epochs = 50\n",
        "log = Report(epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Zi6alI3hG1OE",
        "outputId": "66d7d720-3d32-4cc6-da86-7c2e78f625f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 1.000\ttrain_loss: 3.334\ttrain_char_accuracy: 0.006\ttrain_word_accuracy: 0.000\tvalid_loss: 3.104\tvalid_char_accuracy: 0.069\tvalid_word_accuracy: 0.000\t(1911.74s - 93675.07s remaining)\n",
            "\n",
            "Prediction: \"a\", Ground Truth: \"wish\"\n",
            "Prediction: \"a\", Ground Truth: \"eat\"\n",
            "Prediction: \"a\", Ground Truth: \"idea\"\n",
            "Prediction: \"s\", Ground Truth: \"TV\"\n",
            "Prediction: \"t\", Ground Truth: \"during\"\n",
            "\n",
            "EPOCH: 2.000\ttrain_loss: 2.872\ttrain_char_accuracy: 0.156\ttrain_word_accuracy: 0.001\tvalid_loss: 2.581\tvalid_char_accuracy: 0.251\tvalid_word_accuracy: 0.002\t(3376.28s - 81030.82s remaining)\n",
            "\n",
            "Prediction: \"ce\", Ground Truth: \"agent\"\n",
            "Prediction: \"tee\", Ground Truth: \"drop\"\n",
            "Prediction: \"tine\", Ground Truth: \"time\"\n",
            "Prediction: \"mete\", Ground Truth: \"reality\"\n",
            "Prediction: \"rete\", Ground Truth: \"return\"\n",
            "\n",
            "EPOCH: 2.522\ttrain_loss: 2.312\ttrain_char_accuracy: 0.369\ttrain_word_accuracy: 0.016\t(4055.13s - 76330.58s remaining)"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    N = len(train_dataloader)\n",
        "    for ix, data in enumerate(train_dataloader):\n",
        "        pos = epoch + (ix + 1) / N\n",
        "        loss, results = train_batch(data, model, optimizer, criterion)\n",
        "        ca, wa = results['char-accuracy'], results['word-accuracy']\n",
        "        log.record(pos, train_loss=loss, train_char_accuracy=ca, train_word_accuracy=wa, end='\\r')\n",
        "    val_results = []\n",
        "    N = len(valid_dataloader)\n",
        "    for ix, data in enumerate(valid_dataloader):\n",
        "        pos = epoch + (ix + 1) / N\n",
        "        loss, results = validate_batch(data, model)\n",
        "        ca, wa = results['char-accuracy'], results['word-accuracy']\n",
        "        log.record(pos, valid_loss=loss, valid_char_accuracy=ca, valid_word_accuracy=wa, end='\\r')\n",
        "    log.report_avgs(epoch + 1)\n",
        "    print()\n",
        "    for i in range(5):\n",
        "        img, label = valid_dataset.sample()\n",
        "        _img = torch.Tensor(valid_dataset.preprocess(img)[None, None]).to(device)\n",
        "        pred = model(_img)[:, 0, :]\n",
        "        pred = train_dataset.decoder_chars(pred)\n",
        "        print(f'Prediction: \"{pred}\", Ground Truth: \"{label}\"')\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "pytorch_book_handwritten_transcription.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}