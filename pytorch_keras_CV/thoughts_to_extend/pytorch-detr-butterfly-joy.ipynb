{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone -qq https://github.com/facebookresearch/detr.git","metadata":{"execution":{"iopub.status.busy":"2022-07-04T12:36:12.566826Z","iopub.execute_input":"2022-07-04T12:36:12.567161Z","iopub.status.idle":"2022-07-04T12:36:15.220747Z","shell.execute_reply.started":"2022-07-04T12:36:12.567079Z","shell.execute_reply":"2022-07-04T12:36:15.219747Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport json\nimport numpy as np \nimport pandas as pd \nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms as T\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\n\nimport cv2\n\nimport sys\nsys.path.append('./detr/')\n\nfrom detr.models.matcher import HungarianMatcher\nfrom detr.models.detr import SetCriterion\n\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nfrom glob import glob\nfrom pprint import pprint","metadata":{"execution":{"iopub.status.busy":"2022-07-04T12:36:19.488735Z","iopub.execute_input":"2022-07-04T12:36:19.489498Z","iopub.status.idle":"2022-07-04T12:36:23.317332Z","shell.execute_reply.started":"2022-07-04T12:36:19.489456Z","shell.execute_reply":"2022-07-04T12:36:23.316579Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Currently using \"{device}\" device')","metadata":{"execution":{"iopub.status.busy":"2022-07-04T12:36:26.570791Z","iopub.execute_input":"2022-07-04T12:36:26.571086Z","iopub.status.idle":"2022-07-04T12:36:26.634007Z","shell.execute_reply.started":"2022-07-04T12:36:26.571054Z","shell.execute_reply":"2022-07-04T12:36:26.633151Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n        \ninvTrans = T.Compose([T.Normalize(mean = [ 0., 0., 0. ],\n                                  std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n                      T.Normalize(mean = [ -0.485, -0.456, -0.406 ],\n                                  std = [ 1., 1., 1. ]),\n                      ])        \n\ndef _concat(x, y):\n    \"\"\" Concat by the last dimension \"\"\"\n    if isinstance(x, np.ndarray):\n        return np.concatenate((x, y), axis=-1)\n    elif isinstance(x, torch.Tensor):\n        return torch.cat([x, y], dim=-1)\n    else:\n        raise TypeError(\"unknown type '{}'\".format(type(x)))\n\ndef xcycwh_to_xywh(xcycwh):\n    \"\"\"Convert [x_c y_c w h] box format to [x1, y1, w, h] format.\"\"\"\n    if isinstance(xcycwh, (list, tuple)):\n        # Single box given as a list of coordinates\n        assert not isinstance(xcycwh[0], (list, tuple))\n        xc, yc = xcycwh[0], xcycwh[1]\n        w = xcycwh[2]\n        h = xcycwh[3]\n        x1 = xc - w / 2.\n        y1 = yc - h / 2.\n        return [x1, y1, w, h]\n    elif isinstance(xcycwh, (np.ndarray, torch.Tensor)):\n        wh = xcycwh[..., 2:4]\n        x1y1 = xcycwh[..., 0:2] - wh / 2.\n        return _concat(x1y1, wh)\n    else:\n        raise TypeError('Argument xcycwh must be a list, tuple, or numpy array.')\n        \ndef xywh_to_xyxy(xywh):\n    \"\"\"Convert [x1 y1 w h] box format to [x1 y1 x2 y2] format.\"\"\"\n    if isinstance(xywh, (list, tuple)):\n        # Single box given as a list of coordinates\n        assert len(xywh) == 4\n        x1, y1 = xywh[0], xywh[1]\n        x2 = x1 + np.maximum(0., xywh[2] - 1.)\n        y2 = y1 + np.maximum(0., xywh[3] - 1.)\n        return (x1, y1, x2, y2)\n    elif isinstance(xywh, np.ndarray):\n        # Multiple boxes given as a 2D ndarray\n        return np.hstack(\n            (xywh[:, 0:2], xywh[:, 0:2] + np.maximum(0, xywh[:, 2:4] - 1))\n        )\n    else:\n        raise TypeError('Argument xywh must be a list, tuple, or numpy array.')\n\ndef xyxy_to_xcycwh(xyxy):\n    \"\"\"Convert [x1 y1 x2, y2] box format to [x_c y_c w h] format.\"\"\"\n    if isinstance(xyxy, (list, tuple)):\n        # Single box given as a list of coordinates\n        assert not isinstance(xyxy[0], (list, tuple))\n        x1, y1 = xyxy[0], xyxy[1]\n        w = xyxy[2] - x1\n        h = xyxy[3] - y1\n        x = (xyxy[0] + xyxy[2]) / 2.\n        y = (xyxy[1] + xyxy[3]) / 2.\n        return [x, y, w, h]\n    elif isinstance(xyxy, (np.ndarray, torch.Tensor)):\n        wh = xyxy[..., 2:4] - xyxy[..., 0:2]\n        xy = (xyxy[..., 0:2] + xyxy[..., 2:4]) / 2.\n        return _concat(xy, wh)\n    else:\n        raise TypeError('Argument xyxy must be a list, tuple, or numpy array.')\n\ndef xywh_to_xcycwh(xywh):\n    \"\"\"Convert [x1 y1 w h] box format to [x_c y_c w h] format.\"\"\"\n    if isinstance(xywh, (list, tuple)):\n        # Single box given as a list of coordinates\n        assert not isinstance(xywh[0], (list, tuple))\n        x1, y1 = xywh[0], xywh[1]\n        xc = x1 + np.maximum(0., xywh[2] / 2.)\n        yc = y1 + np.maximum(0., xywh[3] )\n        return [xc, yc, xywh[2], xywh[3]]\n\n    elif isinstance(xywh, (np.ndarray, torch.Tensor)):\n        wh = xywh[..., 2:4]\n        xcyc = xywh[..., 0:2] + wh / 2\n        return _concat(xcyc, wh)\n    else:\n        raise TypeError('Argument xyxy must be a list, tuple, numpy array, or tensor.')\n        \ndef xyxy_to_xywh(xyxy):\n    \"\"\"Convert [x1 y1 x2 y2] box format to [x1 y1 w h] format.\"\"\"\n    if isinstance(xyxy, (list, tuple)):\n        # Single box given as a list of coordinates\n        assert len(xyxy) == 4\n        x1, y1 = xyxy[0], xyxy[1]\n        w = xyxy[2] - x1 + 1\n        h = xyxy[3] - y1 + 1\n        return (x1, y1, w, h)\n    elif isinstance(xyxy, np.ndarray):\n        # Multiple boxes given as a 2D ndarray\n        return np.hstack((xyxy[:, 0:2], xyxy[:, 2:4] - xyxy[:, 0:2] + 1))\n    else:\n        raise TypeError('Argument xyxy must be a list, tuple, or numpy array.')","metadata":{"execution":{"iopub.status.busy":"2022-07-04T12:36:31.040875Z","iopub.execute_input":"2022-07-04T12:36:31.041348Z","iopub.status.idle":"2022-07-04T12:36:31.070751Z","shell.execute_reply.started":"2022-07-04T12:36:31.041307Z","shell.execute_reply":"2022-07-04T12:36:31.069888Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"n_folds = 5\nseed = 42\nnum_classes = 8 # 7 unique classes + background class\nnum_queries = 20 \nnull_class_coef = 1/num_classes\nBATCH_SIZE = 4\nIMAGE_SIZE = 512\nLR = 1e-4 # 2e-5 \nEPOCHS = 8","metadata":{"execution":{"iopub.status.busy":"2022-07-04T12:36:35.559445Z","iopub.execute_input":"2022-07-04T12:36:35.559737Z","iopub.status.idle":"2022-07-04T12:36:35.565576Z","shell.execute_reply.started":"2022-07-04T12:36:35.559706Z","shell.execute_reply":"2022-07-04T12:36:35.564703Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything(seed)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T12:36:39.172631Z","iopub.execute_input":"2022-07-04T12:36:39.172894Z","iopub.status.idle":"2022-07-04T12:36:39.181687Z","shell.execute_reply.started":"2022-07-04T12:36:39.172865Z","shell.execute_reply":"2022-07-04T12:36:39.180882Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"paths = r'../input/arthropod-taxonomy-orders-object-detection-dataset/ArTaxOr/'\nannots = glob(paths + '/*/annotations/*.json')\nannots = [str(annot) for annot in annots]","metadata":{"execution":{"iopub.status.busy":"2022-07-04T12:36:43.494185Z","iopub.execute_input":"2022-07-04T12:36:43.494934Z","iopub.status.idle":"2022-07-04T12:36:45.701751Z","shell.execute_reply.started":"2022-07-04T12:36:43.494887Z","shell.execute_reply":"2022-07-04T12:36:45.700959Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def create_dataframe(annots: list = annots) -> pd.DataFrame:\n    array = np.zeros((1, 7), dtype=np.object)\n    for i in tqdm(range(len(annots)), total=len(annots)):\n        with open(annots[i], 'rb') as infile:\n            data = json.load(infile)\n            image_path = paths + data['asset'].get('path').split('ArTaxOr')[-1][1:]\n            for item in data['regions']:\n                row = np.zeros((1,7), dtype=np.object)\n                row[0,0] = image_path\n                row[0,1] = data['asset'].get('name')\n                row[0,2] = item['tags'][0]\n                row[0,3] = item['boundingBox'].get('height')\n                row[0,4] = item['boundingBox'].get('width')\n                row[0,5] = item['boundingBox'].get('left')\n                row[0,6] = item['boundingBox'].get('top')\n                array = np.concatenate([array, row], axis=0)\n    df = pd.DataFrame(data=array[1:, :], columns=['path', 'image_id', 'label', 'height', 'width', 'left', 'top'])\n    df[['height', 'width', 'left', 'top']] = df[['height', 'width', 'left', 'top']].astype(np.float32)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-07-04T12:36:47.063084Z","iopub.execute_input":"2022-07-04T12:36:47.063644Z","iopub.status.idle":"2022-07-04T12:36:47.078320Z","shell.execute_reply.started":"2022-07-04T12:36:47.063601Z","shell.execute_reply":"2022-07-04T12:36:47.077084Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df = create_dataframe()\ndf[['xmin', 'ymin', 'xmax', 'ymax']] = xywh_to_xyxy(df[['left', 'top', 'width', 'height']].values)\nenc = LabelEncoder()\ndf['class_id'] = enc.fit_transform(df['label'])","metadata":{"execution":{"iopub.status.busy":"2022-07-04T12:36:51.604374Z","iopub.execute_input":"2022-07-04T12:36:51.604642Z","iopub.status.idle":"2022-07-04T12:38:43.013731Z","shell.execute_reply.started":"2022-07-04T12:36:51.604613Z","shell.execute_reply":"2022-07-04T12:38:43.012980Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# coco bbox format\nrandom_path = df['path'].sample(1).iloc[0]\ndf_random = df[df['path'] == random_path]\nprint(random_path)\nsample_image = cv2.imread(random_path, cv2.IMREAD_COLOR)\nsample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(8,8))\nplt.imshow(sample_image)\nax = plt.gca()\n\nfor idx, row in df_random.iterrows():\n    x, y, w, h = row[['left', 'top', 'width', 'height']]\n    ax.add_patch(plt.Rectangle((x, y), w, h,\n                                fill=False, color='red', linewidth=3))\n    text = f'Class_id: {row[\"label\"]}'\n    ax.text(x, y, text, fontsize=15,\n            bbox=dict(facecolor='yellow', alpha=0.5))\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-03T14:17:41.885515Z","iopub.execute_input":"2022-07-03T14:17:41.885787Z","iopub.status.idle":"2022-07-03T14:17:43.358057Z","shell.execute_reply.started":"2022-07-03T14:17:41.885753Z","shell.execute_reply":"2022-07-03T14:17:43.356283Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# get n_folded dataframe, stratified by number of bboxes, trying to preserve target-value counts\nskf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n\ndf_folds = df[['image_id']].copy()\ndf_folds.loc[:, 'bbox_count'] = 1\ndf_folds = df_folds.groupby('image_id').count()\ndf_folds.loc[:, 'class_id'] = df[['image_id', 'class_id']].groupby('image_id').min()['class_id']\ndf_folds.loc[:, 'stratify_group'] = np.char.add(\n    df_folds['class_id'].values.astype(str),\n    df_folds['bbox_count'].apply(lambda x: f'_{x // ((num_classes-1)*2 + 1)}').values.astype(str)\n)\ndf_folds.loc[:, 'fold'] = 0\n\nfor fold_number, (train_index, val_index) in enumerate(skf.split(X=df_folds.index, y=df_folds['stratify_group'])):\n    df_folds.loc[df_folds.iloc[val_index].index, 'fold'] = fold_number","metadata":{"execution":{"iopub.status.busy":"2022-07-04T12:38:43.015299Z","iopub.execute_input":"2022-07-04T12:38:43.015633Z","iopub.status.idle":"2022-07-04T12:38:43.169607Z","shell.execute_reply.started":"2022-07-04T12:38:43.015595Z","shell.execute_reply":"2022-07-04T12:38:43.168835Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def get_train_transforms():\n    return A.Compose([A.OneOf([A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),      \n                      A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.9)],p=0.5),\n                      A.ToGray(p=0.01),\n                      A.HorizontalFlip(p=0.1),\n                      A.VerticalFlip(p=0.1),\n                      A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE, p=1),\n                      A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.2),\n                      A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=1.0),\n                      ToTensorV2(p=1.0),\n                      ],p=1.0,\n                      bbox_params=A.BboxParams(format='yolo', min_area=0, min_visibility=0, label_fields=['labels']),\n                      )\n\ndef get_valid_transforms():\n    return A.Compose([A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE, p=1.0),\n                      A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=1.0),\n                      ToTensorV2(p=1.0),\n                     ], \n                      p=1.0, \n                      bbox_params=A.BboxParams(format='yolo', min_area=0, min_visibility=0, label_fields=['labels']),\n                      )","metadata":{"execution":{"iopub.status.busy":"2022-07-04T12:38:43.170793Z","iopub.execute_input":"2022-07-04T12:38:43.172212Z","iopub.status.idle":"2022-07-04T12:38:43.184408Z","shell.execute_reply.started":"2022-07-04T12:38:43.172167Z","shell.execute_reply":"2022-07-04T12:38:43.183369Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class InsectDataset(Dataset):\n    \"\"\" Define custom dataset class that returns an image tensor with corresponded target and image name\"\"\"\n    def __init__(self, image_ids, df, transforms=None):\n        self.image_ids = image_ids\n        self.df = df\n        self.transforms = transforms\n        \n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n    \n    def __getitem__(self, index):\n        image_id = self.image_ids[index]\n        records = self.df[self.df['image_id'] == image_id]\n\n        image = cv2.imread(records['path'].iloc[0], cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        h,w,_ = image.shape\n        image /= 255.0\n        \n        boxes = records[['xmin', 'ymin', 'xmax', 'ymax']].values\n        boxes[:, [0,2]] *= (IMAGE_SIZE / w)\n        boxes[:, [1,3]] *= (IMAGE_SIZE / h)\n        boxes = A.augmentations.bbox_utils.normalize_bboxes(boxes, IMAGE_SIZE, IMAGE_SIZE)\n        boxes = np.array([xyxy_to_xcycwh(box) for box in boxes]) # yolo\n\n        area = boxes[:,2] * boxes[:,3]\n        area = torch.as_tensor(area, dtype=torch.float32)\n        \n        labels = records['class_id'].values.astype(np.int32)\n\n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': boxes,\n                'labels': labels\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            boxes = sample['bboxes']\n            labels = sample['labels']\n                                \n        target = {}\n        target['boxes'] = torch.as_tensor(boxes,dtype=torch.float32)\n        target['labels'] = torch.as_tensor(labels,dtype=torch.long)\n        target['image_id'] = torch.tensor([index])\n        target['area'] = area\n        \n        return image, target, image_id\n    \n    def collate_fn(self, batch):\n        return tuple(zip(*batch))","metadata":{"execution":{"iopub.status.busy":"2022-07-04T12:38:43.186885Z","iopub.execute_input":"2022-07-04T12:38:43.187281Z","iopub.status.idle":"2022-07-04T12:38:43.202710Z","shell.execute_reply.started":"2022-07-04T12:38:43.187237Z","shell.execute_reply":"2022-07-04T12:38:43.201949Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"#### Example","metadata":{}},{"cell_type":"code","source":"ds = InsectDataset(df_folds[df_folds['fold'] != 0].index.values, df, get_train_transforms())","metadata":{"execution":{"iopub.status.busy":"2022-07-03T14:17:44.873608Z","iopub.execute_input":"2022-07-03T14:17:44.873847Z","iopub.status.idle":"2022-07-03T14:17:44.880416Z","shell.execute_reply.started":"2022-07-03T14:17:44.873814Z","shell.execute_reply":"2022-07-03T14:17:44.879763Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"sample = ds[509]\nimg = invTrans(sample[0]).permute(1,2,0).cpu().numpy()\nboxes = sample[1]['boxes'].cpu().numpy()\nh,w,_ = img.shape\nboxes = xcycwh_to_xywh(boxes)\nboxes = [np.array(box).astype(np.int32) for box in A.augmentations.bbox_utils.denormalize_bboxes(boxes,h,w)]\n\n\nprint(boxes)\nplt.imshow(img)\nax = plt.gca()\n\nfor box in boxes:\n    x, y, w, h = box\n    ax.add_patch(plt.Rectangle((x, y), w, h, fill=False, color='red', linewidth=3))\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-03T14:17:44.881467Z","iopub.execute_input":"2022-07-03T14:17:44.881960Z","iopub.status.idle":"2022-07-03T14:17:45.198020Z","shell.execute_reply.started":"2022-07-03T14:17:44.881922Z","shell.execute_reply":"2022-07-03T14:17:45.196607Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"#### continue ","metadata":{}},{"cell_type":"code","source":"from detr.models.detr import MLP\n\nclass DETRModel(nn.Module):\n    def __init__(self, num_classes, num_queries):\n        super(DETRModel,self).__init__()\n        self.num_classes = num_classes\n        self.num_queries = num_queries\n        \n        self.model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)\n        self.in_features = self.model.class_embed.in_features\n        \n        for param in self.model.parameters():\n            param.requires_grad = False\n        \n        self.model.class_embed = nn.Linear(in_features=self.in_features, out_features=self.num_classes)\n        #self.model.num_queries = self.num_queries\n        \n        #self.model.query_embed = nn.Embedding(self.num_queries, 256)\n        self.model.bbox_embed = MLP(256, 256, 4, 3)\n        \n    def forward(self,images):\n        return self.model(images)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T12:38:43.204228Z","iopub.execute_input":"2022-07-04T12:38:43.204500Z","iopub.status.idle":"2022-07-04T12:38:43.216757Z","shell.execute_reply.started":"2022-07-04T12:38:43.204464Z","shell.execute_reply":"2022-07-04T12:38:43.215883Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# second way\n\nclass DETRModel(nn.Module):\n    def __init__(self, num_classes, num_queries):\n        super(DETRModel,self).__init__()\n        self.num_classes = num_classes\n        self.num_queries = num_queries\n        \n        self.model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=False, num_classes=50)\n        checkpoint = torch.hub.load_state_dict_from_url(\n                                    url='https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth',\n                                    map_location=device,\n                                    check_hash=True)\n        del checkpoint[\"model\"][\"class_embed.weight\"]\n        del checkpoint[\"model\"][\"class_embed.bias\"]\n        self.model.load_state_dict(checkpoint[\"model\"], strict=False)\n        for param in self.model.parameters():\n            param.requires_grad = False\n        \n        self.in_features = self.model.class_embed.in_features\n        self.model.class_embed = nn.Linear(in_features=self.in_features, out_features=self.num_classes)\n        self.model.bbox_embed = MLP(256, 256, 4, 3) # multilayer perceptron\n        \n    def forward(self,images):\n        return self.model(images)","metadata":{"execution":{"iopub.status.busy":"2022-07-03T16:46:47.226766Z","iopub.execute_input":"2022-07-03T16:46:47.227070Z","iopub.status.idle":"2022-07-03T16:46:47.239107Z","shell.execute_reply.started":"2022-07-03T16:46:47.227037Z","shell.execute_reply":"2022-07-03T16:46:47.236333Z"},"trusted":true},"execution_count":196,"outputs":[]},{"cell_type":"code","source":"matcher = HungarianMatcher()\n\nweight_dict = {'loss_ce': 1, 'loss_bbox': 1 , 'loss_giou': 1}\n\nlosses = ['labels', 'boxes', 'cardinality']","metadata":{"execution":{"iopub.status.busy":"2022-07-04T12:38:43.218325Z","iopub.execute_input":"2022-07-04T12:38:43.218635Z","iopub.status.idle":"2022-07-04T12:38:43.229599Z","shell.execute_reply.started":"2022-07-04T12:38:43.218599Z","shell.execute_reply":"2022-07-04T12:38:43.228818Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def train_fn(dataloader, model, criterion, optimizer, scheduler, epoch):\n    model.train()\n    criterion.train()\n    \n    summary_loss = AverageMeter()\n    \n    tk0 = tqdm(dataloader, total=len(dataloader), leave=True)\n    \n    for step, (images, targets, image_ids) in enumerate(tk0):\n        \n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        \n\n        output = model(images)\n\n        loss_dict = criterion(output, targets)\n        weight_dict = criterion.weight_dict\n        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n        \n        optimizer.zero_grad()\n\n        losses.backward()\n        optimizer.step()   \n        \n        summary_loss.update(losses.item(), BATCH_SIZE)\n        tk0.set_postfix(loss=summary_loss.avg) # print out average losses after each epoch\n    \n    if scheduler is not None:\n        scheduler.step() \n            \n    return summary_loss","metadata":{"execution":{"iopub.status.busy":"2022-07-04T12:38:43.230887Z","iopub.execute_input":"2022-07-04T12:38:43.231302Z","iopub.status.idle":"2022-07-04T12:38:43.243727Z","shell.execute_reply.started":"2022-07-04T12:38:43.231267Z","shell.execute_reply":"2022-07-04T12:38:43.243045Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef eval_fn(dataloader, model, criterion):\n    model.eval()\n    criterion.eval()\n    summary_loss = AverageMeter()\n            \n    tk0 = tqdm(dataloader, total=len(dataloader), leave=True)\n    for step, (images, targets, image_ids) in enumerate(tk0):\n            \n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        output = model(images)\n\n        loss_dict = criterion(output, targets)\n        weight_dict = criterion.weight_dict\n        \n        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n        summary_loss.update(losses.item(),BATCH_SIZE)\n        tk0.set_postfix(loss=summary_loss.avg)\n        \n    return summary_loss","metadata":{"execution":{"iopub.status.busy":"2022-07-04T12:38:43.245643Z","iopub.execute_input":"2022-07-04T12:38:43.246275Z","iopub.status.idle":"2022-07-04T12:38:43.254796Z","shell.execute_reply.started":"2022-07-04T12:38:43.246238Z","shell.execute_reply":"2022-07-04T12:38:43.254079Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def run(fold):\n    \n    df_train = df_folds[df_folds['fold'] != fold]\n    df_valid = df_folds[df_folds['fold'] == fold]\n        \n    train_dataset = InsectDataset(\n                                  image_ids=df_train.index.values,\n                                  df=df,\n                                  transforms=get_train_transforms())\n\n    valid_dataset = InsectDataset(\n                                  image_ids=df_valid.index.values,\n                                  df=df,\n                                  transforms=get_valid_transforms())\n    \n    train_data_loader = DataLoader(\n                                   train_dataset,\n                                   batch_size=BATCH_SIZE,\n                                   shuffle=False,\n                                   num_workers=4,\n                                   collate_fn=train_dataset.collate_fn)\n\n    valid_data_loader = DataLoader(\n                                   valid_dataset,\n                                   batch_size=BATCH_SIZE,\n                                   shuffle=False,\n                                   num_workers=4,\n                                   collate_fn=valid_dataset.collate_fn)\n    \n    model = DETRModel(num_classes=num_classes, num_queries=num_queries).to(device)\n    # model.load_state_dict(torch.load(\"../input/detr-weights/detr_best_0.pth\"))\n    criterion = SetCriterion(num_classes-1, matcher, weight_dict, eos_coef=1/num_classes, losses=losses).to(device)    \n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n    scheduler=torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5, verbose=True)\n    \n    best_loss = 10**5\n    for epoch in range(EPOCHS):\n        train_loss = train_fn(train_data_loader, model, criterion, optimizer,scheduler=scheduler, epoch=epoch)\n        valid_loss = eval_fn(valid_data_loader, model, criterion)\n        \n        print('|EPOCH {}| TRAIN_LOSS {}| VALID_LOSS {}|'.format(epoch+1, train_loss.avg, valid_loss.avg))\n        if valid_loss.avg < best_loss:\n            best_loss = valid_loss.avg\n            print('Best model found for Fold {} in Epoch {}........Saving Model'.format(fold, epoch+1))\n            torch.save(model.state_dict(), f'detr_best_{fold}.pth')","metadata":{"execution":{"iopub.status.busy":"2022-07-04T12:39:26.184049Z","iopub.execute_input":"2022-07-04T12:39:26.184495Z","iopub.status.idle":"2022-07-04T12:39:26.195904Z","shell.execute_reply.started":"2022-07-04T12:39:26.184459Z","shell.execute_reply":"2022-07-04T12:39:26.194876Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"for fold in range(5):\n    run(fold)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T12:39:32.326726Z","iopub.execute_input":"2022-07-04T12:39:32.327356Z","iopub.status.idle":"2022-07-04T13:48:13.288217Z","shell.execute_reply.started":"2022-07-04T12:39:32.327306Z","shell.execute_reply":"2022-07-04T13:48:13.286706Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from torchvision.ops import nms\n\ndef view_sample(frame, model, device, threshold=0.7):\n\n    valid_dataset = InsectDataset(image_ids=frame.index.values,\n                                  df=df,\n                                  transforms=get_valid_transforms()\n                                  )\n     \n    valid_data_loader = DataLoader(\n                                    valid_dataset,\n                                    batch_size=BATCH_SIZE,\n                                    shuffle=False,\n                                    num_workers=4,\n                                    collate_fn=valid_dataset.collate_fn)\n    \n    images, targets, image_ids = next(iter(valid_data_loader))\n    _,h,w = images[0].shape\n    images = list(img.to(device) for img in images)\n    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n    \n    boxes = targets[0]['boxes'].cpu().numpy()\n    boxes = xcycwh_to_xywh(boxes)\n    boxes = [np.array(box).astype(np.int32) for box in A.augmentations.bbox_utils.denormalize_bboxes(boxes,h,w)]\n    sample = invTrans(images[0]).permute(1,2,0).cpu().numpy()\n    \n    model.eval()\n    model.to(device)\n    cpu_device = torch.device(\"cpu\")\n    \n    with torch.no_grad():\n        outputs = model(images)\n        \n    outputs = [{k: v.to(cpu_device) for k, v in outputs.items()}]\n    \n    plt.figure(figsize=(16,8))\n    ax = plt.gca()\n\n    for box in boxes:\n        ax.add_patch(plt.Rectangle((box[0], box[1]), box[2], box[3], fill=False, color='red', linewidth=2))\n        \n    probs = outputs[0]['pred_logits'].softmax(-1).detach().cpu().numpy()[0, :, :-1] \n    keep = probs.max(-1) > threshold\n    probs = probs[keep]\n    \n    oboxes = outputs[0]['pred_boxes'].detach().cpu().numpy()[0, keep]\n    oboxes = xcycwh_to_xywh(oboxes)\n    oboxes = [np.array(box).astype(np.int32) for box in A.augmentations.bbox_utils.denormalize_bboxes(oboxes,h,w)]\n\n    labels = outputs[0]['pred_logits'][...,:-1].max(-1)[1].cpu().numpy()[0, keep]\n    \n    for box, prob, label in zip(oboxes, probs, labels):\n        ax.add_patch(plt.Rectangle((box[0], box[1]), box[2], box[3], fill=False, color='blue', linewidth=2))\n        text = f'Class_id: {enc.inverse_transform([label])}'\n        ax.text(box[0], box[1], text, fontsize=10, bbox=dict(facecolor='yellow', alpha=0.5))\n    \n    ax.set_axis_off()\n    ax.imshow(sample)\n    \n    return outputs","metadata":{"execution":{"iopub.status.busy":"2022-07-04T13:48:19.803926Z","iopub.execute_input":"2022-07-04T13:48:19.804386Z","iopub.status.idle":"2022-07-04T13:48:19.822789Z","shell.execute_reply.started":"2022-07-04T13:48:19.804348Z","shell.execute_reply":"2022-07-04T13:48:19.821949Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"model = DETRModel(num_classes=num_classes,num_queries=num_queries)\nmodel.load_state_dict(torch.load(\"../input/detr-weights/detr_best_0.pth\"))","metadata":{"execution":{"iopub.status.busy":"2022-07-04T13:49:27.211632Z","iopub.execute_input":"2022-07-04T13:49:27.211919Z","iopub.status.idle":"2022-07-04T13:49:30.955719Z","shell.execute_reply.started":"2022-07-04T13:49:27.211888Z","shell.execute_reply":"2022-07-04T13:49:30.954976Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"for i in range(5):\n    view = view_sample(df_folds[df_folds['fold'] == i], model=model, device=device, threshold=0.06)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T13:53:40.447214Z","iopub.execute_input":"2022-07-04T13:53:40.448102Z","iopub.status.idle":"2022-07-04T13:53:56.803746Z","shell.execute_reply.started":"2022-07-04T13:53:40.448065Z","shell.execute_reply":"2022-07-04T13:53:56.802945Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"--------------","metadata":{}},{"cell_type":"code","source":"def view_sample(df_valid,model,device):\n    '''\n    Code taken from Peter's Kernel \n    https://www.kaggle.com/pestipeti/pytorch-starter-fasterrcnn-train\n    '''\n    valid_dataset = InsectDataset(image_ids=df_valid.index.values,\n                                 df=df,\n                                 transforms=get_valid_transforms()\n                                )\n     \n    valid_data_loader = DataLoader(\n                                    valid_dataset,\n                                    batch_size=BATCH_SIZE,\n                                    shuffle=False,\n                                   num_workers=4,\n                                   collate_fn=valid_dataset.collate_fn)\n    images, targets, image_ids = next(iter(valid_data_loader))\n    _,h,w = images[0].shape # for de normalizing images\n    \n    images = list(img.to(device) for img in images)\n    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n    \n    boxes = targets[0]['boxes'].cpu().numpy()\n    boxes = xcycwh_to_xywh(boxes)\n    boxes = [np.array(box).astype(np.int32) for box in A.augmentations.bbox_utils.denormalize_bboxes(boxes,h,w)]\n    sample = sample = invTrans(images[0]).permute(1,2,0).cpu().numpy()\n    \n    model.eval()\n    model.to(device)\n    cpu_device = torch.device(\"cpu\")\n    \n    with torch.no_grad():\n        outputs = model(images)\n    outputs = [{k: v.to(cpu_device) for k, v in outputs.items()}]\n    \n    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\n    for box in boxes:\n        cv2.rectangle(sample,\n                  (box[0], box[1]),\n                  (box[2]+box[0], box[3]+box[1]),\n                  (220, 0, 0), 1)\n        \n\n    oboxes = outputs[0]['pred_boxes'][0].detach().cpu().numpy()\n    oboxes = xcycwh_to_xywh(oboxes)\n    oboxes = [np.array(box).astype(np.int32) for box in A.augmentations.bbox_utils.denormalize_bboxes(oboxes,h,w)]\n    prob = outputs[0]['pred_logits'][0].softmax(1).detach().cpu().numpy()[:,0]\n    \n    indices = nms(torch.as_tensor(oboxes, dtype=torch.float32), torch.tensor(prob), 0.03)\n    oboxes, prob = [tensor[indices.numpy().astype('int')] for tensor in [np.array(oboxes), np.array(prob)]]\n\n    for box,p in zip(oboxes,prob):\n        if p >0.035:\n            color = (0,0,220)\n            cv2.rectangle(sample, (box[0], box[1]), (box[2]+box[0], box[3]+box[1]),color, 1)\n    \n    ax.set_axis_off()\n    ax.imshow(sample)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T13:54:23.519156Z","iopub.execute_input":"2022-07-04T13:54:23.519793Z","iopub.status.idle":"2022-07-04T13:54:23.537568Z","shell.execute_reply.started":"2022-07-04T13:54:23.519751Z","shell.execute_reply":"2022-07-04T13:54:23.536874Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"for i in range(5):\n    view_sample(df_folds[df_folds['fold'] == i], model=model, device=device)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T13:54:27.069797Z","iopub.execute_input":"2022-07-04T13:54:27.070560Z","iopub.status.idle":"2022-07-04T13:54:43.630751Z","shell.execute_reply.started":"2022-07-04T13:54:27.070517Z","shell.execute_reply":"2022-07-04T13:54:43.629964Z"},"trusted":true},"execution_count":28,"outputs":[]}]}