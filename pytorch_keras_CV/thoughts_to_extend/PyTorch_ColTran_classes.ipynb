{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nimport cv2\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom glob import glob\nfrom tqdm.autonotebook import tqdm, trange\nfrom skimage.color import rgb2lab, lab2rgb\nfrom PIL import Image\n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-29T22:50:14.088064Z","iopub.execute_input":"2022-07-29T22:50:14.088544Z","iopub.status.idle":"2022-07-29T22:50:18.996018Z","shell.execute_reply.started":"2022-07-29T22:50:14.088506Z","shell.execute_reply":"2022-07-29T22:50:18.994530Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"seed = 42\ntorch.random.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\nnp.random.seed(seed)\ngen = torch.Generator()\ngen.manual_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2022-07-29T22:50:18.998815Z","iopub.execute_input":"2022-07-29T22:50:19.000067Z","iopub.status.idle":"2022-07-29T22:50:19.022876Z","shell.execute_reply.started":"2022-07-29T22:50:19.000021Z","shell.execute_reply":"2022-07-29T22:50:19.021277Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 8\nIMAGE_HEIGHT = 256\nIMAGE_WIDTH = 256\nUP_SIZE = 512\nepochs = 20\n\nPATH = 'model.pth'\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Currently using \"{device.upper()}\" device.')","metadata":{"execution":{"iopub.status.busy":"2022-07-29T22:50:19.024834Z","iopub.execute_input":"2022-07-29T22:50:19.025451Z","iopub.status.idle":"2022-07-29T22:50:19.034048Z","shell.execute_reply.started":"2022-07-29T22:50:19.025408Z","shell.execute_reply":"2022-07-29T22:50:19.032561Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"size_transforms = A.Compose([A.Resize(height=IMAGE_WIDTH, width=IMAGE_HEIGHT, p=1),\n])\nfinal_transforms = ToTensorV2(p=1.0)\nupsize_transforms = A.Resize(height=UP_SIZE, width=UP_SIZE, p=1)","metadata":{"execution":{"iopub.status.busy":"2022-07-29T22:50:19.036428Z","iopub.execute_input":"2022-07-29T22:50:19.037971Z","iopub.status.idle":"2022-07-29T22:50:19.045886Z","shell.execute_reply.started":"2022-07-29T22:50:19.037915Z","shell.execute_reply":"2022-07-29T22:50:19.044567Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"coco = glob(r'../input/cocotest2014/test2014/' + '*.jpg')\ncoco = sorted([str(x) for x in coco])\n\ndf = pd.DataFrame(data={'color': coco, 'name': np.zeros(len(coco))})\n# clean data from grayscale images\ngray_indices = []\nfor i in trange(len(df)):\n    img = Image.open(df.loc[i, 'color'])\n    if img.mode == 'L':\n        gray_indices.append(i)\ndf.drop(gray_indices, inplace=True)\n\ntrain, valid = train_test_split(df, test_size=4000, shuffle=True, random_state=seed)\nprint(f'Train size: {len(train)}, valid size: {len(valid)}')","metadata":{"execution":{"iopub.status.busy":"2022-07-29T22:50:19.048127Z","iopub.execute_input":"2022-07-29T22:50:19.049336Z","iopub.status.idle":"2022-07-29T22:54:18.035431Z","shell.execute_reply.started":"2022-07-29T22:50:19.049276Z","shell.execute_reply":"2022-07-29T22:54:18.033393Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"example = df.sample(1000, random_state=seed)","metadata":{"execution":{"iopub.status.busy":"2022-07-29T22:54:18.038822Z","iopub.execute_input":"2022-07-29T22:54:18.039530Z","iopub.status.idle":"2022-07-29T22:54:18.050791Z","shell.execute_reply.started":"2022-07-29T22:54:18.039480Z","shell.execute_reply":"2022-07-29T22:54:18.049298Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def shift_right(x):\n    return torch.roll(x,1,dims=2)\n\ndef shift_down(x):\n    return torch.roll(x,1,dims=1)\n\ndef create_mask(batch_size, W):\n    mask = np.tril(np.ones((batch_size,W,W)),k=0).astype(\"uint8\")\n    return torch.Tensor(mask).int()\n\ndef positionalencoding2d(d_model, height, width, batch_size):\n    if d_model % 4 != 0:\n        raise ValueError(\"Cannot use sin/cos positional encoding with \"\n                         \"odd dimension (got dim={:d})\".format(d_model))\n    pe = torch.zeros(d_model, height, width)\n    d_model = d_model // 2\n    div_term = torch.exp(torch.arange(0., d_model, 2) *\n                         -(np.log(10000.0) / d_model))\n    pos_w = torch.arange(0., width).unsqueeze(1)\n    pos_h = torch.arange(0., height).unsqueeze(1)\n    pe[0:d_model:2, :, :] = torch.sin(pos_w * div_term).transpose(0, 1).unsqueeze(1).repeat(1, height, 1)\n    pe[1:d_model:2, :, :] = torch.cos(pos_w * div_term).transpose(0, 1).unsqueeze(1).repeat(1, height, 1)\n    pe[d_model::2, :, :] = torch.sin(pos_h * div_term).transpose(0, 1).unsqueeze(2).repeat(1, 1, width)\n    pe[d_model + 1::2, :, :] = torch.cos(pos_h * div_term).transpose(0, 1).unsqueeze(2).repeat(1, 1, width)\n\n    return pe.permute(1,2,0).repeat(batch_size,1,1,1)\n\ndef convertTo3bit(x, N):\n    return torch.round(torch.round(x*(N/255))*(255/N)).long()\n\ndef intTo3bit(value):\n    tmp = value\n    v1 = 64\n    c1 = value // v1\n    tmp -= c1 * v1\n    v2 = 8\n    c2 = tmp // v2\n    tmp -= c2 * v2\n    v3 = 1\n    c3 = tmp\n    return (c1,c2,c3)\n\ndef bitsToInt(channels):\n    res = 0\n    for k in range(3):\n        res += torch.round(channels[k]*(7/255)) * 8**k \n    return res\n\ndef toOneChannel(x):\n    n, r, c, _ = x.shape\n    res = torch.zeros(n, r, c)\n    for b in range(n):\n        for i in range(r):\n            for j in range(c): \n                res[b,i,j] = bitsToInt(x[b,i,j])\n    return res","metadata":{"execution":{"iopub.status.busy":"2022-07-29T22:54:18.053298Z","iopub.execute_input":"2022-07-29T22:54:18.054103Z","iopub.status.idle":"2022-07-29T22:54:18.083311Z","shell.execute_reply.started":"2022-07-29T22:54:18.054049Z","shell.execute_reply":"2022-07-29T22:54:18.081236Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"positionalencoding2d(32, 256, 256, 8).shape","metadata":{"execution":{"iopub.status.busy":"2022-07-29T22:54:18.086169Z","iopub.execute_input":"2022-07-29T22:54:18.086778Z","iopub.status.idle":"2022-07-29T22:54:18.181565Z","shell.execute_reply.started":"2022-07-29T22:54:18.086724Z","shell.execute_reply":"2022-07-29T22:54:18.179965Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class ColorDataset(Dataset):\n    \"\"\"\n    transforms: 256 or 512 image size\n    convert: \"bit\", \"upscale\", \"all\"\n    \"\"\"\n    def __init__(self, df, convert, image_size=256):\n        self.df = df\n        self.image_size = image_size\n        self.transforms = A.Resize(height=image_size, width=image_size, p=1)\n        self.convert = convert\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, ix):\n        row = self.df.iloc[ix].squeeze()\n        color_image = cv2.imread(row['color'])\n        color_image = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)\n        color_image = self.transforms(image=color_image)['image']\n        color_image = np.array(color_image)\n        \n        img_lab = rgb2lab(color_image).astype(\"float32\")\n        img_lab = final_transforms(image=img_lab)['image']\n        # L = img_lab[[0], ...] / 50. - 1.\n        L = img_lab[[0], ...].squeeze(0).long()  # / 100.\n        color_image = torch.as_tensor(color_image, dtype=torch.int32) # final_transforms(image=color_image)['image'][:,:,::-1]\n\n        ### + / 255. normalization ###\n        if self.convert == 'bit':\n            bit_image = convertTo3bit(color_image, 7).long()\n            return L, bit_image\n        elif self.convert == 'all':\n            bit_image = convertTo3bit(color_image, 7).long()\n            return L, bit_image, color_image\n        elif self.convert == 'upscale':  # image_size = 512\n            scaled_image = nn.functional.interpolate(color_image.unsqueeze(0).permute(0,3,1,2).float(), \n                                                     (self.image_size//2, self.image_size//2), \n                                                     mode=\"bilinear\").long().squeeze().permute(1,2,0)\n            scaled_image = scaled_image.squeeze(0)\n            return L, scaled_image, color_image\n        else:\n            raise ValueError(f'Unknown transformations provided.')\n            \n    def collate_fn(self, batch):\n        tup = list(zip(*batch))\n        tup = [[tensor[None].to(device) for tensor in arr] for arr in tup]\n        tup = [torch.cat(i) for i in tup]\n        return tup","metadata":{"execution":{"iopub.status.busy":"2022-07-29T22:54:18.183628Z","iopub.execute_input":"2022-07-29T22:54:18.184079Z","iopub.status.idle":"2022-07-29T22:54:18.204512Z","shell.execute_reply.started":"2022-07-29T22:54:18.184039Z","shell.execute_reply":"2022-07-29T22:54:18.203003Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"upscale_ds = ColorDataset(example, 'upscale', image_size=512)\nall_ds = ColorDataset(example, 'all')\nbit_ds = ColorDataset(example, 'bit')","metadata":{"execution":{"iopub.status.busy":"2022-07-29T22:54:18.212015Z","iopub.execute_input":"2022-07-29T22:54:18.212964Z","iopub.status.idle":"2022-07-29T22:54:18.223727Z","shell.execute_reply.started":"2022-07-29T22:54:18.212915Z","shell.execute_reply":"2022-07-29T22:54:18.222387Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"bit_ds[0][1].shape","metadata":{"execution":{"iopub.status.busy":"2022-07-29T22:54:18.225654Z","iopub.execute_input":"2022-07-29T22:54:18.226404Z","iopub.status.idle":"2022-07-29T22:54:18.315161Z","shell.execute_reply.started":"2022-07-29T22:54:18.226361Z","shell.execute_reply":"2022-07-29T22:54:18.314229Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, D):\n        super(MLP, self).__init__()\n\n        self.fc1 = nn.Linear(D,D)\n        self.fc2 = nn.Linear(D,D)\n        self.relu = nn.ReLU()\n\n    def forward(self,x):\n        return self.relu(self.fc2(self.fc1(x)))\n\nclass AttentionBlock(nn.Module):\n    def __init__(self, t, D):\n        super(AttentionBlock, self).__init__()\n\n        self.D = D\n        self.Type = t\n\n        # head 1\n        self.Q1 = nn.Linear(D,D,bias=False)\n        self.K1 = nn.Linear(D,D,bias=False)\n        self.V1 = nn.Linear(D,D,bias=False)\n        # head 2\n        self.Q2 = nn.Linear(D,D,bias=False)\n        self.K2 = nn.Linear(D,D,bias=False)\n        self.V2 = nn.Linear(D,D,bias=False)\n        # head 3\n        self.Q3 = nn.Linear(D,D,bias=False)\n        self.K3 = nn.Linear(D,D,bias=False)\n        self.V3 = nn.Linear(D,D,bias=False)\n        # head 4\n        self.Q4 = nn.Linear(D,D,bias=False)\n        self.K4 = nn.Linear(D,D,bias=False)\n        self.V4 = nn.Linear(D,D,bias=False)\n        # linear \n        self.out = nn.Linear(4*D,D)\n\n        self.LN = nn.LayerNorm(D)\n\n        self.mlp = MLP(D)\n\n    def forward(self, input_):\n\n        batch, row, col, _ = input_.shape\n        out = torch.empty_like(input_)\n      \n        softmax = torch.nn.Softmax(-1)\n\n        if self.Type == 'row':          \n            for i in range(row):\n                ln_input = self.LN(input_[:,i,:,:])\n\n                A1 = softmax(torch.matmul(self.Q1(ln_input),\n                                          self.K1(ln_input).transpose(1,2))/np.sqrt(self.D))\n                A2 = softmax(torch.matmul(self.Q2(ln_input),\n                                          self.K2(ln_input).transpose(1,2))/np.sqrt(self.D))\n                A3 = softmax(torch.matmul(self.Q3(ln_input),\n                                          self.K3(ln_input).transpose(1,2))/np.sqrt(self.D))\n                A4 = softmax(torch.matmul(self.Q4(ln_input),\n                                          self.K4(ln_input).transpose(1,2))/np.sqrt(self.D))\n                # A : BATCH * W * W\n\n                HA1 = torch.matmul(A1,self.V1(ln_input))\n                HA2 = torch.matmul(A2,self.V2(ln_input))\n                HA3 = torch.matmul(A3,self.V3(ln_input))\n                HA4 = torch.matmul(A4,self.V4(ln_input))\n\n                HSA = self.out(torch.cat((HA1,HA2,HA3,HA4),2))\n \n                tmp = HSA + input_[:,i,:,:] \n  \n                out[:,i,:,:] = self.mlp(self.LN(tmp)) + tmp # W * D\n          \n          # ColumnAttention\n        else:\n            for j in range(col):\n                ln_input = self.LN(input_[:,:,j,:])\n\n                A1 = softmax(torch.matmul(self.Q1(ln_input),\n                                          self.K1(ln_input).transpose(1,2))/np.sqrt(self.D))\n                A2 = softmax(torch.matmul(self.Q2(ln_input),\n                                          self.K2(ln_input).transpose(1,2))/np.sqrt(self.D))\n                A3 = softmax(torch.matmul(self.Q3(ln_input),\n                                          self.K3(ln_input).transpose(1,2))/np.sqrt(self.D))\n                A4 = softmax(torch.matmul(self.Q4(ln_input),\n                                          self.K4(ln_input).transpose(1,2))/np.sqrt(self.D))\n              # Ai : BATCH * H * H\n\n                HA1 = torch.matmul(A1,self.V1(ln_input))\n                HA2 = torch.matmul(A2,self.V2(ln_input))\n                HA3 = torch.matmul(A3,self.V3(ln_input))\n                HA4 = torch.matmul(A4,self.V4(ln_input))\n\n                HSA = self.out(torch.cat((HA1,HA2,HA3,HA4),2))\n \n                tmp = HSA + input_[:,:,j,:]\n\n                out[:,:,j,:] = self.mlp(self.LN(tmp)) + tmp # H * D\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-07-29T22:54:18.316614Z","iopub.execute_input":"2022-07-29T22:54:18.317171Z","iopub.status.idle":"2022-07-29T22:54:18.349741Z","shell.execute_reply.started":"2022-07-29T22:54:18.317138Z","shell.execute_reply":"2022-07-29T22:54:18.348300Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class MLPConditional(nn.Module):\n    def __init__(self, D):\n        super(MLPConditional, self).__init__()\n\n        self.fc1 = nn.Linear(D,D)\n        self.fc2 = nn.Linear(D,D)\n        self.relu = nn.ReLU()\n\n    def forward(self, x, conv1_context_h, conv2_context_h):      \n        h = self.relu(self.fc2(self.fc1(x)))\n        y = conv1_context_h * h + conv2_context_h\n        return y\n\nclass AttentionBlockConditional(nn.Module):\n    def __init__(self, t, D):\n\n        super(AttentionBlockConditional, self).__init__()\n\n        self.D = D\n        self.Type = t\n\n        # head 1\n        self.Q1 = nn.Linear(D,D,bias=False)\n        self.K1 = nn.Linear(D,D,bias=False)\n        self.V1 = nn.Linear(D,D,bias=False)\n        # head 2\n        self.Q2 = nn.Linear(D,D,bias=False)\n        self.K2 = nn.Linear(D,D,bias=False)\n        self.V2 = nn.Linear(D,D,bias=False)\n        # head 3\n        self.Q3 = nn.Linear(D,D,bias=False)\n        self.K3 = nn.Linear(D,D,bias=False)\n        self.V3 = nn.Linear(D,D,bias=False)\n        # head 4\n        self.Q4 = nn.Linear(D,D,bias=False)\n        self.K4 = nn.Linear(D,D,bias=False)\n        self.V4 = nn.Linear(D,D,bias=False)\n        # linear \n        self.out = nn.Linear(4*D,D)\n\n        self.conv1_z = nn.Conv2d(D,D,kernel_size=1,bias=False)\n        self.conv2_z = nn.Conv2d(D,D,kernel_size=1,bias=False)\n        self.conv1_h = nn.Conv2d(D,D,kernel_size=1,bias=False)\n        self.conv2_h = nn.Conv2d(D,D,kernel_size=1,bias=False)\n      \n        self.LN = nn.LayerNorm(D)\n        self.mean_avg_pool = nn.AvgPool1d(kernel_size=1)\n\n        self.mlp = MLPConditional(D)\n\n    def forward(self, input_, context, mask = None):\n        conv1_context_z = self.conv1_z(context.transpose(1,-1)).transpose(1,-1)\n        conv2_context_z = self.conv2_z(context.transpose(1,-1)).transpose(1,-1)\n      \n        conv1_context_h = self.conv1_h(context.transpose(1,-1)).transpose(1,-1)\n        conv2_context_h = self.conv2_h(context.transpose(1,-1)).transpose(1,-1)\n\n        batch, row, col, _ = input_.shape\n        out = torch.empty_like(input_)\n      \n        softmax = nn.Softmax(-1)\n\n        if self.Type == 'row':\n            for i in range(row):   \n            \n                ln_input = self.LN(input_[:,i,:,:])\n                maski = 1 if mask is None else mask[:,i,:].unsqueeze(-1) @ torch.ones(1, col)\n          \n                Q1c = self.Q1(ln_input) * conv1_context_z[:,i,:,:] + conv2_context_z[:,i,:,:]\n                K1c = self.K1(ln_input) * conv1_context_z[:,i,:,:] + conv2_context_z[:,i,:,:]\n                V1c = self.V1(ln_input) * conv1_context_z[:,i,:,:] + conv2_context_z[:,i,:,:]\n\n                Q2c = self.Q2(ln_input) * conv1_context_z[:,i,:,:] + conv2_context_z[:,i,:,:]\n                K2c = self.K2(ln_input) * conv1_context_z[:,i,:,:] + conv2_context_z[:,i,:,:]\n                V2c = self.V2(ln_input) * conv1_context_z[:,i,:,:] + conv2_context_z[:,i,:,:]\n\n                Q3c = self.Q3(ln_input) * conv1_context_z[:,i,:,:] + conv2_context_z[:,i,:,:]\n                K3c = self.K3(ln_input) * conv1_context_z[:,i,:,:] + conv2_context_z[:,i,:,:]\n                V3c = self.V3(ln_input) * conv1_context_z[:,i,:,:] + conv2_context_z[:,i,:,:]\n\n                Q4c = self.Q4(ln_input) * conv1_context_z[:,i,:,:] + conv2_context_z[:,i,:,:]\n                K4c = self.K4(ln_input) * conv1_context_z[:,i,:,:] + conv2_context_z[:,i,:,:]\n                V4c = self.V4(ln_input) * conv1_context_z[:,i,:,:] + conv2_context_z[:,i,:,:]\n\n                A1 = softmax(torch.matmul(Q1c, K1c.transpose(1,2)) * maski / np.sqrt(self.D))\n                A2 = softmax(torch.matmul(Q2c, K2c.transpose(1,2)) * maski / np.sqrt(self.D))\n                A3 = softmax(torch.matmul(Q3c, K3c.transpose(1,2)) * maski / np.sqrt(self.D))\n                A4 = softmax(torch.matmul(Q4c, K4c.transpose(1,2)) * maski / np.sqrt(self.D))\n          \n                # W * W\n\n                HA1 = torch.matmul(A1,V1c)\n                HA2 = torch.matmul(A2,V2c)\n                HA3 = torch.matmul(A3,V3c)\n                HA4 = torch.matmul(A4,V4c)\n\n                HSA = self.out(torch.cat((HA1,HA2,HA3,HA4),2))\n\n                tmp = HSA + input_[:,i,:,:] \n\n                out[:,i,:,:] = self.mlp(self.LN(tmp), conv1_context_h[:,i,:,:], conv2_context_h[:,i,:,:]) + tmp # W * D\n          \n        # ColumnAttention\n        else:\n            for j in range(col):\n                ln_input = self.LN(input_[:,:,j,:])\n                maskj = 1 if mask is None else mask[:,:,j].unsqueeze(-1) @ torch.ones(1, row) \n          \n                Q1c = self.Q1(ln_input) * conv1_context_z[:,:,j,:] + conv2_context_z[:,:,j,:]\n                K1c = self.K1(ln_input) * conv1_context_z[:,:,j,:] + conv2_context_z[:,:,j,:]\n                V1c = self.V1(ln_input) * conv1_context_z[:,:,j,:] + conv2_context_z[:,:,j,:]\n\n                Q2c = self.Q2(ln_input) * conv1_context_z[:,:,j,:] + conv2_context_z[:,:,j,:]\n                K2c = self.K2(ln_input) * conv1_context_z[:,:,j,:] + conv2_context_z[:,:,j,:]\n                V2c = self.V2(ln_input) * conv1_context_z[:,:,j,:] + conv2_context_z[:,:,j,:]\n\n                Q3c = self.Q3(ln_input) * conv1_context_z[:,:,j,:] + conv2_context_z[:,:,j,:]\n                K3c = self.K3(ln_input) * conv1_context_z[:,:,j,:] + conv2_context_z[:,:,j,:]\n                V3c = self.V3(ln_input) * conv1_context_z[:,:,j,:] + conv2_context_z[:,:,j,:]\n\n                Q4c = self.Q4(ln_input) * conv1_context_z[:,:,j,:] + conv2_context_z[:,:,j,:]\n                K4c = self.K4(ln_input) * conv1_context_z[:,:,j,:] + conv2_context_z[:,:,j,:]\n                V4c = self.V4(ln_input) * conv1_context_z[:,:,j,:] + conv2_context_z[:,:,j,:]\n\n                A1 = softmax(torch.matmul(Q1c, K1c.transpose(1,2)) * maskj / np.sqrt(self.D))\n                A2 = softmax(torch.matmul(Q2c, K2c.transpose(1,2)) * maskj / np.sqrt(self.D))\n                A3 = softmax(torch.matmul(Q3c, K3c.transpose(1,2)) * maskj / np.sqrt(self.D))\n                A4 = softmax(torch.matmul(Q4c, K4c.transpose(1,2)) * maskj / np.sqrt(self.D))\n\n                # W * W\n\n                HA1 = torch.matmul(A1,V1c)\n                HA2 = torch.matmul(A2,V2c)\n                HA3 = torch.matmul(A3,V3c)\n                HA4 = torch.matmul(A4,V4c)\n\n                HSA = self.out(torch.cat((HA1,HA2,HA3,HA4),2))\n\n                tmp = HSA + input_[:,:,j,:] \n\n                out[:,:,j,:] = self.mlp(self.LN(tmp),conv1_context_h[:,:,j,:], conv2_context_h[:,:,j,:]) + tmp # W * D\n          \n\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-07-29T22:54:18.351970Z","iopub.execute_input":"2022-07-29T22:54:18.352448Z","iopub.status.idle":"2022-07-29T22:54:18.409647Z","shell.execute_reply.started":"2022-07-29T22:54:18.352393Z","shell.execute_reply":"2022-07-29T22:54:18.408317Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class TransformerEncoder(nn.Module):\n    def __init__(self, D):\n        super(TransformerEncoder, self).__init__()\n\n        self.D = D\n        self.RowAttention = AttentionBlock('row',D)\n        self.ColumnAttention = AttentionBlock('col',D)\n\n    def forward(self, input_):\n        input_ = self.RowAttention(input_)\n        input_ = self.ColumnAttention(input_)\n        return input_\n\nclass GrayscaleEncoder(nn.Module):\n\n    def __init__(self, D):\n        super(GrayscaleEncoder, self).__init__()\n\n        self.D = D\n\n        self.TransformerEncoder_Layer1 = TransformerEncoder(D) \n        # self.TransformerEncoder_Layer2 = TransformerEncoder(D)\n        # self.TransformerEncoder_Layer3 = TransformerEncoder(D)\n        # self.TransformerEncoder_Layer4 = TransformerEncoder(D)\n\n\n    def forward(self, embedding_x_g):\n        \"\"\"\n        embedding_x_g : B* M * N * D\n        out : B * M * N * D\n        \"\"\"\n        out = self.TransformerEncoder_Layer1(embedding_x_g)\n        # out = self.TransformerEncoder_Layer2(out)\n        # out = self.TransformerEncoder_Layer3(out)\n        # out = self.TransformerEncoder_Layer4(out)\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-07-29T22:54:18.411380Z","iopub.execute_input":"2022-07-29T22:54:18.413022Z","iopub.status.idle":"2022-07-29T22:54:18.428865Z","shell.execute_reply.started":"2022-07-29T22:54:18.412965Z","shell.execute_reply":"2022-07-29T22:54:18.427306Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class TransformerDecoderInner(nn.Module):\n    def __init__(self, D):\n        super(TransformerDecoderInner, self).__init__()\n\n        self.D = D\n        self.ConditionalRowAttention = AttentionBlockConditional('row',D)\n\n    def forward(self, input_, ctx_encoder_decoder, j):\n        batch,row,col,_ = input_.shape\n    \n        mask = torch.ones(input_.shape[:-1])\n        mask[:,:,j:] = -1e9\n    \n        out = self.ConditionalRowAttention(input_, ctx_encoder_decoder, mask)\n\n        return out\n\nclass InnerDecoder(nn.Module):\n    \"\"\"\n    Generate a row, one pixel at a time\n    \"\"\"\n    def __init__(self, D):\n        super(InnerDecoder, self).__init__()\n        self.TransformerDecoderInner_Layer1 = TransformerDecoderInner(D) \n\n    def forward(self, emb_x_s_c, ctx_encoder_decoder, j):\n        \"\"\"\n        z = o + ShiftRight(e)\n        h = MaskedRow(z)\n        p(xij) = Dense(h)\n        \"\"\"\n        out = self.TransformerDecoderInner_Layer1(emb_x_s_c, ctx_encoder_decoder, j)\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-07-29T22:54:18.430546Z","iopub.execute_input":"2022-07-29T22:54:18.431712Z","iopub.status.idle":"2022-07-29T22:54:18.448784Z","shell.execute_reply.started":"2022-07-29T22:54:18.431660Z","shell.execute_reply":"2022-07-29T22:54:18.447672Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class TransformerDecoderOuter(nn.Module):\n    def __init__(self, D):\n        super(TransformerDecoderOuter, self).__init__()\n\n        self.D = D\n        self.ConditionalRowAttention = AttentionBlockConditional('row',D)\n        self.ConditionalColumnAttention = AttentionBlockConditional('col',D)\n        self.row = None\n\n    def forward(self, input_, ctx_grayscale_encoder, i):\n        batch,row,col,_ = input_.shape\n    \n        mask = torch.ones(input_.shape[:-1])\n        mask[:,i+1:,:] = -1e9\n    \n        if self.row is None:\n            self.row = self.ConditionalRowAttention(input_, ctx_grayscale_encoder)\n\n        out = self.ConditionalColumnAttention(self.row, ctx_grayscale_encoder, mask)\n\n        return out\n\nclass OuterDecoder(nn.Module):\n    def __init__(self, D):\n        super(OuterDecoder, self).__init__()\n        self.D = D\n        self.TransformerDecoderOuter_Layer1 = TransformerDecoderOuter(D) \n\n    def forward(self, emb_x_s_c, ctx_grayscale_encoder, i):\n        \"\"\"\n            |e = Embeddings(x)\n        N x |s_o = MaskedColumn(Row(e))\n            |o = ShiftDown(s_o)\n        \n        \"\"\"\n        out = self.TransformerDecoderOuter_Layer1(emb_x_s_c, ctx_grayscale_encoder, i)\n    \n        out = shift_down(out)\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-07-29T22:54:18.450421Z","iopub.execute_input":"2022-07-29T22:54:18.451564Z","iopub.status.idle":"2022-07-29T22:54:18.467168Z","shell.execute_reply.started":"2022-07-29T22:54:18.451513Z","shell.execute_reply":"2022-07-29T22:54:18.466003Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class ColTranSpatialUpsampler(nn.Module):\n\n    def __init__(self, D, NColor, H, W):\n        super(ColTranSpatialUpsampler, self).__init__()\n\n        self.D = D\n        self.H = H\n        self.W = W\n        self.NColor = NColor\n\n        self.embedding_x_g = nn.Embedding(NColor,D) # BATCH * H * W * D\n        self.embedding_x_rgb = nn.Sequential(nn.Embedding(NColor,D), nn.Embedding(NColor,D), nn.Embedding(NColor,D))\n\n        self.grayscale_encoder = GrayscaleEncoder(D)\n        self.linear = nn.Linear(D,NColor)\n\n    def forward(self, x_g, x_s):\n        \"\"\"\n        x_s : M * N * 3\n        x_g : H * W * 1\n        return\n        x :  H * W * 3\n        \"\"\"\n    \n        x_s = nn.functional.interpolate(x_s.permute(0,3,1,2).float(),size=(self.H, self.W),mode=\"bilinear\").permute(0,2,3,1).long()\n        batch,row,col,channel = x_s.shape\n        pe = positionalencoding2d(self.D, row, col, batch)\n    \n        emb_g = pe + self.embedding_x_g(x_g)  \n        out = torch.zeros(batch, row, col, channel, self.NColor)   \n    \n        for k in range(channel):\n        \n            emb_k = pe + self.embedding_x_rgb[k](x_s[:,:,:,k])  \n            input_encoder = emb_g + emb_k\n            out_encoder = self.grayscale_encoder(input_encoder)\n  \n            out[:,:,:,k] = self.linear(out_encoder)\n\n        return out.argmax(-1), out","metadata":{"execution":{"iopub.status.busy":"2022-07-29T22:54:18.469132Z","iopub.execute_input":"2022-07-29T22:54:18.469927Z","iopub.status.idle":"2022-07-29T22:54:18.489371Z","shell.execute_reply.started":"2022-07-29T22:54:18.469876Z","shell.execute_reply":"2022-07-29T22:54:18.487146Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"class ColTranColorUpsampler(nn.Module):\n\n    def __init__(self, D, NColor):\n        super(ColTranColorUpsampler, self).__init__()\n\n        self.D = D\n        self.NColor = NColor\n\n        self.embedding_x_g = nn.Embedding(NColor,D) # BATCH * M * N * D\n        self.embedding_x_rgb = nn.Sequential(nn.Embedding(NColor,D), nn.Embedding(NColor,D), nn.Embedding(NColor,D))\n\n        self.grayscale_encoder = GrayscaleEncoder(D)\n\n        self.linear = nn.Linear(D,NColor)\n\n    def forward(self, x_g, x_s_c):\n        \"\"\"\n        x_s_c : M * N * 3\n        x_g : M * N * 1\n        return:\n        x : H * W * 3\n        \"\"\"\n    \n        batch,row,col,channel = x_s_c.shape\n        pe = positionalencoding2d(self.D, row, col, batch)\n        emb_g = pe + self.embedding_x_g(x_g)  \n    \n        out = torch.zeros(batch, row, col, channel, self.NColor)    \n    \n        for k in range(channel):\n        \n            emb_k = pe + self.embedding_x_rgb[k](x_s_c[:,:,:,k])       \n            input_encoder = emb_g + emb_k\n            out_encoder = self.grayscale_encoder(input_encoder)\n            out[:,:,:,k] = self.linear(out_encoder)\n\n        return out.argmax(-1), out","metadata":{"execution":{"iopub.status.busy":"2022-07-29T22:54:18.491437Z","iopub.execute_input":"2022-07-29T22:54:18.492394Z","iopub.status.idle":"2022-07-29T22:54:18.509374Z","shell.execute_reply.started":"2022-07-29T22:54:18.492292Z","shell.execute_reply":"2022-07-29T22:54:18.507519Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"class ColTranCore(nn.Module):\n\n    def __init__(self, D, nb_colors):\n        super(ColTranCore, self).__init__()\n\n        self.D = D\n        self.nb_colors = nb_colors\n\n        self.embedding_x_g = nn.Embedding(nb_colors, D) # H * W * D\n        self.embedding_x_s_c = nn.Embedding(nb_colors, D) # H * W * D\n\n        self.grayscale_encoder = GrayscaleEncoder(D)\n        self.outer_decoder = OuterDecoder(D)\n        self.inner_decoder = InnerDecoder(D)\n\n        self.out_inner_decoder = nn.Linear(D,512)\n        self.out_grayscale_encoder = nn.Linear(D,512)\n    \n    def sampling(self, proba):\n        value = torch.linspace(0, self.nb_colors, 8, dtype=torch.long)\n        b, m, n, l = proba.shape\n        x_hat_s_c = torch.zeros(b,m,n,3)\n        \n        for i in range(m):\n            for j in range(n):\n                prob_dist = torch.distributions.Categorical(proba[:,i,j,:])\n                rgb = intTo3bit(prob_dist.sample()) \n                for k in range(3):\n                    x_hat_s_c[:,i,j,k] = value[rgb[k]]\n        return x_hat_s_c\n\n    def forward(self, x_g, x_s_c=None):\n        \"\"\"\n        x_g : B * M * N\n        x_s_c : B * M * N * 3\n        return\n        proba : B * M * N * 8^3\n        \"\"\"\n        batch, row, col, channel = x_g.shape if x_s_c is None else x_s_c.shape\n        pe = positionalencoding2d(self.D, row, col, batch)\n\n        out_g = pe + self.embedding_x_g(x_g)\n        out_i = torch.zeros(batch, row, col, self.D)\n        out_o = torch.zeros(batch, row, col, self.D)\n    \n        projection = torch.zeros(batch, row, col, 512)\n        x_hat_s_c = torch.zeros(batch, row, col, 3)\n    \n        for k in range(channel):\n            x_s_ck = self.embedding_x_s_c((x_g if x_s_c is None else x_s_c)[:,:,:,k]) + pe\n            out_g = self.grayscale_encoder(out_g)\n            for i in range(row):\n                out_o = self.outer_decoder(x_s_ck, out_g, i)\n            \n                context_i = (out_g + out_o)[:,i].unsqueeze(1)\n                input_i = (context_i + shift_right(x_s_ck))[:,i].unsqueeze(1)\n            \n                for j in range(col):\n                    out_i = self.inner_decoder(input_i, context_i, j)\n                    projection[:,i,j] += self.out_inner_decoder(out_i[:,0,j])\n                    # B * M * N * 8**3           \n        \n            # On reset conditionnal row attention               \n            self.outer_decoder = OuterDecoder(self.D)\n\n        x_hat_s_c = self.sampling(projection.softmax(-1))    \n\n        return x_hat_s_c, projection, self.out_grayscale_encoder(out_g) ","metadata":{"execution":{"iopub.status.busy":"2022-07-29T22:54:18.511853Z","iopub.execute_input":"2022-07-29T22:54:18.512420Z","iopub.status.idle":"2022-07-29T22:54:18.537772Z","shell.execute_reply.started":"2022-07-29T22:54:18.512372Z","shell.execute_reply":"2022-07-29T22:54:18.535962Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\nM = N = 64\nH = W = 256\nD = 32 # 512\n\ncore = ColTranCore(D, 256).to(device)\ncolor = ColTranColorUpsampler(D, 256).to(device)\nspatial = ColTranSpatialUpsampler(D, 256, H, W).to(device)\n\noptimizer_core = torch.optim.RMSprop(core.parameters(), lr=3e-4)\noptimizer_color = torch.optim.RMSprop(color.parameters(), lr=3e-4)\noptimizer_spatial = torch.optim.RMSprop(spatial.parameters(), lr=3e-4)","metadata":{"execution":{"iopub.status.busy":"2022-07-29T22:54:18.539939Z","iopub.execute_input":"2022-07-29T22:54:18.540457Z","iopub.status.idle":"2022-07-29T22:54:18.592321Z","shell.execute_reply.started":"2022-07-29T22:54:18.540418Z","shell.execute_reply":"2022-07-29T22:54:18.590590Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"x_g = torch.randint(0, 255, [1,M,N])\nx_s_c = torch.randint(0, 255, [1,M,N,3])\n\ncore.eval()\nout1 = core(x_g, x_s_c)","metadata":{"execution":{"iopub.status.busy":"2022-07-29T22:54:18.594637Z","iopub.execute_input":"2022-07-29T22:54:18.595167Z","iopub.status.idle":"2022-07-29T22:55:36.069535Z","shell.execute_reply.started":"2022-07-29T22:54:18.595114Z","shell.execute_reply":"2022-07-29T22:55:36.067873Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"out1[0].shape, out1[1].shape, out1[2].shape","metadata":{"execution":{"iopub.status.busy":"2022-07-29T22:55:36.071769Z","iopub.execute_input":"2022-07-29T22:55:36.072336Z","iopub.status.idle":"2022-07-29T22:55:36.083224Z","shell.execute_reply.started":"2022-07-29T22:55:36.072283Z","shell.execute_reply":"2022-07-29T22:55:36.081677Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"x_g = torch.randint(0, 255, [1,M,N])\nx_s = torch.randint(0, 255, [1,M,N,3])\n\ncolor.eval()\nout2 = color(x_g, x_s)","metadata":{"execution":{"iopub.status.busy":"2022-07-29T22:55:36.086039Z","iopub.execute_input":"2022-07-29T22:55:36.087164Z","iopub.status.idle":"2022-07-29T22:55:36.847644Z","shell.execute_reply.started":"2022-07-29T22:55:36.087107Z","shell.execute_reply":"2022-07-29T22:55:36.845814Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"out2[0].shape, out2[1].shape","metadata":{"execution":{"iopub.status.busy":"2022-07-29T22:55:36.850691Z","iopub.execute_input":"2022-07-29T22:55:36.851303Z","iopub.status.idle":"2022-07-29T22:55:36.859725Z","shell.execute_reply.started":"2022-07-29T22:55:36.851241Z","shell.execute_reply":"2022-07-29T22:55:36.858514Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"x_g = torch.randint(0, 255, [1,H,W])\nx_s = torch.randint(0, 255, [1,M,N,3])\n\nspatial.eval()\nout3 = spatial(x_g, x_s)","metadata":{"execution":{"iopub.status.busy":"2022-07-29T22:55:36.861331Z","iopub.execute_input":"2022-07-29T22:55:36.862383Z"},"trusted":true},"execution_count":null,"outputs":[]}]}