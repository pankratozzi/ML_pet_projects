{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nfrom torchvision import transforms as T\nfrom torch.utils.data import Dataset, DataLoader\n\nimport cv2\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.autonotebook import tqdm\nfrom glob import glob\n\nfrom sklearn.model_selection import train_test_split\n\n!pip install -qq editdistance torchsummary\nimport editdistance\nfrom torchsummary import summary\n\nseed = 42","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-03T22:04:28.079305Z","iopub.execute_input":"2022-05-03T22:04:28.079998Z","iopub.status.idle":"2022-05-03T22:04:37.569417Z","shell.execute_reply.started":"2022-05-03T22:04:28.079961Z","shell.execute_reply":"2022-05-03T22:04:37.568392Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"torch.random.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\nnp.random.seed(seed)\n\nPATH = r'../input/handwritten/synthetic-data/'\nLR = 0.001\nBATCH_SIZE = 32\nHIDDEN = 512\nENC_LAYERS = 2\nDEC_LAYERS = 2\nN_HEADS = 4\nDROPOUT = 0.1\nIMG_WIDTH = 256\nIMG_HEIGHT = 64\nEPOCHS = 100\n\nVOCAB = ['PAD', 'SOS', ' ',] + [char for char in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'] + ['EOS']\n\nchar2idx = {char: idx for idx, char in enumerate(VOCAB)}\nidx2char = {idx: char for idx, char in enumerate(VOCAB)}\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Currenly using {device.upper()} device.')","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:04:37.571678Z","iopub.execute_input":"2022-05-03T22:04:37.571941Z","iopub.status.idle":"2022-05-03T22:04:37.583733Z","shell.execute_reply.started":"2022-05-03T22:04:37.571904Z","shell.execute_reply":"2022-05-03T22:04:37.583021Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"def text_to_labels(text, char2idx=char2idx):\n    return [char2idx['SOS']] + [char2idx[i.upper()] for i in text if i.upper() in char2idx.keys()] + [char2idx['EOS']]\n\ndef labels_to_text(text, idx2char=idx2char):\n    S = \"\".join([idx2char[i] for i in text])\n    if S.find('EOS') == -1:\n        return S\n    else:\n        return S[:S.find('EOS')]\n\ndef char_error_rate(p_seq1, p_seq2):\n    p_vocab = set(p_seq1 + p_seq2)\n    p2c = dict(zip(p_vocab, range(len(p_vocab))))\n    c_seq1 = [chr(p2c[p]) for p in p_seq1]\n    c_seq2 = [chr(p2c[p]) for p in p_seq2]\n    return editdistance.eval(''.join(c_seq1),\n                             ''.join(c_seq2)) / max(len(c_seq1), len(c_seq2))","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:04:37.585332Z","iopub.execute_input":"2022-05-03T22:04:37.585539Z","iopub.status.idle":"2022-05-03T22:04:37.596925Z","shell.execute_reply.started":"2022-05-03T22:04:37.585508Z","shell.execute_reply":"2022-05-03T22:04:37.596248Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"transforms = T.Compose([\n                        T.ToPILImage(),\n                        T.Resize((IMG_HEIGHT, IMG_WIDTH)),\n                        T.ColorJitter(contrast=(0.5,1),saturation=(0.5,1)),\n                        T.RandomRotation(degrees=(-9, 9), fill=255),\n                        T.RandomAffine(10 ,None ,[0.6 ,1] ,3 ,fillcolor=255),\n                        T.ToTensor()\n                        ])\nvalid_transforms = T.Compose([\n                              T.ToPILImage(),\n                              T.Resize((IMG_HEIGHT, IMG_WIDTH)),\n                              T.ToTensor()\n                              ])","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:04:40.067321Z","iopub.execute_input":"2022-05-03T22:04:40.068053Z","iopub.status.idle":"2022-05-03T22:04:40.074237Z","shell.execute_reply.started":"2022-05-03T22:04:40.068013Z","shell.execute_reply":"2022-05-03T22:04:40.073402Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"images_paths = glob(PATH+'*.png')\nimages_paths = sorted([str(path) for path in images_paths])\nimages_labels = [path.split('/')[-1].split('_')[0] for path in images_paths]\ndf = pd.DataFrame(data={'path': images_paths, 'label': images_labels})\ndf.sample(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:04:45.559655Z","iopub.execute_input":"2022-05-03T22:04:45.560218Z","iopub.status.idle":"2022-05-03T22:04:45.702629Z","shell.execute_reply.started":"2022-05-03T22:04:45.560180Z","shell.execute_reply":"2022-05-03T22:04:45.701955Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"df_train, df_valid = train_test_split(df, test_size=0.25, shuffle=True, random_state=seed)\nprint(f'Train size: {df_train.shape[0]}, validation size: {df_valid.shape[0]}')","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:04:49.072935Z","iopub.execute_input":"2022-05-03T22:04:49.073607Z","iopub.status.idle":"2022-05-03T22:04:49.085328Z","shell.execute_reply.started":"2022-05-03T22:04:49.073561Z","shell.execute_reply":"2022-05-03T22:04:49.084416Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"class CharDataset(Dataset):\n    def __init__(self, dataframe, transforms=transforms):\n        self.dataframe = dataframe\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.dataframe)\n    \n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx].squeeze()\n        image = cv2.imread(row['path'])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = transforms(image)\n        label = text_to_labels(row['label'])\n        return torch.FloatTensor(image / 255.), torch.LongTensor(label) \n    \n    def collate_fn(self, batch):\n        x_padded = []\n        max_y_len = max([i[1].size(0) for i in batch])\n        y_padded = torch.LongTensor(max_y_len, len(batch))\n        y_padded.zero_()\n\n        for i in range(len(batch)):\n            x_padded.append(batch[i][0].unsqueeze(0))\n            y = batch[i][1]\n            y_padded[:y.size(0), i] = y\n\n        x_padded = torch.cat(x_padded)\n        return x_padded.to(device), y_padded.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:04:51.206745Z","iopub.execute_input":"2022-05-03T22:04:51.207309Z","iopub.status.idle":"2022-05-03T22:04:51.219965Z","shell.execute_reply.started":"2022-05-03T22:04:51.207271Z","shell.execute_reply":"2022-05-03T22:04:51.218437Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"class TransformerModel(nn.Module):\n    def __init__(self, bb_name, outtoken, hidden, enc_layers=1, dec_layers=1, nhead=1, dropout=0.1, pretrained=False):\n        super(TransformerModel, self).__init__()\n        self.backbone = torchvision.models.__getattribute__(bb_name)(pretrained=pretrained)\n        self.backbone.fc = nn.Conv2d(2048, int(hidden/2), 1)\n\n        self.pos_encoder = PositionalEncoding(hidden, dropout)\n        self.decoder = nn.Embedding(outtoken, hidden)\n        self.pos_decoder = PositionalEncoding(hidden, dropout)\n        self.transformer = nn.Transformer(d_model=hidden, nhead=nhead, num_encoder_layers=enc_layers,\n                                          num_decoder_layers=dec_layers, dim_feedforward=hidden * 4, dropout=dropout,\n                                          activation='relu')\n\n        self.fc_out = nn.Linear(hidden, outtoken)\n        self.src_mask = None\n        self.trg_mask = None\n        self.memory_mask = None\n\n    def generate_square_subsequent_mask(self, sz):\n        mask = torch.triu(torch.ones(sz, sz), 1)\n        mask = mask.masked_fill(mask == 1, float('-inf'))\n        return mask\n\n    def make_len_mask(self, inp):\n        return (inp == 0).transpose(0, 1)\n\n    def forward(self, src, trg):\n        if self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n            self.trg_mask = self.generate_square_subsequent_mask(len(trg)).to(device) \n        x = self.backbone.conv1(src)\n\n        x = self.backbone.bn1(x)\n        x = self.backbone.relu(x)\n        x = self.backbone.maxpool(x)\n        x = self.backbone.layer1(x)\n        x = self.backbone.layer2(x)\n        x = self.backbone.layer3(x)\n        x = self.backbone.layer4(x) # [64, 2048, 2, 8] : [B,C,H,W]\n            \n        x = self.backbone.fc(x) # [64, 256, 2, 8] : [B,C,H,W]\n        x = x.permute(0, 3, 1, 2) # [64, 8, 256, 2] : [B,W,C,H]\n        x = x.flatten(2) # [64, 8, 512] : [B,W,CH]\n        x = x.permute(1, 0, 2) # [8, 64, 512] : [W,B,CH]\n        src_pad_mask = self.make_len_mask(x[:, :, 0])\n        src = self.pos_encoder(x) # [8, 64, 512]\n        trg_pad_mask = self.make_len_mask(trg)\n        trg = self.decoder(trg)\n        trg = self.pos_decoder(trg)\n\n        output = self.transformer(src, trg, src_mask=self.src_mask, tgt_mask=self.trg_mask,\n                                  memory_mask=self.memory_mask,\n                                  src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=trg_pad_mask,\n                                  memory_key_padding_mask=src_pad_mask) # [13, 64, 512] : [L,B,CH]\n        output = self.fc_out(output) # [13, 64, 92] : [L,B,H]\n\n        return output\n    \nclass PositionalEncoding(nn.Module):  # when having a sentences\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        self.scale = nn.Parameter(torch.ones(1))\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.scale * self.pe[:x.size(0), :]\n        return self.dropout(x) ","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:04:54.103388Z","iopub.execute_input":"2022-05-03T22:04:54.103660Z","iopub.status.idle":"2022-05-03T22:04:54.129516Z","shell.execute_reply.started":"2022-05-03T22:04:54.103629Z","shell.execute_reply":"2022-05-03T22:04:54.128745Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"def train_one_batch(model, data, optimizer, criterion):\n    model.train()\n    image, target = data\n    optimizer.zero_grad()\n    output = model(image, target[:-1, :])\n    loss = criterion(output.view(-1, output.shape[-1]), torch.reshape(target[1:, :], (-1,)))\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\n@torch.no_grad()\ndef validate_one_batch(model, data, criterion):\n    model.eval()\n    image, target = data\n    output = model(image, target[:-1, :])\n    loss = criterion(output.view(-1, output.shape[-1]), torch.reshape(target[1:, :], (-1,)))\n    return loss.item()\n\ndef evaluate(model, dataloader, max_len=30):  # assuming dataloader has batch_size=1\n    model.eval()\n    wer_overall = 0\n    cer_overall = 0\n    with torch.no_grad():\n        for src, trg in tqdm(dataloader, leave=False):\n            out_indexes = [char2idx['SOS'], ]\n\n            for i in range(max_len):\n                trg_tensor = torch.LongTensor(out_indexes).unsqueeze(1).to(device)\n                output = model(src, trg_tensor)\n                out_token = output.argmax(2)[-1].item()\n                out_indexes.append(out_token)\n                if out_token == char2idx['EOS']:\n                    break\n                    \n            out_char = labels_to_text(out_indexes[1:])\n            real_char = labels_to_text(trg[1:, 0].detach().cpu().numpy()).lower()\n            wer_overall += int(real_char != out_char)\n            if out_char:\n                cer = char_error_rate(real_char, out_char)\n            else:\n                cer = 1\n            \n            cer_overall += cer\n    \n    return cer_overall / len(dataloader) * 100, wer_overall / len(dataloader) * 100\n\n@torch.no_grad()\ndef prediction(model, filepath='random', max_len=30):\n    label = None\n    if filepath == 'random':\n        idx = np.random.randint(len(df_valid))\n        filepath = df_valid.iloc[idx, 0]\n        label = df_valid.iloc[idx, 1]\n\n    model.eval()\n    img = cv2.imread(filepath)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    image = valid_transforms(img)\n    src = torch.FloatTensor(image / 255.).unsqueeze(0).to(device)\n\n    out_indexes = [char2idx['SOS'], ]\n\n    for i in range(max_len):\n                \n        trg_tensor = torch.LongTensor(out_indexes).unsqueeze(1).to(device)\n                \n        output = model(src, trg_tensor)\n        out_token = output.argmax(2)[-1].item()\n        out_indexes.append(out_token)\n        if out_token == char2idx['EOS']:\n            break\n    preds = labels_to_text(out_indexes[1:], idx2char)\n    plt.figure(figsize=(6,4))\n    plt.title(f'Prediction: {preds}, Truth: {label if label is not None else \"NO label\"}')\n    plt.imshow(img)\n    plt.tight_layout()\n    plt.show()\n    plt.pause(0.001)\n    \n    return preds","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:04:57.881866Z","iopub.execute_input":"2022-05-03T22:04:57.882168Z","iopub.status.idle":"2022-05-03T22:04:57.904727Z","shell.execute_reply.started":"2022-05-03T22:04:57.882114Z","shell.execute_reply":"2022-05-03T22:04:57.904031Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"train_dataset = CharDataset(df_train, transforms)\nvalid_dataset = CharDataset(df_valid, valid_transforms)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=train_dataset.collate_fn, drop_last=True)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=1, shuffle=False, collate_fn=valid_dataset.collate_fn, drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:05:01.543056Z","iopub.execute_input":"2022-05-03T22:05:01.543619Z","iopub.status.idle":"2022-05-03T22:05:01.548777Z","shell.execute_reply.started":"2022-05-03T22:05:01.543581Z","shell.execute_reply":"2022-05-03T22:05:01.548114Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"model = TransformerModel('resnet50', len(VOCAB), hidden=HIDDEN, enc_layers=ENC_LAYERS, dec_layers=DEC_LAYERS,   \n                         nhead=N_HEADS, dropout=DROPOUT).to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-6)\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, mode='min', patience=10, min_lr=1e-6,)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:05:03.788494Z","iopub.execute_input":"2022-05-03T22:05:03.789043Z","iopub.status.idle":"2022-05-03T22:05:04.453002Z","shell.execute_reply.started":"2022-05-03T22:05:03.789005Z","shell.execute_reply":"2022-05-03T22:05:04.452284Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"train_losses, valid_losses = [], []\n\nfor epoch in range(EPOCHS):\n    print(f'{epoch+1}/{EPOCHS} epoch.')\n    epoch_train_losses, epoch_valid_losses = [], []\n    for _, batch in enumerate(tqdm(train_dataloader, leave=False)):\n        loss = train_one_batch(model, batch, optimizer, criterion)\n        epoch_train_losses.append(loss)\n        \n    train_epoch_loss = np.array(epoch_train_losses).mean()\n    train_losses.append(train_epoch_loss)\n    \n    for _, batch in enumerate(tqdm(valid_dataloader, leave=False)):\n        loss = validate_one_batch(model, batch, criterion)\n        epoch_valid_losses.append(loss)\n        \n    valid_epoch_loss = np.array(epoch_valid_losses).mean()\n    valid_losses.append(valid_epoch_loss)\n    print(f'Train loss: {train_epoch_loss:.4f}, validation loss: {valid_epoch_loss:.4f}')\n    \n    if (epoch + 1) % 5 == 0:\n        valid_cer, valid_wer = evaluate(model, valid_dataloader)\n        print(f'Char_error_rate: {valid_cer:.4f}, Word_error_rate: {valid_wer:.4f}')\n    \n    scheduler.step(epoch_loss)\n    \n    if (epoch+1) % 10 == 0:\n        pred = prediction(model)\n        torch.save(model.state_dict(), 'model.pth')","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:05:08.606114Z","iopub.execute_input":"2022-05-03T22:05:08.606694Z","iopub.status.idle":"2022-05-03T22:05:34.423457Z","shell.execute_reply.started":"2022-05-03T22:05:08.606658Z","shell.execute_reply":"2022-05-03T22:05:34.420793Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"prediction(model)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T21:47:08.381698Z","iopub.execute_input":"2022-05-03T21:47:08.382094Z","iopub.status.idle":"2022-05-03T21:47:08.705641Z","shell.execute_reply.started":"2022-05-03T21:47:08.382057Z","shell.execute_reply":"2022-05-03T21:47:08.704893Z"},"trusted":true},"execution_count":79,"outputs":[]}]}