{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom glob import glob\nimport cv2\nimport os\nfrom PIL import Image\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nimport torchvision.transforms as T\n\nfrom tqdm.notebook import tqdm\n\nseed = 42","metadata":{"execution":{"iopub.status.busy":"2022-08-01T07:07:00.419471Z","iopub.execute_input":"2022-08-01T07:07:00.420154Z","iopub.status.idle":"2022-08-01T07:07:02.953087Z","shell.execute_reply.started":"2022-08-01T07:07:00.420004Z","shell.execute_reply":"2022-08-01T07:07:02.952086Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"**Link to great advanced inpainting models**\n\n* [lama](https://github.com/saic-mdal/lama)\n* [csqiangwen](https://github.com/csqiangwen/DeepFillv2_Pytorch)","metadata":{}},{"cell_type":"code","source":"def seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything()","metadata":{"execution":{"iopub.status.busy":"2022-08-01T07:07:02.956881Z","iopub.execute_input":"2022-08-01T07:07:02.957341Z","iopub.status.idle":"2022-08-01T07:07:02.965696Z","shell.execute_reply.started":"2022-08-01T07:07:02.957303Z","shell.execute_reply":"2022-08-01T07:07:02.964731Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"epochs = 40\nbatch_size = 16\nlr = 8e-5\nimage_size = 128\nmask_size = 64\npath = r'painting_model.pth'\nb1 = 0.5\nb2 = 0.999\n\npatch_h, patch_w = int(mask_size / 2 ** 3), int(mask_size / 2 ** 3)\npatch = (1, patch_h, patch_w)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Currently using \"{device.upper()}\" device.')","metadata":{"execution":{"iopub.status.busy":"2022-08-01T07:07:02.967231Z","iopub.execute_input":"2022-08-01T07:07:02.967871Z","iopub.status.idle":"2022-08-01T07:07:03.044196Z","shell.execute_reply.started":"2022-08-01T07:07:02.967833Z","shell.execute_reply":"2022-08-01T07:07:03.042277Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"transforms = T.Compose([\n    T.Resize((image_size, image_size), Image.BICUBIC),\n    T.ToTensor(),\n    T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n])\n\ninverse_transforms = T.Compose([\n    T.Normalize(-1, 2),\n    T.ToPILImage()\n])","metadata":{"execution":{"iopub.status.busy":"2022-08-01T07:07:03.047655Z","iopub.execute_input":"2022-08-01T07:07:03.048515Z","iopub.status.idle":"2022-08-01T07:07:03.055412Z","shell.execute_reply.started":"2022-08-01T07:07:03.048474Z","shell.execute_reply":"2022-08-01T07:07:03.054499Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"image_paths = sorted([str(p) for p in glob('../input/celebahq-resized-256x256/celeba_hq_256' + '/*.jpg')])\n\ntrain, valid = train_test_split(image_paths, test_size=5000, shuffle=True, random_state=seed)\nvalid, test = train_test_split(valid, test_size=1000, shuffle=True, random_state=seed)\nprint(f'Train size: {len(train)}, validation size: {len(valid)}, test size: {len(test)}.')","metadata":{"execution":{"iopub.status.busy":"2022-08-01T07:07:03.057322Z","iopub.execute_input":"2022-08-01T07:07:03.058108Z","iopub.status.idle":"2022-08-01T07:07:08.387064Z","shell.execute_reply.started":"2022-08-01T07:07:03.058065Z","shell.execute_reply":"2022-08-01T07:07:08.386013Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class CelebaDataset(Dataset):\n    def __init__(self, images_paths, transforms=transforms, train=True):\n        self.images_paths = images_paths\n        self.transforms = transforms\n        self.train = train\n        \n    def __len__(self):\n        return len(self.images_paths)\n    \n    def apply_center_mask(self, image):\n        idx = (image_size - mask_size) // 2\n        masked_image = image.clone()\n        masked_image[:, idx:idx+mask_size, idx:idx+mask_size] = 1\n        masked_part = image[:, idx:idx+mask_size, idx:idx+mask_size]\n        return masked_image, idx\n    \n    def apply_random_mask(self, image):\n        y1, x1 = np.random.randint(0, image_size-mask_size, 2)\n        y2, x2 = y1 + mask_size, x1 + mask_size\n        masked_part = image[:, y1:y2, x1:x2]\n        masked_image = image.clone()\n        masked_image[:, y1:y2, x1:x2] = 1\n        return masked_image, masked_part\n    \n    def __getitem__(self, ix):\n        path = self.images_paths[ix]\n        image = Image.open(path)\n        image = self.transforms(image)\n        \n        if self.train:\n            masked_image, masked_part = self.apply_random_mask(image)\n        else:\n            masked_image, masked_part = self.apply_center_mask(image)\n            \n        return image, masked_image, masked_part\n    \n    def collate_fn(self, batch):\n        images, masked_images, masked_parts = list(zip(*batch))\n        images, masked_images, masked_parts = [[tensor[None].to(device) for tensor in ims] for ims in [images, masked_images, masked_parts]]\n        images, masked_images, masked_parts = [torch.cat(ims) for ims in [images, masked_images, masked_parts]]\n        return images, masked_images, masked_parts","metadata":{"execution":{"iopub.status.busy":"2022-08-01T07:07:08.388621Z","iopub.execute_input":"2022-08-01T07:07:08.388981Z","iopub.status.idle":"2022-08-01T07:07:08.403703Z","shell.execute_reply.started":"2022-08-01T07:07:08.388943Z","shell.execute_reply":"2022-08-01T07:07:08.402667Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_dataset = CelebaDataset(train)\nvalid_dataset = CelebaDataset(valid, train=True)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn, drop_last=True)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=valid_dataset.collate_fn, drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-01T07:07:08.405404Z","iopub.execute_input":"2022-08-01T07:07:08.405761Z","iopub.status.idle":"2022-08-01T07:07:08.426469Z","shell.execute_reply.started":"2022-08-01T07:07:08.405725Z","shell.execute_reply":"2022-08-01T07:07:08.425596Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def init_weights(m):\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n        nn.init.normal_(m.weight, 0, 0.02)\n        if m.bias is not None:\n            nn.init.zeros_(m.bias)\n    if isinstance(m, nn.BatchNorm2d):\n        nn.init.normal_(m.weight, 1, 0.02)\n        nn.init.zeros_(m.bias)\n        \ndef set_params(model, unfreeze):\n    for param in model.parameters():\n        param.requires_grad = unfreeze","metadata":{"execution":{"iopub.status.busy":"2022-08-01T07:07:08.427739Z","iopub.execute_input":"2022-08-01T07:07:08.428632Z","iopub.status.idle":"2022-08-01T07:07:08.443144Z","shell.execute_reply.started":"2022-08-01T07:07:08.428596Z","shell.execute_reply":"2022-08-01T07:07:08.442167Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"!pip install -qq torchsummary\nfrom torchsummary import summary","metadata":{"execution":{"iopub.status.busy":"2022-08-01T07:07:08.445050Z","iopub.execute_input":"2022-08-01T07:07:08.445958Z","iopub.status.idle":"2022-08-01T07:07:21.157053Z","shell.execute_reply.started":"2022-08-01T07:07:08.445922Z","shell.execute_reply":"2022-08-01T07:07:21.155826Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, channels=3):\n        super(Generator, self).__init__()\n\n        def downsample(in_feat, out_feat, normalize=True):\n            layers = [nn.Conv2d(in_feat, out_feat, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2))\n            return layers\n\n        def upsample(in_feat, out_feat, normalize=True):\n            layers = [nn.ConvTranspose2d(in_feat, out_feat, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n            layers.append(nn.ReLU())\n            return layers\n\n        self.model = nn.Sequential(\n            *downsample(channels, 64, normalize=False),\n            *downsample(64, 64),\n            *downsample(64, 128),\n            *downsample(128, 256),\n            *downsample(256, 512),\n            nn.Conv2d(512, 4000, 1),\n            *upsample(4000, 512),\n            *upsample(512, 256),\n            *upsample(256, 128),\n            *upsample(128, 64),\n            nn.Conv2d(64, channels, 3, 1, 1),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        return self.model(x)","metadata":{"execution":{"iopub.status.busy":"2022-08-01T07:07:21.162177Z","iopub.execute_input":"2022-08-01T07:07:21.163121Z","iopub.status.idle":"2022-08-01T07:07:21.179805Z","shell.execute_reply.started":"2022-08-01T07:07:21.163075Z","shell.execute_reply":"2022-08-01T07:07:21.178541Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, channels=3):\n        super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, stride, normalize, dropout, spectral):\n            \"\"\"Returns layers of each discriminator block\"\"\"\n            if spectral:\n                layers = [nn.utils.spectral_norm(nn.Conv2d(in_filters, out_filters, 3, stride, 1), n_power_iterations=2)]\n            else:\n                layers = [nn.Conv2d(in_filters, out_filters, 3, stride, 1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            if dropout:\n                layers.append(nn.Dropout(p=0.5))\n            return layers\n\n        layers = []\n        in_filters = channels\n        for out_filters, stride, normalize, dropout, spectral in [(64, 2, False, 0, 0), (128, 2, True, 0, 0), (256, 2, True, 0, 0), (512, 1, True, 0, 0)]:\n            layers.extend(discriminator_block(in_filters, out_filters, stride, normalize, dropout, spectral))\n            in_filters = out_filters\n\n        layers.append(nn.Conv2d(out_filters, 1, 3, 1, 1))\n\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, img):\n        return self.model(img)","metadata":{"execution":{"iopub.status.busy":"2022-08-01T07:07:21.181789Z","iopub.execute_input":"2022-08-01T07:07:21.182226Z","iopub.status.idle":"2022-08-01T07:07:21.197119Z","shell.execute_reply.started":"2022-08-01T07:07:21.182188Z","shell.execute_reply":"2022-08-01T07:07:21.195848Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"generator = Generator().apply(init_weights).to(device)\ndiscriminator = Discriminator().apply(init_weights).to(device)","metadata":{"execution":{"iopub.status.busy":"2022-08-01T07:07:21.198907Z","iopub.execute_input":"2022-08-01T07:07:21.199597Z","iopub.status.idle":"2022-08-01T07:07:24.963401Z","shell.execute_reply.started":"2022-08-01T07:07:21.199558Z","shell.execute_reply":"2022-08-01T07:07:24.962410Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"summary(generator, (3, 128, 128))","metadata":{"execution":{"iopub.status.busy":"2022-08-01T07:07:24.964943Z","iopub.execute_input":"2022-08-01T07:07:24.965298Z","iopub.status.idle":"2022-08-01T07:07:30.446151Z","shell.execute_reply.started":"2022-08-01T07:07:24.965259Z","shell.execute_reply":"2022-08-01T07:07:30.444570Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"summary(discriminator, (3, 64, 64))","metadata":{"execution":{"iopub.status.busy":"2022-08-01T07:07:30.447763Z","iopub.execute_input":"2022-08-01T07:07:30.448179Z","iopub.status.idle":"2022-08-01T07:07:30.654027Z","shell.execute_reply.started":"2022-08-01T07:07:30.448139Z","shell.execute_reply":"2022-08-01T07:07:30.652984Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Losses: wgan, perceptual, mse, l1, smoothl1","metadata":{}},{"cell_type":"code","source":"adversarial_loss = nn.MSELoss()\npixelwise_loss = nn.L1Loss()\n\noptimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))  # AdamW weight_decay=1e-3, eps=1e-2\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))","metadata":{"execution":{"iopub.status.busy":"2022-08-01T07:07:30.655640Z","iopub.execute_input":"2022-08-01T07:07:30.656252Z","iopub.status.idle":"2022-08-01T07:07:30.664155Z","shell.execute_reply.started":"2022-08-01T07:07:30.656211Z","shell.execute_reply":"2022-08-01T07:07:30.663135Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def train_one_batch(batch, generator, discriminator, criterion_adv, criterion_pix, optimizer_G, optimizer_D):\n    generator.train()\n    discriminator.train()\n    \n    images, masked_images, masked_parts = batch\n    real = torch.FloatTensor(batch_size, *patch).fill_(1.0).requires_grad_(False).to(device)  # (1.0 - lambda)\n    fake = torch.FloatTensor(batch_size, *patch).fill_(0.0).requires_grad_(False).to(device)  # (lambda)\n    \n    set_params(discriminator, False)\n    optimizer_G.zero_grad()\n    gen_parts = generator(masked_images)\n    \n    gan_loss = criterion_adv(discriminator(gen_parts), real)\n    pix_loss = criterion_pix(gen_parts, masked_parts)\n    \n    loss_g = 0.001 * gan_loss + 0.999 * pix_loss\n    loss_g.backward()\n    optimizer_G.step()\n    \n    set_params(discriminator, True)\n    optimizer_D.zero_grad()\n\n    real_loss = criterion_adv(discriminator(masked_parts), real)\n    fake_loss = criterion_adv(discriminator(gen_parts.detach()), fake)\n    \n    loss_d = (real_loss + fake_loss) / 2\n    loss_d.backward()\n    optimizer_D.step()\n    \n    return loss_g.item(), loss_d.item()\n\n@torch.no_grad()\ndef validate_one_batch(batch, generator, discriminator, criterion_adv, criterion_pix):\n    generator.eval()\n    discriminator.eval()\n    \n    images, masked_images, masked_parts = batch\n    real = torch.FloatTensor(batch_size, *patch).fill_(1.0).requires_grad_(False).to(device)\n    fake = torch.FloatTensor(batch_size, *patch).fill_(0.0).requires_grad_(False).to(device)\n    \n    gen_parts = generator(masked_images)\n    \n    gan_loss = criterion_adv(discriminator(gen_parts), real)\n    pix_loss = criterion_pix(gen_parts, masked_parts)\n    \n    loss_g = 0.001 * gan_loss + 0.999 * pix_loss\n    \n    real_loss = criterion_adv(discriminator(masked_parts), real)\n    fake_loss = criterion_adv(discriminator(gen_parts.detach()), fake)\n    \n    loss_d = (real_loss + fake_loss) / 2\n    \n    return loss_g.item(), loss_d.item()\n\n@torch.no_grad()\ndef test_plot(test, generator):\n    idx = np.random.randint(len(test))\n    random_path = test[idx]\n    \n    image = Image.open(random_path)\n    image = transforms(image)\n    \n    masked_image, idx = train_dataset.apply_center_mask(image)\n    \n    generator.eval()\n    gen_part = generator(masked_image.unsqueeze(0).to(device)).squeeze(0).cpu().detach()\n    gen_image = masked_image.clone()\n    gen_image[:, idx:idx+mask_size, idx:idx+mask_size] = gen_part\n    \n    image = inverse_transforms(image)\n    masked_image = inverse_transforms(masked_image)\n    gen_image = inverse_transforms(gen_image)\n    \n    plt.figure(figsize=(10, 5))\n    plt.subplot(131)\n    plt.title('Original Image')\n    plt.imshow(image)\n    \n    plt.subplot(132)\n    plt.title('Masked Image')\n    plt.imshow(masked_image)\n    \n    plt.subplot(133)\n    plt.title('Inpainted Image')\n    plt.imshow(gen_image)\n    \n    plt.tight_layout()\n    plt.show()\n    plt.pause(0.01)","metadata":{"execution":{"iopub.status.busy":"2022-08-01T07:07:30.665594Z","iopub.execute_input":"2022-08-01T07:07:30.666152Z","iopub.status.idle":"2022-08-01T07:07:30.688254Z","shell.execute_reply.started":"2022-08-01T07:07:30.666113Z","shell.execute_reply":"2022-08-01T07:07:30.687180Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"train_d_losses, valid_d_losses = [], []\ntrain_g_losses, valid_g_losses = [], []\n\nfor epoch in range(epochs):\n    print(f'Epoch {epoch+1}/{epochs}')\n    tq_bar = tqdm(train_dataloader, total=len(train_dataloader), desc=f'Train step {epoch+1}')\n    epoch_d_losses, epoch_g_losses = [], []\n    for _, batch in enumerate(tq_bar):\n        g_loss, d_loss = train_one_batch(batch, generator, discriminator, adversarial_loss, \n                                         pixelwise_loss, optimizer_G, optimizer_D)\n        epoch_g_losses.append(g_loss)\n        epoch_d_losses.append(d_loss)\n        tq_bar.set_postfix(g_loss=np.mean(epoch_g_losses), d_loss=np.mean(epoch_d_losses))\n    train_d_losses.append(np.mean(epoch_d_losses))\n    train_g_losses.append(np.mean(epoch_g_losses))\n    \n    tq_bar = tqdm(valid_dataloader, total=len(valid_dataloader), desc=f'Validation step {epoch+1}')\n    epoch_d_losses, epoch_g_losses = [], []\n    for _, batch in enumerate(tq_bar):\n        g_loss, d_loss = validate_one_batch(batch, generator, discriminator, adversarial_loss, pixelwise_loss)\n        epoch_d_losses.append(d_loss)\n        epoch_g_losses.append(g_loss)\n        tq_bar.set_postfix(g_loss=np.mean(epoch_g_losses), d_loss=np.mean(epoch_d_losses))\n    valid_d_losses.append(np.mean(epoch_d_losses))\n    valid_g_losses.append(np.mean(epoch_g_losses))\n    \n    if (epoch+1) % 2 == 0 or (epoch+1) == epochs:\n        test_plot(test, generator)\n        checkpoint = {\n            'discriminator': discriminator,\n            'generator': generator,\n        }\n        torch.save(checkpoint, path)\n        \nplt.figure(figsize=(8, 4))\nx_axis = np.arange(1,epochs+1)\nplt.subplot(121)\nplt.title('Discriminator train/valid losses')\nplt.plot(x_axis, train_d_losses, label='train d_loss')\nplt.plot(x_axis, valid_d_losses, label='valid d_loss')\nplt.grid()\nplt.legend()\n\nplt.subplot(122)\nplt.title('Genrator train/valid losses')\nplt.plot(x_axis, train_g_losses, label='train g_loss')\nplt.plot(x_axis, valid_g_losses, label='valid_g_loss')\nplt.grid()\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-01T07:07:30.691002Z","iopub.execute_input":"2022-08-01T07:07:30.691293Z","iopub.status.idle":"2022-08-01T08:25:25.062811Z","shell.execute_reply.started":"2022-08-01T07:07:30.691267Z","shell.execute_reply":"2022-08-01T08:25:25.061879Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### [Kernel outputs visualization.](https://www.kaggle.com/code/pankratozzi/pytorch-vaegan-face-inpainting?scriptVersionId=102242524)","metadata":{}},{"cell_type":"markdown","source":"* for non-fixed masked area (when mask_size is not defined) + upsample(64,64) [3, 128, 128] -> [3, 64->128,64->128]\n* OR try UNET architecture with residual connections [3, 128, 128] -> [3, 128, 128]\n* OR maybe CycleGAN or Pix2PixGAN\n* OR, for example, this great paper: https://arxiv.org/pdf/1806.03589.pdf; [Fine&easy to understand implementation](https://github.com/csqiangwen/DeepFillv2_Pytorch)","metadata":{}}]}