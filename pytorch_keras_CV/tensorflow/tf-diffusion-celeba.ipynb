{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nimport os, shutil\nfrom glob import glob\n\nfrom tensorflow import keras\nfrom keras import layers\nfrom tensorflow_addons.optimizers import AdamW\n\nfrom PIL import Image","metadata":{"id":"eXRViErFgv9u","execution":{"iopub.status.busy":"2022-10-27T09:54:37.146713Z","iopub.execute_input":"2022-10-27T09:54:37.147204Z","iopub.status.idle":"2022-10-27T09:54:44.563059Z","shell.execute_reply.started":"2022-10-27T09:54:37.147102Z","shell.execute_reply":"2022-10-27T09:54:44.561769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Основной целью является итеративно с вещественным шагом (0-1) зашумлять изображение и производя обратный процесс нучить модель предсказывать шум (отделять шумный компонент) в изображении. Модель получает случайный шум $\\sim \\mathcal{N}(0, 1)$ на вход и итеративно устраняет шум, оставляя сгенерированное изображение.","metadata":{"id":"3hN0yaW9FH1J"}},{"cell_type":"code","source":"image = Image.open(\"../input/celeba-dataset/img_align_celeba/img_align_celeba/000001.jpg\")\nplt.imshow(image)\nnp.array(image).shape","metadata":{"id":"wGbFHnHBlA8d","outputId":"6a52c21e-bc30-4ff6-e7bb-dec6f1372975","execution":{"iopub.status.busy":"2022-10-25T09:44:03.276105Z","iopub.execute_input":"2022-10-25T09:44:03.277488Z","iopub.status.idle":"2022-10-25T09:44:03.533578Z","shell.execute_reply.started":"2022-10-25T09:44:03.277446Z","shell.execute_reply":"2022-10-25T09:44:03.532567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if not os.path.exists(\"celeba_sampled\"):\n#     os.makedirs(\"celeba_sampled\")\n\n# all_images = glob(\"../input/celeba-dataset/img_align_celeba/img_align_celeba/*.jpg\")","metadata":{"id":"myGswivyWhOL","execution":{"iopub.status.busy":"2022-10-25T09:02:42.882406Z","iopub.execute_input":"2022-10-25T09:02:42.882733Z","iopub.status.idle":"2022-10-25T09:02:42.888642Z","shell.execute_reply.started":"2022-10-25T09:02:42.882702Z","shell.execute_reply":"2022-10-25T09:02:42.887321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sampled_idx = np.random.choice(len(all_images), size=25000)\n# all_images = np.array(all_images)\n# all_images = all_images[sampled_idx]","metadata":{"id":"cdxpzo9yXuOJ","execution":{"iopub.status.busy":"2022-10-25T09:02:42.892351Z","iopub.execute_input":"2022-10-25T09:02:42.892774Z","iopub.status.idle":"2022-10-25T09:02:42.898769Z","shell.execute_reply.started":"2022-10-25T09:02:42.892740Z","shell.execute_reply":"2022-10-25T09:02:42.897894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cur = os.getcwd() + \"/\"\n\n# i = 0\n# for path in all_images:\n#     path = str(path)\n#     try:\n#         shutil.move(str(path), \"celeba_sampled/\"+str(path).split(\"/\")[-1])\n#         i+=1\n#     except FileNotFoundError:\n#         continue\n#     except OSError:\n#         continue\n# print(f\"{len(os.listdir('celeba_sampled/'))} files replaced\")","metadata":{"id":"KZgneNh8Yg03","outputId":"82db6cb2-dbca-478a-b8c8-aa6c19f11e47","execution":{"iopub.status.busy":"2022-10-25T09:02:42.900570Z","iopub.execute_input":"2022-10-25T09:02:42.901053Z","iopub.status.idle":"2022-10-25T09:02:42.906158Z","shell.execute_reply.started":"2022-10-25T09:02:42.901014Z","shell.execute_reply":"2022-10-25T09:02:42.905086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Устанавливаем гиперпараметры**","metadata":{"id":"e67z930-BJNc"}},{"cell_type":"markdown","source":"v1+: lr 1e-4, select 40000 ims, repeat 2,\n\nv2+: lr 1e-4, select 60000 ims, epochs 80, repeat 2\n\nv3+: lr 1e-3, select 40000, repeat 3 + scheduler-patience 8 epochs 80\n\n* best quality v4+: mse -> mae 40000, repeat 3, 1e-3 epochs 65, val 90%\n\nv5: =v4 (mae, epochs=65, repeat=3, nsamp=40000, lr=1e-3+scheduler) + block_depth = 3\n\nv6: best + upsample -> conv2d_transpose in upsample_block, epochs = 60","metadata":{}},{"cell_type":"code","source":"# данные\nimage_directory = \"../input/celeba-dataset/img_align_celeba/img_align_celeba/\" # весь набор\n# image_directory = \"celeba_sampled/\"\n\ndataset_repetitions = 3  # выбрать 1-5, 5 для маленького набора\nnum_epochs = 60  # 50\nimage_size = 64\n\n# KID = Kernel Inception Distance, метрика для измерения качества сгенерированных изображений по отношению к оригинальным\nkid_image_size = 75  # размер изображения для подачи в Inception (она генерирует фичи для вычисления метрики)\nkid_diffusion_steps = 5  #  количество итераций генерации: чем выше, тем точнее измерение\nplot_diffusion_steps = 30  # (20) то же самое, но используется во время отрисовки результатов в процессе обучения,  >> повышает качетсво и осмысленность\n\n# sampling\nmin_signal_rate = 0.02  # минимальное и максимальное значения noise rate and signal rate: стандартное отклонение в зашумленных изображениях, \nmax_signal_rate = 0.95  # \n\n# architecture\nembedding_dims = 32  # создается позиционный embedding из дисперсии шума (компонента зашумленного изображения), подаваемого на вход сети 2м входом\nembedding_max_frequency = 1000.0\nwidths = [32, 64, 96, 128]  # количество фильтров в сверточных слоях\nblock_depth = 2  # 2, количество блоков в UNET сети, для данного набора лучше взять > 2, но железо не потянет\n\n# optimization\nbatch_size = 64\nema = 0.999  # коэффициент для использования (вычисления экспоненциальных средних весов) клона основной сети на инференсе с трансформированными весами\nlearning_rate = 1e-3  # 1e-3 by default\nweight_decay = 1e-4","metadata":{"id":"WKHD6qBJg7pv","execution":{"iopub.status.busy":"2022-10-27T09:54:44.565135Z","iopub.execute_input":"2022-10-27T09:54:44.565916Z","iopub.status.idle":"2022-10-27T09:54:44.577247Z","shell.execute_reply.started":"2022-10-27T09:54:44.565874Z","shell.execute_reply":"2022-10-27T09:54:44.575942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_images = glob(\"../input/celeba-dataset/img_align_celeba/img_align_celeba/*.jpg\")\nsampled_idx = np.random.choice(len(all_images), size=40000)\nall_images = np.array(all_images)[sampled_idx]\nborder = int(len(all_images) * 0.9)\ntrain_images = all_images[:border]\nvalid_images = all_images[border:]","metadata":{"execution":{"iopub.status.busy":"2022-10-27T09:54:44.579504Z","iopub.execute_input":"2022-10-27T09:54:44.580461Z","iopub.status.idle":"2022-10-27T09:54:47.096535Z","shell.execute_reply.started":"2022-10-27T09:54:44.580391Z","shell.execute_reply":"2022-10-27T09:54:47.095152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_generator():\n    for path in train_images:\n        image = np.array(Image.open(path))\n        yield tf.cast(image, dtype=tf.float32)\n        \ndef valid_generator():\n    for path in valid_images:\n        image = np.array(Image.open(path))\n        yield tf.cast(image, dtype=tf.float32)","metadata":{"execution":{"iopub.status.busy":"2022-10-27T09:54:47.099487Z","iopub.execute_input":"2022-10-27T09:54:47.100061Z","iopub.status.idle":"2022-10-27T09:54:47.107951Z","shell.execute_reply.started":"2022-10-27T09:54:47.100008Z","shell.execute_reply":"2022-10-27T09:54:47.106597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Аугментация**\n\nЕсли будем применять аугментацию (что еще увеличит время обучения) - то лучше выбрать horizontal flips (повысит качетсво, не исказив результаты)","metadata":{"id":"JXcNxjR_4mod"}},{"cell_type":"code","source":"def get_augmenter(image_size, uncropped_image_size=(64,64,3)):\n    return keras.Sequential(\n        [\n            keras.Input(shape=uncropped_image_size),\n            layers.Normalization(),\n            layers.RandomFlip(mode=\"horizontal\"),\n            layers.RandomCrop(height=image_size, width=image_size),\n        ],\n        name=\"augmenter\",\n    )\n\ndef preprocess_image(data, crop_size=140):\n    # вырезаем центр изображения\n    height = 218\n    width = 178\n    image = tf.image.crop_to_bounding_box(\n        data,\n        (height - crop_size) // 2,\n        (width - crop_size) // 2,\n        crop_size,\n        crop_size,\n    )\n    # изменяем размер изображения и нормализуем\n    image = tf.image.resize(image, size=[image_size, image_size], method=\"bicubic\", antialias=True)\n    return tf.clip_by_value(image / 255.0, 0.0, 1.0)\n\ndef prepare_dataset(split, split_size=0.2):\n    # shuffle обязательно - требуется для вычисления KID\n    return (\n        tf.data.Dataset.from_generator(train_generator, output_shapes=(218,178,3), output_types=tf.float32)\n        .map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n        .cache()\n        .repeat(dataset_repetitions)\n        .shuffle(10 * batch_size)\n        .batch(batch_size, drop_remainder=True)\n        .prefetch(buffer_size=tf.data.AUTOTUNE)\n    )\n\ntrain_dataset = prepare_dataset(\"training\")\nval_dataset = prepare_dataset(\"validation\")","metadata":{"id":"YjZl-rl1iNcf","outputId":"8e2cae75-aa6a-4f82-ac2e-8f2add6eef8d","execution":{"iopub.status.busy":"2022-10-27T09:54:48.310813Z","iopub.execute_input":"2022-10-27T09:54:48.311233Z","iopub.status.idle":"2022-10-27T09:54:48.602489Z","shell.execute_reply.started":"2022-10-27T09:54:48.311197Z","shell.execute_reply":"2022-10-27T09:54:48.601161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for p in train_dataset.take(1):\n    print(p.shape)\n\nimage = p[np.random.randint(batch_size)].numpy()\nplt.imshow(image)","metadata":{"id":"4xlPIWq2iyj4","outputId":"b0e3de4e-2298-4bf2-ce2e-28aa5817287e","execution":{"iopub.status.busy":"2022-10-25T09:44:26.913624Z","iopub.execute_input":"2022-10-25T09:44:26.914564Z","iopub.status.idle":"2022-10-25T09:44:31.602340Z","shell.execute_reply.started":"2022-10-25T09:44:26.914523Z","shell.execute_reply":"2022-10-25T09:44:31.601318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class KID(keras.metrics.Metric):\n    def __init__(self, name, **kwargs):\n        super().__init__(name=name, **kwargs)\n        # KID - своего рода полиномиальные метрика по вычислению расстояния между двумя распределениями (изображениями)\n        # KID вычисляется на каждом пакете и усредняется по эпохе\n        self.kid_tracker = keras.metrics.Mean(name=\"kid_tracker\")\n\n        # для получения сверток изображений используем InceptionV3 без головы\n        # пиксели варьируются (0-255), далее те же преобрахзования как и в датасете\n        self.encoder = keras.Sequential(\n            [\n                keras.Input(shape=(image_size, image_size, 3)),\n                layers.Rescaling(255.0),\n                layers.Resizing(height=kid_image_size, width=kid_image_size),\n                layers.Lambda(keras.applications.inception_v3.preprocess_input),\n                keras.applications.InceptionV3(\n                    include_top=False,\n                    input_shape=(kid_image_size, kid_image_size, 3),\n                    weights=\"imagenet\",\n                ),\n                layers.GlobalAveragePooling2D(),\n            ],\n            name=\"inception_encoder\",\n        )\n\n    def polynomial_kernel(self, features_1, features_2):\n        feature_dimensions = tf.cast(tf.shape(features_1)[1], dtype=tf.float32)\n        return (features_1 @ tf.transpose(features_2) / feature_dimensions + 1.0) ** 3.0\n\n    def update_state(self, real_images, generated_images, sample_weight=None):\n        real_features = self.encoder(real_images, training=False)\n        generated_features = self.encoder(generated_images, training=False)\n\n        # вычисляем полиномиальные ядра для двух фичей (таргета и сгенерировнных)\n        kernel_real = self.polynomial_kernel(real_features, real_features)\n        kernel_generated = self.polynomial_kernel(\n            generated_features, generated_features\n        )\n        kernel_cross = self.polynomial_kernel(real_features, generated_features)\n\n        # вычисляем квадратичное максимальное \"несоответствие\" используя средние значения ядер\n        batch_size = tf.shape(real_features)[0]\n        batch_size_f = tf.cast(batch_size, dtype=tf.float32)\n        mean_kernel_real = tf.reduce_sum(kernel_real * (1.0 - tf.eye(batch_size))) / (\n            batch_size_f * (batch_size_f - 1.0)\n        )\n        mean_kernel_generated = tf.reduce_sum(\n            kernel_generated * (1.0 - tf.eye(batch_size))\n        ) / (batch_size_f * (batch_size_f - 1.0))\n        mean_kernel_cross = tf.reduce_mean(kernel_cross)\n        kid = mean_kernel_real + mean_kernel_generated - 2.0 * mean_kernel_cross\n\n        self.kid_tracker.update_state(kid)\n\n    def result(self):\n        return self.kid_tracker.result()\n\n    def reset_state(self):\n        self.kid_tracker.reset_state()","metadata":{"id":"bnahqsYToOBB","execution":{"iopub.status.busy":"2022-10-27T09:54:51.319238Z","iopub.execute_input":"2022-10-27T09:54:51.319682Z","iopub.status.idle":"2022-10-27T09:54:51.508266Z","shell.execute_reply.started":"2022-10-27T09:54:51.319645Z","shell.execute_reply":"2022-10-27T09:54:51.507162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# позиционный эмбеддинг для применения к шумовым дисперсиям, подаваемым на вход сети вместе с зашумленными изображениями\n# используем только на входе для простоты (хотя в научных работах данное преобразование применяется несколько раз при проходе по сети)\ndef sinusoidal_embedding(x):\n    embedding_min_frequency = 1.0\n    frequencies = tf.exp(\n        tf.linspace(\n            tf.math.log(embedding_min_frequency),\n            tf.math.log(embedding_max_frequency),\n            embedding_dims // 2,\n        )\n    )\n    angular_speeds = 2.0 * np.pi * frequencies\n    embeddings = tf.concat(\n        [tf.sin(angular_speeds * x), tf.cos(angular_speeds * x)], axis=3\n    )\n    return embeddings\n\n# блок обратной связи для UNET\ndef ResidualBlock(width):\n    def apply(x):\n        input_width = x.shape[3]\n        if input_width == width:\n            residual = x\n        else:\n            residual = layers.Conv2D(width, kernel_size=1)(x)\n        x = layers.BatchNormalization(center=False, scale=False)(x)  # ввиду дальнейших сверток обучаемые параметры определять не требуется\n        x = layers.Conv2D(\n            width, kernel_size=3, padding=\"same\", activation=keras.activations.swish\n        )(x)\n        x = layers.Conv2D(width, kernel_size=3, padding=\"same\")(x)\n        x = layers.Add()([x, residual])\n        return x\n\n    return apply\n\n# блок левой ветки\ndef DownBlock(width, block_depth):\n    def apply(x):\n        x, skips = x\n        for _ in range(block_depth):\n            x = ResidualBlock(width)(x)\n            skips.append(x)\n        x = layers.AveragePooling2D(pool_size=2)(x)\n        return x\n\n    return apply\n\n# блок правой ветки\ndef UpBlock(width, block_depth):\n    def apply(x):\n        x, skips = x\n        x = layers.UpSampling2D(size=2, interpolation=\"bilinear\")(x)\n        # x = layers.Conv2DTranspose(width, 3, strides=2, padding=\"same\", activation=keras.activations.swish)(x)  # activation ?\n        for _ in range(block_depth):\n            x = layers.Concatenate()([x, skips.pop()])\n            x = ResidualBlock(width)(x)\n        return x\n\n    return apply\n\n# архитектура UNET в сборе\ndef get_network(image_size, widths, block_depth):\n    noisy_images = keras.Input(shape=(image_size, image_size, 3))\n    noise_variances = keras.Input(shape=(1, 1, 1))\n\n    e = layers.Lambda(sinusoidal_embedding)(noise_variances)\n    e = layers.UpSampling2D(size=image_size, interpolation=\"nearest\")(e)\n\n    x = layers.Conv2D(widths[0], kernel_size=1)(noisy_images)\n    x = layers.Concatenate()([x, e])\n\n    skips = []\n    for width in widths[:-1]:\n        x = DownBlock(width, block_depth)([x, skips])\n\n    for _ in range(block_depth):\n        x = ResidualBlock(widths[-1])(x)\n\n    for width in reversed(widths[:-1]):\n        x = UpBlock(width, block_depth)([x, skips])\n\n    x = layers.Conv2D(3, kernel_size=1, kernel_initializer=\"zeros\")(x)  # предсказание на первых этапах будет средним по таргету, лосс = 1\n\n    return keras.Model([noisy_images, noise_variances], x, name=\"residual_unet\")","metadata":{"id":"bccz_xODsw-N","execution":{"iopub.status.busy":"2022-10-27T09:54:52.338443Z","iopub.execute_input":"2022-10-27T09:54:52.338930Z","iopub.status.idle":"2022-10-27T09:54:52.361772Z","shell.execute_reply.started":"2022-10-27T09:54:52.338891Z","shell.execute_reply":"2022-10-27T09:54:52.359472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DiffusionModel(keras.Model):\n    def __init__(self, image_size, widths, block_depth, augmenter):\n        super().__init__()\n\n        self.augmenter = augmenter\n        self.normalizer = layers.Normalization()  # слой нормализации\n        self.network = get_network(image_size, widths, block_depth)  # сеть UNET - основа\n        self.ema_network = keras.models.clone_model(self.network)  # сеть для инфереснса с трансформированными весами основной сети\n\n    def compile(self, **kwargs):  # метрика\n        super().compile(**kwargs)\n\n        self.noise_loss_tracker = keras.metrics.Mean(name=\"n_loss\")\n        self.image_loss_tracker = keras.metrics.Mean(name=\"i_loss\")\n        self.kid = KID(name=\"kid\")\n\n    @property\n    def metrics(self):\n        return [self.noise_loss_tracker, self.image_loss_tracker, self.kid]\n\n    def denormalize(self, images):\n        # преобразуем пиксели к диапазонк (0-1)\n        images = self.augmenter.layers[0].mean + (\n            images * self.augmenter.layers[0].variance ** 0.5\n        )\n        # images = self.normalizer.mean + images * self.normalizer.variance**0.5  # без аугментации\n        return tf.clip_by_value(images, 0.0, 1.0)\n\n    def diffusion_schedule(self, diffusion_times):\n        # возвращает на заданном шаге уровень шума и сигнала\n        start_angle = tf.acos(max_signal_rate)\n        end_angle = tf.acos(min_signal_rate)\n\n        diffusion_angles = start_angle + diffusion_times * (end_angle - start_angle)\n\n        # углы - вычисляем оценку шума и сигнала (их сумма квадратов дает 1 по основному тригонометрическому правилу)\n        signal_rates = tf.cos(diffusion_angles)\n        noise_rates = tf.sin(diffusion_angles)\n\n        return noise_rates, signal_rates\n\n    def denoise(self, noisy_images, noise_rates, signal_rates, training):\n        # экспоненциальные скользящие средние весов используются при инференсе\n        if training:\n            network = self.network\n        else:\n            network = self.ema_network\n\n        # предсказываем шум и используем его для вычисления изображения\n        pred_noises = network([noisy_images, noise_rates**2], training=training)\n        pred_images = (noisy_images - noise_rates * pred_noises) / signal_rates\n\n        return pred_noises, pred_images\n\n    def reverse_diffusion(self, initial_noise, diffusion_steps):\n        # обратный процесс - дешумизация\n        num_images = initial_noise.shape[0]\n        step_size = 1.0 / diffusion_steps\n\n        # на начальном этапе зашумленное изображение - это чистый шум\n        # однако уровень полезного сигнала данного изображение = min_signal_rate\n        next_noisy_images = initial_noise\n        for step in range(diffusion_steps):\n            noisy_images = next_noisy_images\n\n            # разделяем зашумленное изображение на полезный сигнал и шум\n            diffusion_times = tf.ones((num_images, 1, 1, 1)) - step * step_size\n            noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n            pred_noises, pred_images = self.denoise(\n                noisy_images, noise_rates, signal_rates, training=False\n            )\n\n            # получаем предсказаныне компоненты (шум-сигнал) используя уровень шума и сигнала на следующем шаге\n            # используем новое полученное зашумленное изображение на следующем шаге (изображение стало чуть чище)\n            next_diffusion_times = diffusion_times - step_size\n            next_noise_rates, next_signal_rates = self.diffusion_schedule(\n                next_diffusion_times\n            )\n            next_noisy_images = (\n                next_signal_rates * pred_images + next_noise_rates * pred_noises\n            )\n\n        return pred_images\n\n    def generate(self, num_images, diffusion_steps):\n        # шум -> изображение -> получение денормализованных значений пикселей\n        initial_noise = tf.random.normal(shape=(num_images, image_size, image_size, 3))\n        generated_images = self.reverse_diffusion(initial_noise, diffusion_steps)\n        generated_images = self.denormalize(generated_images)\n        return generated_images\n\n    def train_step(self, images):  # главный шаг обучения\n        # нормализуем изображения, чтобы получить стандартное отклонение = 1, прямо как у шума\n        images = self.augmenter(images, training=True)\n        # images = self.normalizer(images, training=True)  # без аугментации\n        noises = tf.random.normal(shape=(batch_size, image_size, image_size, 3))\n\n        # генерируем случайное количество шагов\n        diffusion_times = tf.random.uniform(\n            shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n        )\n        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)  # уровень шума и сигнала\n        # перемешиваем изображения с шумом\n        noisy_images = signal_rates * images + noise_rates * noises\n\n        with tf.GradientTape() as tape:\n            # обучаем сеть разделять изображение на шум и полезный сигнал (незашумленное изображение)\n            pred_noises, pred_images = self.denoise(\n                noisy_images, noise_rates, signal_rates, training=True\n            )\n\n            noise_loss = self.loss(noises, pred_noises)  # лосс при предсказании шума используем для обновления весов\n            image_loss = self.loss(images, pred_images)  # лосс на сгенерированном изображении используем для вычисления метрики\n\n        gradients = tape.gradient(noise_loss, self.network.trainable_weights)\n        self.optimizer.apply_gradients(zip(gradients, self.network.trainable_weights))\n\n        self.noise_loss_tracker.update_state(noise_loss)\n        self.image_loss_tracker.update_state(image_loss)\n\n        # после вычисления весов при обратном распространении ошибки трансфомрируем веса второй модели через экспоненциальное скользящее среднее\n        for weight, ema_weight in zip(self.network.weights, self.ema_network.weights):\n            ema_weight.assign(ema * ema_weight + (1 - ema) * weight)\n\n        # метрика не вычисляется на этапе оптимизации для ускорения процесса (прогнать два изображения через сверточную сеть и произвести вычисление долго)\n        return {m.name: m.result() for m in self.metrics[:-1]}\n\n    def test_step(self, images):\n        # нормализуем изображения, чтобы получить стандартное отклонение = 1, прямо как у шума\n        images = self.augmenter(images, training=False)\n        # images = self.normalizer(images, training=False)  # без аугментации\n        noises = tf.random.normal(shape=(batch_size, image_size, image_size, 3))\n\n        # генерируем случайное количество шагов\n        diffusion_times = tf.random.uniform(\n            shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n        )\n        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n        # перемешиваем изображение и шум\n        noisy_images = signal_rates * images + noise_rates * noises\n\n        # разделяем зашумленные изображения на полезный сигнал и шум\n        pred_noises, pred_images = self.denoise(\n            noisy_images, noise_rates, signal_rates, training=False\n        )\n\n        noise_loss = self.loss(noises, pred_noises)\n        image_loss = self.loss(images, pred_images)\n\n        self.image_loss_tracker.update_state(image_loss)\n        self.noise_loss_tracker.update_state(noise_loss)\n\n        # несколько раз вычисляем метрику\n        images = self.denormalize(images)\n        generated_images = self.generate(\n            num_images=batch_size, diffusion_steps=kid_diffusion_steps\n        )\n        self.kid.update_state(images, generated_images)\n\n        return {m.name: m.result() for m in self.metrics}\n\n    def plot_images(self, epoch=None, logs=None, num_rows=3, num_cols=6):\n        # функция для отрисовки сгенерированных изображений\n        generated_images = self.generate(\n            num_images=num_rows * num_cols,\n            diffusion_steps=plot_diffusion_steps,\n        )\n\n        plt.figure(figsize=(num_cols * 2.0, num_rows * 2.0))\n        for row in range(num_rows):\n            for col in range(num_cols):\n                index = row * num_cols + col\n                plt.subplot(num_rows, num_cols, index + 1)\n                plt.imshow(generated_images[index])\n                plt.axis(\"off\")\n        plt.tight_layout()\n        plt.show()\n        plt.close()","metadata":{"id":"S5Lc_RKow0Qg","execution":{"iopub.status.busy":"2022-10-27T09:54:53.982604Z","iopub.execute_input":"2022-10-27T09:54:53.983346Z","iopub.status.idle":"2022-10-27T09:54:54.015463Z","shell.execute_reply.started":"2022-10-27T09:54:53.983304Z","shell.execute_reply":"2022-10-27T09:54:54.014175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Также попробуем:\n* использовать подрезку весов;\n* mean average error вместо mean squared error\n* использовать обратную свертку вместо апсэмплинга (добавим обучаемые параметры: апсэмплинг это интерполяция, обратная свертка это обучение с весами).","metadata":{"id":"2FpnmtYEKvgZ"}},{"cell_type":"code","source":"# создаем модель\n# model = DiffusionModel(image_size, widths, block_depth,)  # без аугментации\nmodel = DiffusionModel(image_size, widths, block_depth, augmenter=get_augmenter(image_size=image_size),)\n\n\n# используем AdamW, более продвинутую версию Adam. Позволяет также динамически изменять скорость обучения для каждого параметра, а также применять\n# более продвинутую регуляризацию\nmodel.compile(\n    optimizer=AdamW(\n        learning_rate=learning_rate, weight_decay=weight_decay\n    ),\n    loss = keras.losses.mean_absolute_error\n    # loss=keras.losses.mean_squared_error,\n)\n\ncheckpoint_path = \"checkpoints/diffusion_model\"\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_path,\n    save_weights_only=True,\n    monitor=\"val_kid\",\n    mode=\"min\",\n    save_best_only=True,\n)\n\n# вычислим среднее и дисперсию тренировочного набора и используем их в нормализовочном слое сети\n# model.normalizer.adapt(train_dataset)  # без аугментации\nmodel.augmenter.layers[0].adapt(train_dataset)\n\ncallbacks = [\n        keras.callbacks.ReduceLROnPlateau(monitor=\"val_n_loss\", mode=\"min\", patience=8),  # for 3+ experiment\n        keras.callbacks.LambdaCallback(on_epoch_end=model.plot_images),\n        checkpoint_callback,\n    ]\n\nmodel.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset, callbacks=callbacks, verbose=1)","metadata":{"id":"o4I2IS_r3PVP","outputId":"deb4d041-8995-47cb-98ee-3772ccd89eaf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Вещь очень полезная на практике: например, используется в DALLE-2 для генерации картики из текста (причем, сразу две: сначала генерируется небольшое изображение, а из него большое), только сэмлится не из шума, а из текстового эмбеддинга, который в свою очередь получен из авторегрессионной модеои и еще одно diffudion модели. В них же информация (текстовый эбеддинг и эмбеддинг картинки) поступают из модели CLIP. Также они задействованы в  google prompt-to-prompt модели: генерирует картинку из текста, но при этом берется исходный текст, в него вносятся правки и на выходе получается картика с учетом нового текста.","metadata":{"id":"Z2c0L6JWh6Xs"}}]}