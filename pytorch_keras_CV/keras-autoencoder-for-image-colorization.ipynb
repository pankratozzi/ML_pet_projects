{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport cv2\n\nimport matplotlib.pyplot as plt\n\nfrom glob import glob\n\nfrom tqdm import tqdm\nimport skimage\nfrom skimage.transform import resize\nfrom skimage.color import rgb2gray, gray2rgb, rgb2lab, lab2rgb\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\nfrom keras.models import Model, load_model, Sequential\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Dense, UpSampling2D, RepeatVector, Reshape\nfrom keras.layers.core import Dropout, Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras import backend as K\n\nimport tensorflow as tf\n\nwarnings.filterwarnings('ignore', category=UserWarning, module='skimage')\nseed = 42\nnp.random.seed = seed","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-23T23:26:00.772767Z","iopub.execute_input":"2022-03-23T23:26:00.773119Z","iopub.status.idle":"2022-03-23T23:26:02.004537Z","shell.execute_reply.started":"2022-03-23T23:26:00.773064Z","shell.execute_reply":"2022-03-23T23:26:02.000242Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Setting constant variables values","metadata":{}},{"cell_type":"code","source":"IMG_WIDTH = 256\nIMG_HEIGHT = 256\nIMG_CHANNELS = 3\nINPUT_SHAPE=(IMG_HEIGHT, IMG_WIDTH, 1)\nTRAIN_PATH_GRAY = '../input/image-colorization-dataset/data/train_black/'\nTRAIN_PATH_RGB = '../input/image-colorization-dataset/data/train_color/'\nTEST_PATH_GRAY = '../input/image-colorization-dataset/data/test_black/'\nTEST_PATH_RGB = '../input/image-colorization-dataset/data/test_color/'","metadata":{"_uuid":"8f3d29f7d5609caff26b9def320e4a76611f5202","execution":{"iopub.status.busy":"2022-03-23T23:26:06.637765Z","iopub.execute_input":"2022-03-23T23:26:06.638172Z","iopub.status.idle":"2022-03-23T23:26:06.643837Z","shell.execute_reply.started":"2022-03-23T23:26:06.638101Z","shell.execute_reply":"2022-03-23T23:26:06.642430Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Getting train and test paths for inputs and target images","metadata":{}},{"cell_type":"code","source":"X_train_gray_paths = glob(TRAIN_PATH_GRAY + '/*')\nX_train_rgb_paths = glob(TRAIN_PATH_RGB + '/*')\nX_test_gray_paths = glob(TEST_PATH_GRAY + '/*')\nX_test_rgb_paths = glob(TEST_PATH_RGB + '/*')\nfor x in [X_train_gray_paths, X_train_rgb_paths, X_test_gray_paths, X_test_rgb_paths]:\n    x = sorted([str(img) for img in x])\n    print(len(x))","metadata":{"execution":{"iopub.status.busy":"2022-03-23T23:26:09.424510Z","iopub.execute_input":"2022-03-23T23:26:09.424867Z","iopub.status.idle":"2022-03-23T23:26:10.166532Z","shell.execute_reply.started":"2022-03-23T23:26:09.424808Z","shell.execute_reply":"2022-03-23T23:26:10.165593Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# checking paths to input and target\nprint(X_train_gray_paths[1000], X_train_rgb_paths[1000], sep='\\n')","metadata":{"execution":{"iopub.status.busy":"2022-03-23T19:21:38.922914Z","iopub.execute_input":"2022-03-23T19:21:38.923210Z","iopub.status.idle":"2022-03-23T19:21:38.929947Z","shell.execute_reply.started":"2022-03-23T19:21:38.923158Z","shell.execute_reply":"2022-03-23T19:21:38.928996Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Creating train and test arrays of images","metadata":{}},{"cell_type":"code","source":"X_train_gray = np.zeros((len(X_train_gray_paths), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.uint8)\nX_train_rgb = np.zeros((len(X_train_rgb_paths), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\nX_test_gray = np.zeros((len(X_test_gray_paths), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.uint8)\nX_test_rgb = np.zeros((len(X_test_rgb_paths), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\nfor x in [X_train_gray, X_train_rgb, X_test_gray, X_test_rgb]:\n    print(x.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T23:26:14.073187Z","iopub.execute_input":"2022-03-23T23:26:14.073509Z","iopub.status.idle":"2022-03-23T23:26:14.083857Z","shell.execute_reply.started":"2022-03-23T23:26:14.073456Z","shell.execute_reply":"2022-03-23T23:26:14.082880Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"img_paths = [X_train_gray_paths, X_train_rgb_paths, X_test_gray_paths, X_test_rgb_paths]\nimg_arrays = [X_train_gray, X_train_rgb, X_test_gray, X_test_rgb]\n\nj = 1\nfor paths, img_array in zip(img_paths, img_arrays):\n    for i, path in tqdm(enumerate(paths), total=len(paths), leave=False):\n        image = cv2.imread(path)\n        image = cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH))\n        if j % 2 != 0:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n            image = image.reshape(IMG_HEIGHT, IMG_WIDTH, 1)\n        else:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        img_array[i] = image\n    j += 1","metadata":{"execution":{"iopub.status.busy":"2022-03-23T23:26:16.493375Z","iopub.execute_input":"2022-03-23T23:26:16.493705Z","iopub.status.idle":"2022-03-23T23:27:57.808265Z","shell.execute_reply.started":"2022-03-23T23:26:16.493646Z","shell.execute_reply":"2022-03-23T23:27:57.807327Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nplt.subplot(121)\nplt.title('Input')\nplt.imshow(X_train_gray[0].squeeze(), cmap='gray')\nplt.subplot(122)\nplt.title('Target')\nplt.imshow(X_train_rgb[0])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T19:25:09.022485Z","iopub.execute_input":"2022-03-23T19:25:09.022987Z","iopub.status.idle":"2022-03-23T19:25:09.495233Z","shell.execute_reply.started":"2022-03-23T19:25:09.022768Z","shell.execute_reply":"2022-03-23T19:25:09.493764Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"**Train / Valid split**","metadata":{}},{"cell_type":"code","source":"X_train_rgb, X_valid_rgb = train_test_split(X_train_rgb, test_size=0.1, random_state=seed)\nX_train_gray, X_valid_gray = train_test_split(X_train_gray, test_size=0.1, random_state=seed)\nprint(f'Train RGB: {X_train_rgb.shape[0]}, train GRAY: {X_train_gray.shape[0]}')\nprint(f'Valid RGB: {X_valid_rgb.shape[0]}, valid GRAY: {X_valid_gray.shape[0]}')","metadata":{"_uuid":"0415a85b1d4897576877179bb7a6e942d803ced9","execution":{"iopub.status.busy":"2022-03-23T23:27:57.810152Z","iopub.execute_input":"2022-03-23T23:27:57.810682Z","iopub.status.idle":"2022-03-23T23:27:58.606328Z","shell.execute_reply.started":"2022-03-23T23:27:57.810463Z","shell.execute_reply":"2022-03-23T23:27:58.605560Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nplt.subplot(121)\nplt.title('Input')\nplt.imshow(X_train_gray[0].squeeze(), cmap='gray')\nplt.subplot(122)\nplt.title('Target')\nplt.imshow(X_train_rgb[0])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T19:25:30.960764Z","iopub.execute_input":"2022-03-23T19:25:30.961129Z","iopub.status.idle":"2022-03-23T19:25:31.434453Z","shell.execute_reply.started":"2022-03-23T19:25:30.961063Z","shell.execute_reply":"2022-03-23T19:25:31.433729Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# convert to float dtype\nX_train_gray = X_train_gray.astype(np.float32) / 255.\nX_train_rgb = X_train_rgb.astype(np.float32) / 255.\nX_valid_rgb = X_valid_rgb.astype(np.float32) / 255.\nX_valid_gray = X_valid_gray.astype(np.float32) / 255.\nX_test_gray = X_test_gray.astype(np.float32) / 255.\nX_test_rgb = X_test_rgb.astype(np.float32) / 255.","metadata":{"execution":{"iopub.status.busy":"2022-03-23T23:27:58.608383Z","iopub.execute_input":"2022-03-23T23:27:58.608689Z","iopub.status.idle":"2022-03-23T23:28:02.454240Z","shell.execute_reply.started":"2022-03-23T23:27:58.608640Z","shell.execute_reply":"2022-03-23T23:28:02.453421Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Create the Model\n\nThe model is a combination of an autoencoder and resnet classifier. The best an autoencoder by itself is just shade everything in a brownish tone. The model uses an resnet classifier to give the neural network an \"idea\" of what things should be colored.","metadata":{"_uuid":"eabcca55a39fca643df6ab142cd26ff5259ec384"}},{"cell_type":"code","source":"inception = InceptionResNetV2(weights=None, include_top=True)\ninception.load_weights('../input/inception-resnet-v2-weights/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5')\ninception.graph = tf.get_default_graph()","metadata":{"_uuid":"f0fda6a03d48ee04f46fc05656d009f75eabedab","execution":{"iopub.status.busy":"2022-03-23T23:28:02.455823Z","iopub.execute_input":"2022-03-23T23:28:02.456124Z","iopub.status.idle":"2022-03-23T23:28:41.051346Z","shell.execute_reply.started":"2022-03-23T23:28:02.456076Z","shell.execute_reply":"2022-03-23T23:28:41.050459Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"We are trying to generate 2 channels (green–red and blue–yellow filters) in addition to our given gray-scale image (that has 1 channel)","metadata":{}},{"cell_type":"code","source":"def Colorize():\n    embed_input = Input(shape=(1000,))\n    \n    #Encoder\n    encoder_input = Input(shape=(256, 256, 1,))\n    encoder_output = Conv2D(128, (3,3), activation='relu', padding='same',strides=1)(encoder_input)\n    encoder_output = MaxPooling2D((2, 2), padding='same')(encoder_output)\n    encoder_output = Conv2D(128, (4,4), activation='relu', padding='same')(encoder_output)\n    encoder_output = Conv2D(128, (3,3), activation='relu', padding='same',strides=1)(encoder_output)\n    encoder_output = MaxPooling2D((2, 2), padding='same')(encoder_output)\n    encoder_output = Conv2D(256, (4,4), activation='relu', padding='same')(encoder_output)\n    encoder_output = Conv2D(256, (3,3), activation='relu', padding='same',strides=1)(encoder_output)\n    encoder_output = MaxPooling2D((2, 2), padding='same')(encoder_output)\n    encoder_output = Conv2D(256, (4,4), activation='relu', padding='same')(encoder_output)\n    encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n    encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n    \n    #Fusion\n    fusion_output = RepeatVector(32 * 32)(embed_input) \n    fusion_output = Reshape(([32, 32, 1000]))(fusion_output)\n    fusion_output = concatenate([encoder_output, fusion_output], axis=3) \n    fusion_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion_output)\n    \n    #Decoder\n    decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(fusion_output)\n    decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n    decoder_output = UpSampling2D((2, 2))(decoder_output)\n    decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(decoder_output)\n    decoder_output = UpSampling2D((2, 2))(decoder_output)\n    decoder_output = Conv2D(64, (4,4), activation='relu', padding='same')(decoder_output)\n    decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n    decoder_output = Conv2D(32, (2,2), activation='relu', padding='same')(decoder_output)\n    decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n    decoder_output = UpSampling2D((2, 2))(decoder_output)\n    return Model(inputs=[encoder_input, embed_input], outputs=decoder_output)\n\nmodel = Colorize()\nmodel.compile(optimizer='adam', loss='mean_squared_error') # lr=1e-3\nmodel.summary()","metadata":{"_uuid":"18df8319e5666b79f9e2997c4babf6fcdf5d63a6","execution":{"iopub.status.busy":"2022-03-23T23:28:41.052494Z","iopub.execute_input":"2022-03-23T23:28:41.052756Z","iopub.status.idle":"2022-03-23T23:28:41.353525Z","shell.execute_reply.started":"2022-03-23T23:28:41.052711Z","shell.execute_reply":"2022-03-23T23:28:41.352623Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Prepare datasets","metadata":{"_uuid":"be004e23a3ea24ef3c6ab07bf5583e4f662c997e"}},{"cell_type":"code","source":"# Image transformer: augmentation\ntrain_datagen = ImageDataGenerator(shear_range=0.2,\n                                   zoom_range=0.2,\n                                   rotation_range=20,\n                                   horizontal_flip=True)\n\nvalid_datagen = ImageDataGenerator()\n\n#Create embedding to decide wich part of image to color\ndef create_inception_embedding(grayscaled_rgb):\n    def resize_gray(x):\n        return resize(x, (299, 299, 3), mode='constant')\n    grayscaled_rgb_resized = np.array([resize_gray(x)for x in grayscaled_rgb])\n    grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)\n    with inception.graph.as_default():\n        embed = inception.predict(grayscaled_rgb_resized)\n    return embed\n\n#Generate training data: [grayed color-images, embedding_colored_from_gray], lab_target\n# converting rgb images to gray is redundant, but it is more useful with skimage lib\n# cv2.COLOR_BGR2LAB\ndef image_train_generator(X, batch_size = 20):\n    for rgbs in train_datagen.flow(X, batch_size=batch_size):\n        X_batch = rgb2gray(rgbs) # convert colored into gray\n        grayscaled_rgb = gray2rgb(X_batch) # convert converted gray into colored\n        lab_batch = rgb2lab(rgbs) # convert colored into Lab format\n        X_batch = lab_batch[:,:,:,0] # take grascale channel as X\n        X_batch = X_batch.reshape(X_batch.shape+(1,)) # reshape X to fit model input\n        Y_batch = lab_batch[:,:,:,1:] / 128 # take 2 channels except gray as target\n        yield [X_batch, create_inception_embedding(grayscaled_rgb)], Y_batch\n\n# the same as train_datagen except augmentation and batch_size\ndef image_valid_generator(X, batch_size=8):\n    for rgbs in valid_datagen.flow(X, batch_size=batch_size):\n        X_batch = rgb2gray(rgbs)\n        grayscaled_rgb = gray2rgb(X_batch)\n        lab_batch = rgb2lab(rgbs)\n        X_batch = lab_batch[:,:,:,0]\n        X_batch = X_batch.reshape(X_batch.shape+(1,))\n        Y_batch = lab_batch[:,:,:,1:] / 128\n        yield [X_batch, create_inception_embedding(grayscaled_rgb)], Y_batch\n        \n# later we can use GT RGBS and GRAYS to compare the results in addition to test imgs\n# test -> gray + embed = predict, compare with GT","metadata":{"_uuid":"93752f041d5c365d4b0cf901154daf791dae08f0","execution":{"iopub.status.busy":"2022-03-23T23:28:41.354539Z","iopub.execute_input":"2022-03-23T23:28:41.354805Z","iopub.status.idle":"2022-03-23T23:28:41.373090Z","shell.execute_reply.started":"2022-03-23T23:28:41.354749Z","shell.execute_reply":"2022-03-23T23:28:41.372071Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Checkpoints","metadata":{"_uuid":"396c296aca3e414f269cb47359dfa65f008c979e"}},{"cell_type":"code","source":"# naive custom lr scheduler\ndef scheduler(epoch, lr):\n    if epoch < 2:\n        return lr * 0.0001\n    elif epoch >= 2 and epoch < 8:\n        return lr * 10\n    else:\n        return lr * 0.65\n\ncustom_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Link to WarmupCosineDecay Scheduler & Callback\n[https://www.dlology.com/blog/bag-of-tricks-for-image-classification-with-convolutional-neural-networks-in-keras/](http://)","metadata":{}},{"cell_type":"code","source":"# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5,\n                                            min_lr=0.00001)\nfilepath = \"color.h5\"\ncheckpoint = ModelCheckpoint(filepath,\n                             save_best_only=True,\n                             monitor='val_loss',\n                             mode='min')\n\nearly_stop = EarlyStopping(monitor='val_loss', \n                           patience=10,\n                           restore_best_weights=True,\n                           mode='min',\n                           )\n\nmodel_callbacks = [learning_rate_reduction, checkpoint, early_stop]\n# as a way to improve try LearningRateScheduler with custom scheduler function\n# e.g. take some warmup step with low lr, then increase it significantly\n# and after decay lr monotoniously","metadata":{"_uuid":"3e212da6f4b8651fde538066174d860aefe9c5b2","execution":{"iopub.status.busy":"2022-03-23T23:31:44.991577Z","iopub.execute_input":"2022-03-23T23:31:44.992098Z","iopub.status.idle":"2022-03-23T23:31:44.999076Z","shell.execute_reply.started":"2022-03-23T23:31:44.992032Z","shell.execute_reply":"2022-03-23T23:31:44.997950Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Train the Model","metadata":{"_uuid":"a799ebe869de0abefc484deb5b5b80ddaf8ca916"}},{"cell_type":"code","source":"BATCH_SIZE = 20\nmodel.fit_generator(image_train_generator(X_train_rgb, BATCH_SIZE),\n            epochs=30,\n            validation_data=image_valid_generator(X_valid_rgb, 8),\n            validation_steps=X_valid_rgb.shape[0]//8,\n            verbose=1,\n            steps_per_epoch=X_train_rgb.shape[0]//BATCH_SIZE,\n            callbacks=model_callbacks\n                   )","metadata":{"_uuid":"2f2bbd8c98f30c143fba98c77502b482c4281475","execution":{"iopub.status.busy":"2022-03-23T20:20:27.937535Z","iopub.execute_input":"2022-03-23T20:20:27.937884Z","iopub.status.idle":"2022-03-23T23:24:04.482641Z","shell.execute_reply.started":"2022-03-23T20:20:27.937820Z","shell.execute_reply":"2022-03-23T23:24:04.481824Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"model.load_weights(filepath)  # after training","metadata":{"execution":{"iopub.status.busy":"2022-03-23T23:31:55.004838Z","iopub.execute_input":"2022-03-23T23:31:55.005171Z","iopub.status.idle":"2022-03-23T23:31:57.239607Z","shell.execute_reply.started":"2022-03-23T23:31:55.005116Z","shell.execute_reply":"2022-03-23T23:31:57.238271Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model.save(filepath)\nmodel.save_weights(\"Color_Weights.h5\")","metadata":{"_uuid":"f710bcb0ebd374f0e2117732af5e03a7d2b850a2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluate on test images","metadata":{"_uuid":"731b1d623a4835b659ab77377cbc5c00c5d7a401"}},{"cell_type":"code","source":"sample = X_test_rgb[:50]\ncolor_me = gray2rgb(rgb2gray(sample))\ncolor_me_embed = create_inception_embedding(color_me)\ncolor_me = rgb2lab(color_me)[:,:,:,0]\ncolor_me = color_me.reshape(color_me.shape+(1,))\n\noutput = model.predict([color_me, color_me_embed])\noutput = output * 128\n\ndecoded_imgs = np.zeros((len(output),256, 256, 3))\n\nfor i in tqdm(range(len(output)), total=len(output), leave=False):\n    cur = np.zeros((256, 256, 3))\n    cur[:,:,0] = color_me[i][:,:,0]\n    cur[:,:,1:] = output[i]\n    decoded_imgs[i] = lab2rgb(cur)","metadata":{"_uuid":"fe0347e93e23d2af2abb3411e391d31461a41de8","execution":{"iopub.status.busy":"2022-03-23T23:32:20.877237Z","iopub.execute_input":"2022-03-23T23:32:20.877586Z","iopub.status.idle":"2022-03-23T23:32:34.286684Z","shell.execute_reply.started":"2022-03-23T23:32:20.877526Z","shell.execute_reply":"2022-03-23T23:32:34.285766Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"#### Plot random test image","metadata":{}},{"cell_type":"code","source":"idx = np.random.randint(50)\n\nplt.figure(figsize=(8, 16))\n\nplt.subplot(311)\nplt.title('Grayscale')\nplt.imshow(X_test_gray[idx].squeeze(), cmap='gray')\nplt.axis('off')\n \nplt.subplot(312)\nplt.title('Colorized')\nplt.imshow(decoded_imgs[idx].reshape(256, 256,3))\nplt.axis('off')\n    \nplt.subplot(313)\nplt.title('Original')\nplt.imshow(X_test_rgb[idx])\nplt.axis('off')\n \nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"46a6262281eabf011fece01610efc392f090e6ff","execution":{"iopub.status.busy":"2022-03-23T23:33:38.011311Z","iopub.execute_input":"2022-03-23T23:33:38.011626Z","iopub.status.idle":"2022-03-23T23:33:38.580217Z","shell.execute_reply.started":"2022-03-23T23:33:38.011570Z","shell.execute_reply":"2022-03-23T23:33:38.579474Z"},"trusted":true},"execution_count":19,"outputs":[]}]}