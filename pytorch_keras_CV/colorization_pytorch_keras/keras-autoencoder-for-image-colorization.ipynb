{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport cv2\n\nimport matplotlib.pyplot as plt\n\nfrom glob import glob\n\nfrom tqdm import tqdm\nimport skimage\nfrom skimage.transform import resize\nfrom skimage.color import rgb2gray, gray2rgb, rgb2lab, lab2rgb\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\nfrom keras.models import Model, load_model, Sequential\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Dense, UpSampling2D, RepeatVector, Reshape\nfrom keras.layers.core import Dropout, Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras import backend as K\n\nimport tensorflow as tf\nimport keras\n\nwarnings.filterwarnings('ignore', category=UserWarning, module='skimage')\nseed = 42\nnp.random.seed = seed","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-24T06:50:32.909746Z","iopub.execute_input":"2022-03-24T06:50:32.910047Z","iopub.status.idle":"2022-03-24T06:50:32.919978Z","shell.execute_reply.started":"2022-03-24T06:50:32.909988Z","shell.execute_reply":"2022-03-24T06:50:32.919144Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"Setting constant variables values","metadata":{}},{"cell_type":"code","source":"IMG_WIDTH = 256\nIMG_HEIGHT = 256\nIMG_CHANNELS = 3\nINPUT_SHAPE=(IMG_HEIGHT, IMG_WIDTH, 1)\nTRAIN_PATH_GRAY = '../input/image-colorization-dataset/data/train_black/'\nTRAIN_PATH_RGB = '../input/image-colorization-dataset/data/train_color/'\nTEST_PATH_GRAY = '../input/image-colorization-dataset/data/test_black/'\nTEST_PATH_RGB = '../input/image-colorization-dataset/data/test_color/'","metadata":{"_uuid":"8f3d29f7d5609caff26b9def320e4a76611f5202","execution":{"iopub.status.busy":"2022-03-24T06:33:50.626882Z","iopub.execute_input":"2022-03-24T06:33:50.627193Z","iopub.status.idle":"2022-03-24T06:33:50.632319Z","shell.execute_reply.started":"2022-03-24T06:33:50.627138Z","shell.execute_reply":"2022-03-24T06:33:50.631559Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Getting train and test paths for inputs and target images","metadata":{}},{"cell_type":"code","source":"X_train_gray_paths = glob(TRAIN_PATH_GRAY + '/*')\nX_train_rgb_paths = glob(TRAIN_PATH_RGB + '/*')\nX_test_gray_paths = glob(TEST_PATH_GRAY + '/*')\nX_test_rgb_paths = glob(TEST_PATH_RGB + '/*')\nfor x in [X_train_gray_paths, X_train_rgb_paths, X_test_gray_paths, X_test_rgb_paths]:\n    x = sorted([str(img) for img in x])\n    print(len(x))","metadata":{"execution":{"iopub.status.busy":"2022-03-24T06:33:53.581766Z","iopub.execute_input":"2022-03-24T06:33:53.582461Z","iopub.status.idle":"2022-03-24T06:33:54.742016Z","shell.execute_reply.started":"2022-03-24T06:33:53.582396Z","shell.execute_reply":"2022-03-24T06:33:54.741242Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# checking paths to input and target\nprint(X_train_gray_paths[1000], X_train_rgb_paths[1000], sep='\\n')","metadata":{"execution":{"iopub.status.busy":"2022-03-23T19:21:38.922914Z","iopub.execute_input":"2022-03-23T19:21:38.923210Z","iopub.status.idle":"2022-03-23T19:21:38.929947Z","shell.execute_reply.started":"2022-03-23T19:21:38.923158Z","shell.execute_reply":"2022-03-23T19:21:38.928996Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Creating train and test arrays of images","metadata":{}},{"cell_type":"code","source":"X_train_gray = np.zeros((len(X_train_gray_paths), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.uint8)\nX_train_rgb = np.zeros((len(X_train_rgb_paths), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\nX_test_gray = np.zeros((len(X_test_gray_paths), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.uint8)\nX_test_rgb = np.zeros((len(X_test_rgb_paths), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\nfor x in [X_train_gray, X_train_rgb, X_test_gray, X_test_rgb]:\n    print(x.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T06:33:58.242117Z","iopub.execute_input":"2022-03-24T06:33:58.242451Z","iopub.status.idle":"2022-03-24T06:33:58.251825Z","shell.execute_reply.started":"2022-03-24T06:33:58.242392Z","shell.execute_reply":"2022-03-24T06:33:58.250966Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"img_paths = [X_train_gray_paths, X_train_rgb_paths, X_test_gray_paths, X_test_rgb_paths]\nimg_arrays = [X_train_gray, X_train_rgb, X_test_gray, X_test_rgb]\n\nj = 1\nfor paths, img_array in zip(img_paths, img_arrays):\n    for i, path in tqdm(enumerate(paths), total=len(paths), leave=False):\n        image = cv2.imread(path)\n        image = cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH))\n        if j % 2 != 0:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n            image = image.reshape(IMG_HEIGHT, IMG_WIDTH, 1)\n        else:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        img_array[i] = image\n    j += 1","metadata":{"execution":{"iopub.status.busy":"2022-03-24T06:34:01.564230Z","iopub.execute_input":"2022-03-24T06:34:01.564539Z","iopub.status.idle":"2022-03-24T06:35:52.105922Z","shell.execute_reply.started":"2022-03-24T06:34:01.564486Z","shell.execute_reply":"2022-03-24T06:35:52.105074Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nplt.subplot(121)\nplt.title('Input')\nplt.imshow(X_train_gray[0].squeeze(), cmap='gray')\nplt.subplot(122)\nplt.title('Target')\nplt.imshow(X_train_rgb[0])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T19:25:09.022485Z","iopub.execute_input":"2022-03-23T19:25:09.022987Z","iopub.status.idle":"2022-03-23T19:25:09.495233Z","shell.execute_reply.started":"2022-03-23T19:25:09.022768Z","shell.execute_reply":"2022-03-23T19:25:09.493764Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"**Train / Valid split**","metadata":{}},{"cell_type":"code","source":"X_train_rgb, X_valid_rgb = train_test_split(X_train_rgb, test_size=0.1, random_state=seed)\nX_train_gray, X_valid_gray = train_test_split(X_train_gray, test_size=0.1, random_state=seed)\nprint(f'Train RGB: {X_train_rgb.shape[0]}, train GRAY: {X_train_gray.shape[0]}')\nprint(f'Valid RGB: {X_valid_rgb.shape[0]}, valid GRAY: {X_valid_gray.shape[0]}')","metadata":{"_uuid":"0415a85b1d4897576877179bb7a6e942d803ced9","execution":{"iopub.status.busy":"2022-03-24T06:35:52.107638Z","iopub.execute_input":"2022-03-24T06:35:52.107942Z","iopub.status.idle":"2022-03-24T06:35:52.870939Z","shell.execute_reply.started":"2022-03-24T06:35:52.107892Z","shell.execute_reply":"2022-03-24T06:35:52.870228Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nplt.subplot(121)\nplt.title('Input')\nplt.imshow(X_train_gray[0].squeeze(), cmap='gray')\nplt.subplot(122)\nplt.title('Target')\nplt.imshow(X_train_rgb[0])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T19:25:30.960764Z","iopub.execute_input":"2022-03-23T19:25:30.961129Z","iopub.status.idle":"2022-03-23T19:25:31.434453Z","shell.execute_reply.started":"2022-03-23T19:25:30.961063Z","shell.execute_reply":"2022-03-23T19:25:31.433729Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# convert to float dtype\nX_train_gray = X_train_gray.astype(np.float32) / 255.\nX_train_rgb = X_train_rgb.astype(np.float32) / 255.\nX_valid_rgb = X_valid_rgb.astype(np.float32) / 255.\nX_valid_gray = X_valid_gray.astype(np.float32) / 255.\nX_test_gray = X_test_gray.astype(np.float32) / 255.\nX_test_rgb = X_test_rgb.astype(np.float32) / 255.","metadata":{"execution":{"iopub.status.busy":"2022-03-24T06:35:52.873505Z","iopub.execute_input":"2022-03-24T06:35:52.874013Z","iopub.status.idle":"2022-03-24T06:35:56.939419Z","shell.execute_reply.started":"2022-03-24T06:35:52.873768Z","shell.execute_reply":"2022-03-24T06:35:56.938598Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Create the Model\n\nThe model is a combination of an autoencoder and resnet classifier. The best an autoencoder by itself is just shade everything in a brownish tone. The model uses an resnet classifier to give the neural network an \"idea\" of what things should be colored.","metadata":{"_uuid":"eabcca55a39fca643df6ab142cd26ff5259ec384"}},{"cell_type":"code","source":"inception = InceptionResNetV2(weights=None, include_top=True)\ninception.load_weights('../input/inception-resnet-v2-weights/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5')\ninception.graph = tf.get_default_graph()","metadata":{"_uuid":"f0fda6a03d48ee04f46fc05656d009f75eabedab","execution":{"iopub.status.busy":"2022-03-24T06:35:56.941249Z","iopub.execute_input":"2022-03-24T06:35:56.941550Z","iopub.status.idle":"2022-03-24T06:36:34.764649Z","shell.execute_reply.started":"2022-03-24T06:35:56.941495Z","shell.execute_reply":"2022-03-24T06:36:34.763854Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"as further investigation we may use Efficientnet_b0 as a classifier\nNOTE: preprocess_input method is built-in, but model expects float input in\nrange [0,255], so we must not devide arrays by 255\nthat is, we may have difficulties with skimage as it may expect float values in\nrange [-1,1], replace it by cv2 transformations, the more complex thing is that\nautoencoder model uses Tanh activation to get output in range [-1,1]\npossibly we may need some simple scaling Layer\n[https://keras.io/api/applications/efficientnet/](http://)","metadata":{}},{"cell_type":"markdown","source":"We are trying to generate 2 channels (green–red and blue–yellow filters) in addition to our given gray-scale image (that has 1 channel)","metadata":{}},{"cell_type":"code","source":"def Colorize():\n    embed_input = Input(shape=(1000,))\n    \n    #Encoder\n    encoder_input = Input(shape=(256, 256, 1,))\n    encoder_output = Conv2D(128, (3,3), activation='relu', padding='same',strides=1)(encoder_input)\n    encoder_output = MaxPooling2D((2, 2), padding='same')(encoder_output)\n    encoder_output = Conv2D(128, (4,4), activation='relu', padding='same')(encoder_output)\n    encoder_output = Conv2D(128, (3,3), activation='relu', padding='same',strides=1)(encoder_output)\n    encoder_output = MaxPooling2D((2, 2), padding='same')(encoder_output)\n    encoder_output = Conv2D(256, (4,4), activation='relu', padding='same')(encoder_output)\n    encoder_output = Conv2D(256, (3,3), activation='relu', padding='same',strides=1)(encoder_output)\n    encoder_output = MaxPooling2D((2, 2), padding='same')(encoder_output)\n    encoder_output = Conv2D(256, (4,4), activation='relu', padding='same')(encoder_output)\n    encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n    encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n    \n    #Fusion\n    fusion_output = RepeatVector(32 * 32)(embed_input) \n    fusion_output = Reshape(([32, 32, 1000]))(fusion_output)\n    fusion_output = concatenate([encoder_output, fusion_output], axis=3) \n    fusion_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion_output)\n    \n    #Decoder\n    decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(fusion_output)\n    decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n    decoder_output = UpSampling2D((2, 2))(decoder_output)\n    decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(decoder_output)\n    decoder_output = UpSampling2D((2, 2))(decoder_output)\n    decoder_output = Conv2D(64, (4,4), activation='relu', padding='same')(decoder_output)\n    decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n    decoder_output = Conv2D(32, (2,2), activation='relu', padding='same')(decoder_output)\n    decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n    decoder_output = UpSampling2D((2, 2))(decoder_output)\n    return Model(inputs=[encoder_input, embed_input], outputs=decoder_output)\n\nmodel = Colorize()\n\n# two ways: 1) SGD more slow but more numericaly stable, also needed for lr warmup\n# 2) usual adam optimizer\n# opt = keras.optimizers.SGD(momentum=0.5)\n# model.compile(optimizer=opt, loss='mean_squared_error')\n\nopt = keras.optimizers.Adam(lr=2e-4)\nmodel.compile(optimizer=opt, loss='mean_squared_error') # default lr=1e-3\nmodel.summary()","metadata":{"_uuid":"18df8319e5666b79f9e2997c4babf6fcdf5d63a6","execution":{"iopub.status.busy":"2022-03-24T10:01:40.323098Z","iopub.execute_input":"2022-03-24T10:01:40.323395Z","iopub.status.idle":"2022-03-24T10:01:40.603312Z","shell.execute_reply.started":"2022-03-24T10:01:40.323340Z","shell.execute_reply":"2022-03-24T10:01:40.602617Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"### Prepare datasets","metadata":{"_uuid":"be004e23a3ea24ef3c6ab07bf5583e4f662c997e"}},{"cell_type":"code","source":"# Image transformer: augmentation\ntrain_datagen = ImageDataGenerator(shear_range=0.2,\n                                   zoom_range=0.2,\n                                   rotation_range=20,\n                                   horizontal_flip=True)\n\nvalid_datagen = ImageDataGenerator()\n\n#Create embedding to decide wich part of image to color\ndef create_inception_embedding(grayscaled_rgb):\n    def resize_gray(x):\n        return resize(x, (299, 299, 3), mode='constant')\n    grayscaled_rgb_resized = np.array([resize_gray(x)for x in grayscaled_rgb])\n    grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)\n    with inception.graph.as_default():\n        embed = inception.predict(grayscaled_rgb_resized)\n    return embed\n\n#Generate training data: [grayed color-images, embedding_colored_from_gray], lab_target\n# converting rgb images to gray is redundant, but it is more useful with skimage lib\n# cv2.COLOR_BGR2LAB\ndef image_train_generator(X, batch_size = 20):\n    for rgbs in train_datagen.flow(X, batch_size=batch_size):\n        X_batch = rgb2gray(rgbs) # convert colored into gray\n        grayscaled_rgb = gray2rgb(X_batch) # convert converted gray into colored\n        lab_batch = rgb2lab(rgbs) # convert colored into Lab format\n        X_batch = lab_batch[:,:,:,0] # take grascale channel as X\n        X_batch = X_batch.reshape(X_batch.shape+(1,)) # reshape X to fit model input\n        Y_batch = lab_batch[:,:,:,1:] / 128 # take 2 channels except gray as target\n        yield [X_batch, create_inception_embedding(grayscaled_rgb)], Y_batch\n\n# the same as train_datagen except augmentation and batch_size\ndef image_valid_generator(X, batch_size=8):\n    for rgbs in valid_datagen.flow(X, batch_size=batch_size):\n        X_batch = rgb2gray(rgbs)\n        grayscaled_rgb = gray2rgb(X_batch)\n        lab_batch = rgb2lab(rgbs)\n        X_batch = lab_batch[:,:,:,0]\n        X_batch = X_batch.reshape(X_batch.shape+(1,))\n        Y_batch = lab_batch[:,:,:,1:] / 128\n        yield [X_batch, create_inception_embedding(grayscaled_rgb)], Y_batch\n        \n# later we can use GT RGBS and GRAYS to compare the results in addition to test imgs\n# test -> gray + embed = predict, compare with GT","metadata":{"_uuid":"93752f041d5c365d4b0cf901154daf791dae08f0","execution":{"iopub.status.busy":"2022-03-24T10:01:50.668599Z","iopub.execute_input":"2022-03-24T10:01:50.668899Z","iopub.status.idle":"2022-03-24T10:01:50.682461Z","shell.execute_reply.started":"2022-03-24T10:01:50.668846Z","shell.execute_reply":"2022-03-24T10:01:50.681424Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"### Checkpoints and Callbacks","metadata":{"_uuid":"396c296aca3e414f269cb47359dfa65f008c979e"}},{"cell_type":"code","source":"# naive custom lr scheduler\ndef scheduler(epoch, lr):\n    if epoch < 2:\n        return lr * 0.0001\n    elif epoch >= 2 and epoch < 8:\n        return lr * 10\n    else:\n        return lr * 0.65\n\ncustom_scheduler = keras.callbacks.LearningRateScheduler(scheduler)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T06:51:10.125843Z","iopub.execute_input":"2022-03-24T06:51:10.126058Z","iopub.status.idle":"2022-03-24T06:51:10.130920Z","shell.execute_reply.started":"2022-03-24T06:51:10.126016Z","shell.execute_reply":"2022-03-24T06:51:10.130178Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"Link to WarmupCosineDecay Scheduler & Callback\n[https://www.dlology.com/blog/bag-of-tricks-for-image-classification-with-convolutional-neural-networks-in-keras/](http://)","metadata":{}},{"cell_type":"code","source":"# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5,\n                                            min_lr=0.00001)\nfilepath = \"color.h5\" # for Adam with Plateau\n#filepath = 'color2.h5' # for SGD with custom scheduler\n\ncheckpoint = ModelCheckpoint(filepath,\n                             save_best_only=True,\n                             monitor='val_loss',\n                             mode='min')\n\nearly_stop = EarlyStopping(monitor='val_loss', \n                           patience=10,\n                           restore_best_weights=True,\n                           mode='min',\n                           )\n\nmodel_callbacks = [learning_rate_reduction, checkpoint, early_stop]\n\n# model_callbacks = [custom_scheduler, checkpoint, early_stop] # comment to use loss monitor\n\n# LearningRateScheduler with custom scheduler function does not improve model convergence\n# we took some warmup step with low lr, then increase it significantly\n# and after decay lr monotoniously","metadata":{"_uuid":"3e212da6f4b8651fde538066174d860aefe9c5b2","execution":{"iopub.status.busy":"2022-03-24T10:02:07.120632Z","iopub.execute_input":"2022-03-24T10:02:07.120932Z","iopub.status.idle":"2022-03-24T10:02:07.127513Z","shell.execute_reply.started":"2022-03-24T10:02:07.120880Z","shell.execute_reply":"2022-03-24T10:02:07.126646Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"### Train the Model","metadata":{"_uuid":"a799ebe869de0abefc484deb5b5b80ddaf8ca916"}},{"cell_type":"code","source":"BATCH_SIZE = 20\nmodel.fit_generator(image_train_generator(X_train_rgb, BATCH_SIZE),\n            epochs=30,\n            validation_data=image_valid_generator(X_valid_rgb, 8),\n            validation_steps=X_valid_rgb.shape[0]//8,\n            verbose=1,\n            steps_per_epoch=X_train_rgb.shape[0]//BATCH_SIZE,\n            callbacks=model_callbacks\n                   )","metadata":{"_uuid":"2f2bbd8c98f30c143fba98c77502b482c4281475","execution":{"iopub.status.busy":"2022-03-24T10:02:21.819215Z","iopub.execute_input":"2022-03-24T10:02:21.819677Z","iopub.status.idle":"2022-03-24T13:55:37.559461Z","shell.execute_reply.started":"2022-03-24T10:02:21.819453Z","shell.execute_reply":"2022-03-24T13:55:37.558746Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"model.load_weights(filepath)  # after training","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:55:37.563494Z","iopub.execute_input":"2022-03-24T13:55:37.565317Z","iopub.status.idle":"2022-03-24T13:55:39.009698Z","shell.execute_reply.started":"2022-03-24T13:55:37.565268Z","shell.execute_reply":"2022-03-24T13:55:39.008930Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"model.save(filepath)\nmodel.save_weights(\"Color_Weights.h5\")","metadata":{"_uuid":"f710bcb0ebd374f0e2117732af5e03a7d2b850a2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluate on test images","metadata":{"_uuid":"731b1d623a4835b659ab77377cbc5c00c5d7a401"}},{"cell_type":"code","source":"sample = X_test_rgb[:50]\ncolor_me = gray2rgb(rgb2gray(sample))\ncolor_me_embed = create_inception_embedding(color_me)\ncolor_me = rgb2lab(color_me)[:,:,:,0]\ncolor_me = color_me.reshape(color_me.shape+(1,))\n\noutput = model.predict([color_me, color_me_embed])\noutput = output * 128\n\ndecoded_imgs = np.zeros((len(output),256, 256, 3))\n\nfor i in tqdm(range(len(output)), total=len(output), leave=False):\n    cur = np.zeros((256, 256, 3))\n    cur[:,:,0] = color_me[i][:,:,0]\n    cur[:,:,1:] = output[i]\n    decoded_imgs[i] = lab2rgb(cur)","metadata":{"_uuid":"fe0347e93e23d2af2abb3411e391d31461a41de8","execution":{"iopub.status.busy":"2022-03-24T13:55:39.011711Z","iopub.execute_input":"2022-03-24T13:55:39.011980Z","iopub.status.idle":"2022-03-24T13:55:46.264403Z","shell.execute_reply.started":"2022-03-24T13:55:39.011936Z","shell.execute_reply":"2022-03-24T13:55:46.263634Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"#### Plot random test image","metadata":{}},{"cell_type":"code","source":"idx = np.random.randint(50)\n\nplt.figure(figsize=(8, 16))\n\nplt.subplot(311)\nplt.title('Grayscale')\nplt.imshow(X_test_gray[idx].squeeze(), cmap='gray')\nplt.axis('off')\n \nplt.subplot(312)\nplt.title('Colorized')\nplt.imshow(decoded_imgs[idx].reshape(256, 256,3))\nplt.axis('off')\n    \nplt.subplot(313)\nplt.title('Original')\nplt.imshow(X_test_rgb[idx])\nplt.axis('off')\n \nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"46a6262281eabf011fece01610efc392f090e6ff","execution":{"iopub.status.busy":"2022-03-24T13:55:46.799225Z","iopub.execute_input":"2022-03-24T13:55:46.799651Z","iopub.status.idle":"2022-03-24T13:55:47.865416Z","shell.execute_reply.started":"2022-03-24T13:55:46.799601Z","shell.execute_reply":"2022-03-24T13:55:47.864718Z"},"trusted":true},"execution_count":44,"outputs":[]}]}