{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport transformers\nimport torchvision\nimport torch.nn as nn\nfrom torchvision import transforms as T\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pack_padded_sequence\n\nimport cv2\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.autonotebook import tqdm\n\nfrom sklearn.model_selection import train_test_split\n\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n\nseed = 42","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-14T15:34:14.213267Z","iopub.execute_input":"2022-04-14T15:34:14.213521Z","iopub.status.idle":"2022-04-14T15:34:17.364701Z","shell.execute_reply.started":"2022-04-14T15:34:14.213447Z","shell.execute_reply":"2022-04-14T15:34:17.363948Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"torch.random.manual_seed(seed)\nnp.random.seed(seed)\n\ntokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n\nEMBEDDING_DIM = 300\nATTENTION_DIM = 224\nDECODER_DIM = 224\nLR = 1e-4\nEPOCHS = 50\nIMAGE_SIZE = 224\nBATCH_SIZE = 32\nPATH = r'./caption_model.pth'\nVOCAB_SIZE = tokenizer.vocab_size\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Currently using {device.upper()} device')","metadata":{"execution":{"iopub.status.busy":"2022-04-14T15:34:17.368015Z","iopub.execute_input":"2022-04-14T15:34:17.368233Z","iopub.status.idle":"2022-04-14T15:34:22.178362Z","shell.execute_reply.started":"2022-04-14T15:34:17.368209Z","shell.execute_reply":"2022-04-14T15:34:22.177657Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"path = '../input/flickr30k/'\ndata = np.zeros((158916, 3), dtype=np.object)\ni = 0\nfor line in open(path + 'captions.txt', 'r'):\n    data[i, :] = line.replace('\\n', \"\").split('|')\n    i += 1","metadata":{"execution":{"iopub.status.busy":"2022-04-14T15:34:22.179780Z","iopub.execute_input":"2022-04-14T15:34:22.180266Z","iopub.status.idle":"2022-04-14T15:34:22.552810Z","shell.execute_reply.started":"2022-04-14T15:34:22.180228Z","shell.execute_reply":"2022-04-14T15:34:22.552065Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(data=data[1:, :], columns=data[0, :])\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T15:34:22.554905Z","iopub.execute_input":"2022-04-14T15:34:22.555197Z","iopub.status.idle":"2022-04-14T15:34:22.600509Z","shell.execute_reply.started":"2022-04-14T15:34:22.555163Z","shell.execute_reply":"2022-04-14T15:34:22.599840Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_transforms = T.Compose([\n                              T.ToPILImage(),\n                              T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n                              T.RandomCrop(IMAGE_SIZE),\n                              T.ColorJitter(brightness=(0.95, 1.05),\n                                            contrast=(0.95, 1.05),\n                                            saturation=(0.98, 1.02),\n                                            hue=0.05),\n                              T.RandomHorizontalFlip(p=0.1),\n                              T.GaussianBlur(kernel_size=(1, 3), sigma=(0.1, 0.5)),\n                              T.RandomAdjustSharpness(sharpness_factor=1.2, p=0.2),\n                              T.RandomRotation(degrees=(-5, 5)),\n                              T.ToTensor(),\n                              T.Normalize(mean=[0.485, 0.456, 0.406], \n                                          std=[0.229, 0.224, 0.225])\n])\n\nvalid_transforms = T.Compose([\n                              T.ToPILImage(),\n                              T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n                              T.ToTensor(),\n                              T.Normalize(mean=[0.485, 0.456, 0.406], \n                                          std=[0.229, 0.224, 0.225])\n])\ninvTrans = T.Compose([T.Normalize(mean = [ 0., 0., 0. ],\n                                      std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n                          T.Normalize(mean = [ -0.485, -0.456, -0.406 ],\n                                      std = [ 1., 1., 1. ]),\n                          T.ToPILImage(),\n                         ])","metadata":{"execution":{"iopub.status.busy":"2022-04-14T15:34:22.601638Z","iopub.execute_input":"2022-04-14T15:34:22.602038Z","iopub.status.idle":"2022-04-14T15:34:22.612207Z","shell.execute_reply.started":"2022-04-14T15:34:22.602001Z","shell.execute_reply":"2022-04-14T15:34:22.611361Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"longest_caption_idx = df.caption_text.str.len().idxmax()\nbiggest_caption = len(df.iloc[longest_caption_idx, -1])\nMAX_SEQ_LEN = 120 # set the longest to 120","metadata":{"execution":{"iopub.status.busy":"2022-04-14T15:34:22.613555Z","iopub.execute_input":"2022-04-14T15:34:22.614040Z","iopub.status.idle":"2022-04-14T15:34:22.725954Z","shell.execute_reply.started":"2022-04-14T15:34:22.614005Z","shell.execute_reply":"2022-04-14T15:34:22.725264Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df.caption_text.str.len().hist(bins=100)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T10:48:45.097359Z","iopub.execute_input":"2022-04-14T10:48:45.097859Z","iopub.status.idle":"2022-04-14T10:48:45.845028Z","shell.execute_reply.started":"2022-04-14T10:48:45.097822Z","shell.execute_reply":"2022-04-14T10:48:45.844006Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"print(np.quantile(df.caption_text.str.len(), 0.75) + 1.5 * (np.quantile(df.caption_text.str.len(), 0.75) - np.quantile(df.caption_text.str.len(), 0.25)))\ndf.caption_text.str.len().describe()\n# stay max_seq_len as 120","metadata":{"execution":{"iopub.status.busy":"2022-04-14T10:51:36.845090Z","iopub.execute_input":"2022-04-14T10:51:36.845898Z","iopub.status.idle":"2022-04-14T10:51:37.241631Z","shell.execute_reply.started":"2022-04-14T10:51:36.845849Z","shell.execute_reply":"2022-04-14T10:51:37.241029Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"root = '../input/flickr30k/images'\n\nclass CapDataset(Dataset):\n    def __init__(self, df, root, tokenizer, transforms=None):\n        self.df = df\n        self.root = root\n        self.tokenizer = tokenizer\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, ix):\n        row = self.df.iloc[ix].squeeze()\n        id = row.image_name\n        image_path = f'{self.root}/{id}'\n\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        caption = row.caption_text\n\n        target = tokenizer(caption, \n                           return_token_type_ids=False, \n                           return_attention_mask=False, \n                           max_length=MAX_SEQ_LEN, \n                           padding=\"do_not_pad\",\n                           return_tensors=\"pt\")\n        target = target['input_ids'].squeeze()\n        target = torch.LongTensor(target)\n        return image, target, caption\n\n    def collate_fn(self, batch):\n        batch.sort(key=lambda x: len(x[1]), reverse=True)\n        images, targets, captions = zip(*batch)\n        images = torch.stack([self.transforms(image) for image in images], 0)\n        lengths = [len(tar) for tar in targets]\n        _targets = torch.zeros(len(captions), max(lengths)).long()\n        for i, tar in enumerate(targets):\n            end = lengths[i]\n            _targets[i, :end] = tar[:end] \n        return images.to(device), _targets.to(device), torch.tensor(lengths).long().to(device)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T15:34:22.727073Z","iopub.execute_input":"2022-04-14T15:34:22.727347Z","iopub.status.idle":"2022-04-14T15:34:22.738525Z","shell.execute_reply.started":"2022-04-14T15:34:22.727313Z","shell.execute_reply":"2022-04-14T15:34:22.737875Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Example","metadata":{}},{"cell_type":"code","source":"ds = CapDataset(df, root, tokenizer, train_transforms)\ndl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=ds.collate_fn, drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T11:18:01.026222Z","iopub.execute_input":"2022-04-14T11:18:01.027157Z","iopub.status.idle":"2022-04-14T11:18:01.033128Z","shell.execute_reply.started":"2022-04-14T11:18:01.027110Z","shell.execute_reply":"2022-04-14T11:18:01.032217Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"p = next(iter(dl))\nimgs, tars, lens = p\nplt.imshow(invTrans(imgs[0]), interpolation=\"bicubic\")\nprint(tokenizer.decode(tars[0]).replace('[PAD]', ''))","metadata":{"execution":{"iopub.status.busy":"2022-04-14T11:18:49.986958Z","iopub.execute_input":"2022-04-14T11:18:49.987896Z","iopub.status.idle":"2022-04-14T11:18:51.418017Z","shell.execute_reply.started":"2022-04-14T11:18:49.987834Z","shell.execute_reply":"2022-04-14T11:18:51.417134Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"### End example","metadata":{}},{"cell_type":"code","source":"train, test = train_test_split(df, test_size=0.01, shuffle=True, random_state=seed)\ntrain, valid = train_test_split(train, test_size=0.1, shuffle=True, random_state=seed)\ntrain.reset_index(drop=True, inplace=True)\nvalid.reset_index(drop=True, inplace=True)\ntest.reset_index(drop=True, inplace=True)\nprint(f'Train size: {train.shape[0]}, valid size: {valid.shape[0]}, test size: {test.shape[0]}')","metadata":{"execution":{"iopub.status.busy":"2022-04-14T15:34:22.739698Z","iopub.execute_input":"2022-04-14T15:34:22.740089Z","iopub.status.idle":"2022-04-14T15:34:22.796866Z","shell.execute_reply.started":"2022-04-14T15:34:22.740037Z","shell.execute_reply":"2022-04-14T15:34:22.796137Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_dataset = CapDataset(train, root, tokenizer, train_transforms)\nvalid_dataset = CapDataset(valid, root, tokenizer, valid_transforms)\ntest_dataset = CapDataset(test, root, tokenizer, valid_transforms)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=train_dataset.collate_fn, drop_last=True)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=valid_dataset.collate_fn, drop_last=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE//4, shuffle=False, collate_fn=test_dataset.collate_fn)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T15:34:22.798269Z","iopub.execute_input":"2022-04-14T15:34:22.798695Z","iopub.status.idle":"2022-04-14T15:34:22.805805Z","shell.execute_reply.started":"2022-04-14T15:34:22.798655Z","shell.execute_reply":"2022-04-14T15:34:22.805093Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, encoded_image_size=14):\n        super(Encoder, self).__init__()\n        self.enc_image_size = encoded_image_size\n\n        resnet = torchvision.models.resnet101(pretrained=True)\n        modules = list(resnet.children())[:-2]\n        self.resnet = nn.Sequential(*modules)\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n\n        for param in self.resnet.parameters():\n            param.requires_grad = False\n\n        for child in list(self.resnet.children())[5:]:\n            for param in child.parameters():\n                param.requires_grad = True\n\n    def forward(self, images):\n        out = self.resnet(images)\n        out = self.adaptive_pool(out)\n        out = out.permute(0, 2, 3, 1)\n        return out\n\nclass Attention(nn.Module):\n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n        super(Attention, self).__init__()\n        self.encoder_att = nn.Linear(encoder_dim, attention_dim)\n        self.decoder_att = nn.Linear(decoder_dim, attention_dim)\n        self.full_att = nn.Linear(attention_dim, 1)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, encoder_out, decoder_hidden):\n        att1 = self.encoder_att(encoder_out)\n        att2 = self.decoder_att(decoder_hidden)\n        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)\n        alpha = self.softmax(att)\n        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n        return attention_weighted_encoding, alpha\n    \nclass Decoder(nn.Module):\n    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size=VOCAB_SIZE, encoder_dim=2048):\n        super(Decoder, self).__init__()\n\n        self.encoder_dim = encoder_dim\n        self.attention_dim = attention_dim\n        self.embed_dim = embed_dim\n        self.decoder_dim = decoder_dim\n        self.vocab_size = vocab_size\n\n        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)\n        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n        self.dropout = nn.Dropout(0.5)\n        self.lstm = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)\n        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n        self.sigmoid = nn.Sigmoid()\n        self.fc = nn.Linear(decoder_dim, vocab_size)\n        self.init_weights()\n\n    def init_weights(self):\n        self.embedding.weight.data.uniform_(-0.1, 0.1)\n        self.fc.bias.data.fill_(0)\n        self.fc.weight.data.uniform_(-0.1, 0.1)\n\n    def init_hidden_state(self, encoder_out):\n        mean_encoder_out = encoder_out.mean(dim=1)\n        h = self.init_h(mean_encoder_out)\n        c = self.init_c(mean_encoder_out)\n        return h, c\n    \n    def forward(self, encoder_out, encoded_captions, caption_lengths):\n        batch_size = encoder_out.size(0) # 32\n        encoder_dim = encoder_out.size(-1) # 2048\n        vocab_size = self.vocab_size # bert vocab size\n\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)\n        num_pixels = encoder_out.size(1)\n\n        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n        encoder_out = encoder_out[sort_ind]\n        encoded_captions = encoded_captions[sort_ind]\n\n        embeddings = self.embedding(encoded_captions)\n\n        h, c = self.init_hidden_state(encoder_out)\n\n        decode_lengths = (caption_lengths - 1).tolist()\n\n        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n\n        for t in range(max(decode_lengths)):\n            batch_size_t = sum([l > t for l in decode_lengths])\n            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t], h[:batch_size_t])\n            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))\n            attention_weighted_encoding = gate * attention_weighted_encoding\n            h, c = self.lstm(torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n                             (h[:batch_size_t], c[:batch_size_t]))\n            preds = self.fc(self.dropout(h))\n            predictions[:batch_size_t, t, :] = preds\n            alphas[:batch_size_t, t, :] = alpha\n        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n    \n    def predict(self, encoder_out, startseq_idx=101, endseq_idx=-1, max_len=MAX_SEQ_LEN, return_alpha=False):\n        enc_image_size = encoder_out.size(1)\n        encoder_dim = encoder_out.size(3)\n        batch_size = encoder_out.size(0)\n\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)\n        h, c = self.init_hidden_state(encoder_out)\n\n        sampled_ids = []  \n        alphas = []\n\n        prev_timestamp_words = torch.LongTensor([[startseq_idx]] * batch_size).to(device)\n        for i in range(max_len):\n            embeddings = self.embedding(prev_timestamp_words).squeeze(1)\n            awe, alpha = self.attention(encoder_out, h)\n            alpha = alpha.view(-1, enc_image_size, enc_image_size).unsqueeze(1)\n\n            gate = self.sigmoid(self.f_beta(h))\n            awe = gate * awe\n\n            h, c = self.lstm(torch.cat([embeddings, awe], dim=1), (h, c))\n            predicted_prob = self.fc(h)\n            predicted = predicted_prob.argmax(1)\n\n            sampled_ids.append(predicted)\n            alphas.append(alpha)\n\n            prev_timestamp_words = predicted.unsqueeze(1)\n\n        sampled_ids = torch.stack(sampled_ids, 1)\n        return (sampled_ids, torch.cat(alphas, 1)) if return_alpha else sampled_ids\n    \nclass CapModel(nn.Module):\n    def __init__(self, \n                 encoded_image_size=14, \n                 attention_dim=ATTENTION_DIM, \n                 embed_dim=EMBEDDING_DIM, \n                 decoder_dim=DECODER_DIM, \n                 vocab_size=VOCAB_SIZE, \n                 encoder_dim=2048):\n        super(CapModel, self).__init__()\n        self.encoder = Encoder(encoded_image_size=encoded_image_size)\n        self.decoder = Decoder(attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim)\n\n    def forward(self, images, encoded_captions, caption_lengths):\n        encoder_out = self.encoder(images)\n        decoder_out = self.decoder(encoder_out, encoded_captions, caption_lengths.unsqueeze(1))\n        return decoder_out\n\n    def predict(self, images, startseq_idx=101, endseq_idx=102, max_len=MAX_SEQ_LEN, return_alpha=False):\n        encoder_out = self.encoder(images)\n        return self.decoder.predict(encoder_out=encoder_out, startseq_idx=startseq_idx, max_len=max_len,\n                                   return_alpha=return_alpha)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T15:34:22.808756Z","iopub.execute_input":"2022-04-14T15:34:22.809026Z","iopub.status.idle":"2022-04-14T15:34:22.846661Z","shell.execute_reply.started":"2022-04-14T15:34:22.808990Z","shell.execute_reply":"2022-04-14T15:34:22.845972Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model = CapModel().to(device)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T15:34:22.847987Z","iopub.execute_input":"2022-04-14T15:34:22.848434Z","iopub.status.idle":"2022-04-14T15:34:32.416296Z","shell.execute_reply.started":"2022-04-14T15:34:22.848398Z","shell.execute_reply":"2022-04-14T15:34:32.415546Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def bleu_score_fn(method_no: int = 4, ref_type='corpus'):\n    smoothing_method = getattr(SmoothingFunction(), f'method{method_no}')\n    \n    def bleu_score_corpus(reference_corpus: list, candidate_corpus: list, n: int = 4):\n        weights = [1 / n] * n\n        return corpus_bleu(reference_corpus, candidate_corpus,\n                           smoothing_function=smoothing_method, weights=weights)\n\n    def bleu_score_sentence(reference_sentences: list, candidate_sentence: list, n: int = 4):\n        weights = [1 / n] * n\n        return sentence_bleu(reference_sentences, candidate_sentence,\n                             smoothing_function=smoothing_method, weights=weights)\n    if ref_type == 'corpus':\n        return bleu_score_corpus\n    elif ref_type == 'sentence':\n        return bleu_score_sentence\n\ndef accuracy_fn(ignore_value: int = 0):\n    def accuracy_ignoring_value(source: torch.Tensor, target: torch.Tensor):\n        mask = target != ignore_value\n        return (source[mask] == target[mask]).sum().item() / mask.sum().item()\n    return accuracy_ignoring_value\n\ndef load_model(path, device=device):\n    if device == 'cuda':\n        checkpoint = torch.load(path)\n    else:\n        checkpoint = torch.load(path, map_location=torch.device('cpu'))\n    epoch = checkpoint['epoch']\n    model = checkpoint['model']\n    optimizer = checkpoint['optimizer']\n    return epoch, model, optimizer","metadata":{"execution":{"iopub.status.busy":"2022-04-14T15:34:32.417441Z","iopub.execute_input":"2022-04-14T15:34:32.417691Z","iopub.status.idle":"2022-04-14T15:34:32.428723Z","shell.execute_reply.started":"2022-04-14T15:34:32.417660Z","shell.execute_reply":"2022-04-14T15:34:32.428078Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, patience=10, min_delta=0, path=PATH):\n        self.path = path\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = None  # np.inf to save model after first epoch\n        self.early_stop = False\n        \n    def __call__(self, val_loss, epoch=None, model=None, optimizer=None):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n        elif self.best_loss - val_loss > self.min_delta:\n            checkpoint = { \n                          'epoch': epoch,\n                          'model': model,\n                          'optimizer': optimizer,\n                          }\n            torch.save(checkpoint, self.path)\n            print(f'Model saved to: {self.path}')\n            self.best_loss = val_loss\n            self.counter = 0\n        elif self.best_loss - val_loss < self.min_delta:\n            self.counter += 1\n            print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n            if self.counter >= self.patience:\n                print('INFO: Early stopping')\n                self.early_stop = True","metadata":{"execution":{"iopub.status.busy":"2022-04-14T15:34:32.429845Z","iopub.execute_input":"2022-04-14T15:34:32.430605Z","iopub.status.idle":"2022-04-14T15:34:32.440676Z","shell.execute_reply.started":"2022-04-14T15:34:32.430570Z","shell.execute_reply":"2022-04-14T15:34:32.439829Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(ignore_index=0)\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-6)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, min_lr=1e-6, factor=0.1)\nearly = EarlyStopping()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T15:34:32.441689Z","iopub.execute_input":"2022-04-14T15:34:32.442012Z","iopub.status.idle":"2022-04-14T15:34:32.454173Z","shell.execute_reply.started":"2022-04-14T15:34:32.441972Z","shell.execute_reply":"2022-04-14T15:34:32.453456Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def train_one_batch(model, data, optimizer, criterion):\n    model.train()\n    optimizer.zero_grad()\n    images, captions, lengths = data\n    scores, caps_sorted, decode_lengths, alphas, sort_ind = model(images, captions, lengths)\n    targets = caps_sorted[:, 1:]\n\n    scores = pack_padded_sequence(scores, decode_lengths, batch_first=True)[0]\n    targets = pack_padded_sequence(targets, decode_lengths, batch_first=True)[0]\n\n    loss = criterion(scores, targets)\n    loss.backward()\n    optimizer.step()\n\n    accuracy = (torch.argmax(scores, dim=1) == targets).sum().float().item() / targets.size(0)\n\n    return loss.item(), accuracy\n\n@torch.no_grad()\ndef validate(model, data, criterion):\n    model.eval()\n    images, captions, lengths = data\n    scores, caps_sorted, decode_lengths, alphas, sort_ind = model(images, captions, lengths)\n    targets = caps_sorted[:, 1:]\n\n    scores = pack_padded_sequence(scores, decode_lengths, batch_first=True)[0]\n    targets = pack_padded_sequence(targets, decode_lengths, batch_first=True)[0]\n\n    loss = criterion(scores, targets)\n    accuracy = (torch.argmax(scores, dim=1) == targets).sum().float().item() / targets.size(0)\n\n    return loss.item(), accuracy\n\n@torch.no_grad()\ndef evaluate_model(dataloader, model, bleu_score_fn, tokenizer):\n    running_bleu = [0.0] * 5\n    model.eval()\n    for batch_idx, batch in enumerate(tqdm(dataloader, leave=False)):\n        images, captions, lengths = batch\n        outputs = model.predict(images)\n\n        captions = [[tokenizer.decode(cap).replace('[PAD]', '').split()[1:-1]] for cap in captions]\n        outputs = [tokenizer.decode(out).replace('[PAD]', '').split('[SEP]')[0].split() for out in outputs] ###\n\n        for i in (1, 2, 3, 4):\n            running_bleu[i] += bleu_score_fn(reference_corpus=captions, candidate_corpus=outputs, n=i)\n    for i in (1, 2, 3, 4):\n        running_bleu[i] /= len(dataloader)\n    return running_bleu\n\n@torch.no_grad()\ndef plot_evaluation(model, dataloader, tokenizer):\n    model.eval()\n    element = np.random.randint(BATCH_SIZE)\n    images, captions, _ = next(iter(dataloader))\n    image = images[element]\n    caption = captions[element]\n    caption = tokenizer.decode(caption).replace('[PAD]', '')\n    output = model.predict(image[None]).squeeze()\n    output = tokenizer.decode(output).replace('[PAD]', '')\n    output = output.split('[SEP]')[0]\n    plt.figure(figsize=(6,6))\n    plt.title(f'True: {caption}\\nPredicted: {output}')\n    image = invTrans(image.cpu().detach())\n    plt.imshow(image, interpolation=\"bicubic\")\n    plt.show()\n    plt.pause(0.001)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T15:34:32.455382Z","iopub.execute_input":"2022-04-14T15:34:32.455774Z","iopub.status.idle":"2022-04-14T15:34:32.475486Z","shell.execute_reply.started":"2022-04-14T15:34:32.455739Z","shell.execute_reply":"2022-04-14T15:34:32.474766Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_losses, valid_losses = [], []\ntrain_accuracy, valid_accuracy = [], []\ncorpus_bleu_score_fn = bleu_score_fn(4, 'corpus')\n\ntry:\n    ep, model, optimizer = load_model(PATH)\n    ep += 1\nexcept FileNotFoundError:\n    ep = 0\n\nfor epoch in range(ep, EPOCHS):\n\n    print(f'Epoch {epoch + 1}/{EPOCHS}')\n    epoch_train_losses, epoch_valid_losses = [], []\n    epoch_train_acc, epoch_valid_acc = [], []\n\n    for _, batch in enumerate(tqdm(train_dataloader, leave=False)):\n        loss, accuracy = train_one_batch(model, batch, optimizer, criterion)\n        epoch_train_losses.append(loss)\n        epoch_train_acc.append(accuracy)\n\n    epoch_train_loss = np.array(epoch_train_losses).mean()\n    epoch_train_accuracy = np.array(epoch_train_acc).mean()\n    train_losses.append(epoch_train_loss)\n    train_accuracy.append(epoch_train_accuracy)\n    print(f'Train loss: {epoch_train_loss:.4f}, train accuracy: {epoch_train_accuracy:.4f}')\n\n    for _, batch in enumerate(tqdm(valid_dataloader, leave=False)):\n        loss, accuracy = validate(model, batch, criterion)\n        epoch_valid_losses.append(loss)\n        epoch_valid_acc.append(accuracy)\n\n    epoch_valid_loss = np.array(epoch_valid_losses).mean()\n    epoch_valid_accuracy = np.array(epoch_valid_acc).mean()\n    valid_losses.append(epoch_valid_loss)\n    valid_accuracy.append(epoch_valid_accuracy)\n    print(f'Valid loss: {epoch_valid_loss:.4f}, valid accuracy: {epoch_valid_accuracy:.4f}')\n\n    print('-'*50)\n\n    if (epoch + 1) % 2 == 0:\n        train_bleu = evaluate_model(train_dataloader, model, corpus_bleu_score_fn, tokenizer)\n        valid_bleu = evaluate_model(valid_dataloader, model, corpus_bleu_score_fn, tokenizer)\n        print(''.join([f'train_bleu{i}: {train_bleu[i]:.4f} ' for i in (1, 4)]),\n              ''.join([f'val_bleu{i}: {valid_bleu[i]:.4f} ' for i in (1, 4)]),)\n        plot_evaluation(model, valid_dataloader, tokenizer)\n\n    scheduler.step(epoch_valid_loss)\n    early(epoch_valid_loss, epoch, model, optimizer)\n    if early.early_stop:\n        print(f'Validation loss did not improve for {early.patience} epochs. Training stopped.')\n        ep, model, optimizer = load_model(PATH)\n        break","metadata":{"execution":{"iopub.status.busy":"2022-04-14T15:34:32.478439Z","iopub.execute_input":"2022-04-14T15:34:32.479103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_evaluation(model, valid_dataloader, tokenizer)\nevaluate_model(test_dataloader, model, corpus_bleu_score_fn, tokenizer)","metadata":{},"execution_count":null,"outputs":[]}]}