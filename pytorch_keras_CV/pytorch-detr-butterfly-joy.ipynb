{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone -q https://github.com/facebookresearch/detr.git","metadata":{"execution":{"iopub.status.busy":"2022-04-04T12:59:31.914773Z","iopub.execute_input":"2022-04-04T12:59:31.915452Z","iopub.status.idle":"2022-04-04T12:59:35.053641Z","shell.execute_reply.started":"2022-04-04T12:59:31.915351Z","shell.execute_reply":"2022-04-04T12:59:35.052375Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport json\nimport numpy as np \nimport pandas as pd \nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms as T\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\n\nimport cv2\n\nimport sys\nsys.path.append('./detr/')\n\nfrom detr.models.matcher import HungarianMatcher\nfrom detr.models.detr import SetCriterion\n\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nfrom glob import glob\nfrom pprint import pprint","metadata":{"execution":{"iopub.status.busy":"2022-04-04T12:59:46.702277Z","iopub.execute_input":"2022-04-04T12:59:46.702580Z","iopub.status.idle":"2022-04-04T12:59:51.201980Z","shell.execute_reply.started":"2022-04-04T12:59:46.702539Z","shell.execute_reply":"2022-04-04T12:59:51.200671Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Currently using \"{device}\" device')","metadata":{"execution":{"iopub.status.busy":"2022-04-04T12:59:56.042903Z","iopub.execute_input":"2022-04-04T12:59:56.043530Z","iopub.status.idle":"2022-04-04T12:59:56.124213Z","shell.execute_reply.started":"2022-04-04T12:59:56.043493Z","shell.execute_reply":"2022-04-04T12:59:56.122916Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n        \ninvTrans = T.Compose([T.Normalize(mean = [ 0., 0., 0. ],\n                                  std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n                      T.Normalize(mean = [ -0.485, -0.456, -0.406 ],\n                                  std = [ 1., 1., 1. ]),\n                      ])        \n\ndef _concat(x, y):\n    \"\"\" Concat by the last dimension \"\"\"\n    if isinstance(x, np.ndarray):\n        return np.concatenate((x, y), axis=-1)\n    elif isinstance(x, torch.Tensor):\n        return torch.cat([x, y], dim=-1)\n    else:\n        raise TypeError(\"unknown type '{}'\".format(type(x)))\n\ndef xcycwh_to_xywh(xcycwh):\n    \"\"\"Convert [x_c y_c w h] box format to [x1, y1, w, h] format.\"\"\"\n    if isinstance(xcycwh, (list, tuple)):\n        # Single box given as a list of coordinates\n        assert not isinstance(xcycwh[0], (list, tuple))\n        xc, yc = xcycwh[0], xcycwh[1]\n        w = xcycwh[2]\n        h = xcycwh[3]\n        x1 = xc - w / 2.\n        y1 = yc - h / 2.\n        return [x1, y1, w, h]\n    elif isinstance(xcycwh, (np.ndarray, torch.Tensor)):\n        wh = xcycwh[..., 2:4]\n        x1y1 = xcycwh[..., 0:2] - wh / 2.\n        return _concat(x1y1, wh)\n    else:\n        raise TypeError('Argument xcycwh must be a list, tuple, or numpy array.')\n        \ndef xywh_to_xyxy(xywh):\n    \"\"\"Convert [x1 y1 w h] box format to [x1 y1 x2 y2] format.\"\"\"\n    if isinstance(xywh, (list, tuple)):\n        # Single box given as a list of coordinates\n        assert len(xywh) == 4\n        x1, y1 = xywh[0], xywh[1]\n        x2 = x1 + np.maximum(0., xywh[2] - 1.)\n        y2 = y1 + np.maximum(0., xywh[3] - 1.)\n        return (x1, y1, x2, y2)\n    elif isinstance(xywh, np.ndarray):\n        # Multiple boxes given as a 2D ndarray\n        return np.hstack(\n            (xywh[:, 0:2], xywh[:, 0:2] + np.maximum(0, xywh[:, 2:4] - 1))\n        )\n    else:\n        raise TypeError('Argument xywh must be a list, tuple, or numpy array.')\n\ndef xyxy_to_xcycwh(xyxy):\n    \"\"\"Convert [x1 y1 x2, y2] box format to [x_c y_c w h] format.\"\"\"\n    if isinstance(xyxy, (list, tuple)):\n        # Single box given as a list of coordinates\n        assert not isinstance(xyxy[0], (list, tuple))\n        x1, y1 = xyxy[0], xyxy[1]\n        w = xyxy[2] - x1\n        h = xyxy[3] - y1\n        x = (xyxy[0] + xyxy[2]) / 2.\n        y = (xyxy[1] + xyxy[3]) / 2.\n        return [x, y, w, h]\n    elif isinstance(xyxy, (np.ndarray, torch.Tensor)):\n        wh = xyxy[..., 2:4] - xyxy[..., 0:2]\n        xy = (xyxy[..., 0:2] + xyxy[..., 2:4]) / 2.\n        return _concat(xy, wh)\n    else:\n        raise TypeError('Argument xyxy must be a list, tuple, or numpy array.')\n\ndef xywh_to_xcycwh(xywh):\n    \"\"\"Convert [x1 y1 w h] box format to [x_c y_c w h] format.\"\"\"\n    if isinstance(xywh, (list, tuple)):\n        # Single box given as a list of coordinates\n        assert not isinstance(xywh[0], (list, tuple))\n        x1, y1 = xywh[0], xywh[1]\n        xc = x1 + np.maximum(0., xywh[2] / 2.)\n        yc = y1 + np.maximum(0., xywh[3] )\n        return [xc, yc, xywh[2], xywh[3]]\n\n    elif isinstance(xywh, (np.ndarray, torch.Tensor)):\n        wh = xywh[..., 2:4]\n        xcyc = xywh[..., 0:2] + wh / 2\n        return _concat(xcyc, wh)\n    else:\n        raise TypeError('Argument xyxy must be a list, tuple, numpy array, or tensor.')","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:00:01.362663Z","iopub.execute_input":"2022-04-04T13:00:01.362963Z","iopub.status.idle":"2022-04-04T13:00:01.395780Z","shell.execute_reply.started":"2022-04-04T13:00:01.362930Z","shell.execute_reply":"2022-04-04T13:00:01.394518Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"n_folds = 5\nseed = 42\nnum_classes = 8 # 7 unique classes + background class\nnum_queries = 20 \nnull_class_coef = 1/num_classes\nBATCH_SIZE = 4\nIMAGE_SIZE = 512\nLR = 1e-3 # 2e-5 \nEPOCHS = 4","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:00:06.360570Z","iopub.execute_input":"2022-04-04T13:00:06.361726Z","iopub.status.idle":"2022-04-04T13:00:06.368580Z","shell.execute_reply.started":"2022-04-04T13:00:06.361676Z","shell.execute_reply":"2022-04-04T13:00:06.367418Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything(seed)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:00:08.961059Z","iopub.execute_input":"2022-04-04T13:00:08.961363Z","iopub.status.idle":"2022-04-04T13:00:08.972522Z","shell.execute_reply.started":"2022-04-04T13:00:08.961332Z","shell.execute_reply":"2022-04-04T13:00:08.971272Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"paths = r'../input/arthropod-taxonomy-orders-object-detection-dataset/ArTaxOr/'\nannots = glob(paths + '/*/annotations/*.json')\nannots = [str(annot) for annot in annots]","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:00:11.206503Z","iopub.execute_input":"2022-04-04T13:00:11.206856Z","iopub.status.idle":"2022-04-04T13:00:11.988640Z","shell.execute_reply.started":"2022-04-04T13:00:11.206822Z","shell.execute_reply":"2022-04-04T13:00:11.987660Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# it takes a lot time to create dataframe, but it is more comfort to use df as we are using previous notebook pipeline\ndef create_dataframe(annots: list = annots) -> pd.DataFrame:\n    array = np.zeros((1, 7), dtype=np.object)\n    for i in tqdm(range(len(annots)), total=len(annots)):\n        with open(annots[i], 'rb') as infile:\n            data = json.load(infile)\n            image_path = paths + data['asset'].get('path').split('ArTaxOr')[-1][1:]\n            for item in data['regions']:\n                row = np.zeros((1,7), dtype=np.object)\n                row[0,0] = image_path\n                row[0,1] = data['asset'].get('name')\n                row[0,2] = item['tags'][0]\n                row[0,3] = item['boundingBox'].get('height')\n                row[0,4] = item['boundingBox'].get('width')\n                row[0,5] = item['boundingBox'].get('left')\n                row[0,6] = item['boundingBox'].get('top')\n                array = np.concatenate([array, row], axis=0)\n    df = pd.DataFrame(data=array[1:, :], columns=['path', 'image_id', 'label', 'height', 'width', 'left', 'top'])\n    df[['height', 'width', 'left', 'top']] = df[['height', 'width', 'left', 'top']].astype(np.float32)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:00:14.306233Z","iopub.execute_input":"2022-04-04T13:00:14.307308Z","iopub.status.idle":"2022-04-04T13:00:14.320253Z","shell.execute_reply.started":"2022-04-04T13:00:14.307258Z","shell.execute_reply":"2022-04-04T13:00:14.319141Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df = create_dataframe()\ndf[['xmin', 'ymin', 'xmax', 'ymax']] = xywh_to_xyxy(df[['left', 'top', 'width', 'height']].values)\nenc = LabelEncoder()\ndf['class_id'] = enc.fit_transform(df['label'])","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:00:16.575792Z","iopub.execute_input":"2022-04-04T13:00:16.576314Z","iopub.status.idle":"2022-04-04T13:01:16.404952Z","shell.execute_reply.started":"2022-04-04T13:00:16.576277Z","shell.execute_reply":"2022-04-04T13:01:16.404067Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# coco bbox format\nrandom_path = df['path'].sample(1).iloc[0]\ndf_random = df[df['path'] == random_path]\nprint(random_path)\nsample_image = cv2.imread(random_path, cv2.IMREAD_COLOR)\nsample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(8,8))\nplt.imshow(sample_image)\nax = plt.gca()\n\nfor idx, row in df_random.iterrows():\n    x, y, w, h = row[['left', 'top', 'width', 'height']]\n    ax.add_patch(plt.Rectangle((x, y), w, h,\n                                fill=False, color='red', linewidth=3))\n    text = f'Class_id: {row[\"label\"]}'\n    ax.text(x, y, text, fontsize=15,\n            bbox=dict(facecolor='yellow', alpha=0.5))\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T12:04:40.195287Z","iopub.execute_input":"2022-04-04T12:04:40.195626Z","iopub.status.idle":"2022-04-04T12:04:40.921783Z","shell.execute_reply.started":"2022-04-04T12:04:40.195585Z","shell.execute_reply":"2022-04-04T12:04:40.917695Z"},"trusted":true},"execution_count":176,"outputs":[]},{"cell_type":"code","source":"# get n_folded dataframe, stratified by number of bboxes, trying to preserve target-value counts\nskf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n\ndf_folds = df[['image_id']].copy()\ndf_folds.loc[:, 'bbox_count'] = 1\ndf_folds = df_folds.groupby('image_id').count()\ndf_folds.loc[:, 'class_id'] = df[['image_id', 'class_id']].groupby('image_id').min()['class_id']\ndf_folds.loc[:, 'stratify_group'] = np.char.add(\n    df_folds['class_id'].values.astype(str),\n    df_folds['bbox_count'].apply(lambda x: f'_{x // ((num_classes-1)*2 + 1)}').values.astype(str)\n)\ndf_folds.loc[:, 'fold'] = 0\n\nfor fold_number, (train_index, val_index) in enumerate(skf.split(X=df_folds.index, y=df_folds['stratify_group'])):\n    df_folds.loc[df_folds.iloc[val_index].index, 'fold'] = fold_number","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:01:20.746505Z","iopub.execute_input":"2022-04-04T13:01:20.746885Z","iopub.status.idle":"2022-04-04T13:01:20.913202Z","shell.execute_reply.started":"2022-04-04T13:01:20.746836Z","shell.execute_reply":"2022-04-04T13:01:20.912171Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def get_train_transforms():\n    return A.Compose([A.OneOf([A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),      \n                      A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.9)],p=0.5),\n                      A.ToGray(p=0.01),\n                      A.HorizontalFlip(p=0.1),\n                      A.VerticalFlip(p=0.1),\n                      A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE, p=1),\n                      A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.2),\n                      A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=1.0),\n                      ToTensorV2(p=1.0),\n                      ],p=1.0,\n                      bbox_params=A.BboxParams(format='yolo', min_area=0, min_visibility=0, label_fields=['labels']),\n                      )\n\ndef get_valid_transforms():\n    return A.Compose([A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE, p=1.0),\n                      A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=1.0),\n                      ToTensorV2(p=1.0),\n                     ], \n                      p=1.0, \n                      bbox_params=A.BboxParams(format='yolo', min_area=0, min_visibility=0, label_fields=['labels']),\n                      )","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:01:23.075582Z","iopub.execute_input":"2022-04-04T13:01:23.076244Z","iopub.status.idle":"2022-04-04T13:01:23.088865Z","shell.execute_reply.started":"2022-04-04T13:01:23.076201Z","shell.execute_reply":"2022-04-04T13:01:23.087559Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class InsectDataset(Dataset):\n    \"\"\" Define custom dataset class that returns an image tensor with corresponded target and image name\"\"\"\n    def __init__(self, image_ids, df, transforms=None):\n        self.image_ids = image_ids\n        self.df = df\n        self.transforms = transforms\n        \n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n    \n    def __getitem__(self, index):\n        image_id = self.image_ids[index]\n        records = self.df[self.df['image_id'] == image_id]\n\n        image = cv2.imread(records['path'].iloc[0], cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        h,w,_ = image.shape\n        image /= 255.0\n        \n        boxes = records[['xmin', 'ymin', 'xmax', 'ymax']].values\n        boxes[:, [0,2]] *= (IMAGE_SIZE / w)\n        boxes[:, [1,3]] *= (IMAGE_SIZE / h)\n        boxes = A.augmentations.bbox_utils.normalize_bboxes(boxes, IMAGE_SIZE, IMAGE_SIZE)\n        boxes = np.array([xyxy_to_xcycwh(box) for box in boxes]) # yolo\n\n        area = boxes[:,2] * boxes[:,3]\n        area = torch.as_tensor(area, dtype=torch.float32)\n        \n        labels = records['class_id'].values.astype(np.int32)\n\n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': boxes,\n                'labels': labels\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            boxes = sample['bboxes']\n            labels = sample['labels']\n                                \n        target = {}\n        target['boxes'] = torch.as_tensor(boxes,dtype=torch.float32)\n        target['labels'] = torch.as_tensor(labels,dtype=torch.long)\n        target['image_id'] = torch.tensor([index])\n        target['area'] = area\n        \n        return image, target, image_id\n    \n    def collate_fn(self, batch):\n        return tuple(zip(*batch))","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:01:25.787338Z","iopub.execute_input":"2022-04-04T13:01:25.787672Z","iopub.status.idle":"2022-04-04T13:01:25.805303Z","shell.execute_reply.started":"2022-04-04T13:01:25.787629Z","shell.execute_reply":"2022-04-04T13:01:25.803948Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"#### Example","metadata":{}},{"cell_type":"code","source":"ds = InsectDataset(df_folds[df_folds['fold'] != 0].index.values, df, get_train_transforms())","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:01:29.233192Z","iopub.execute_input":"2022-04-04T13:01:29.233514Z","iopub.status.idle":"2022-04-04T13:01:29.242116Z","shell.execute_reply.started":"2022-04-04T13:01:29.233480Z","shell.execute_reply":"2022-04-04T13:01:29.240993Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"sample = ds[509]\nimg = invTrans(sample[0]).permute(1,2,0).cpu().numpy()\nboxes = sample[1]['boxes'].cpu().numpy()\nh,w,_ = img.shape\nboxes = xcycwh_to_xywh(boxes)\nboxes = [np.array(box).astype(np.int32) for box in A.augmentations.bbox_utils.denormalize_bboxes(boxes,h,w)]\n\n\nprint(boxes)\nplt.imshow(img)\nax = plt.gca()\n\nfor box in boxes:\n    x, y, w, h = box\n    ax.add_patch(plt.Rectangle((x, y), w, h, fill=False, color='red', linewidth=3))\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:01:31.061036Z","iopub.execute_input":"2022-04-04T13:01:31.062797Z","iopub.status.idle":"2022-04-04T13:01:31.502241Z","shell.execute_reply.started":"2022-04-04T13:01:31.062743Z","shell.execute_reply":"2022-04-04T13:01:31.501179Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"#### continue ","metadata":{}},{"cell_type":"code","source":"class DETRModel(nn.Module):\n    def __init__(self, num_classes, num_queries):\n        super(DETRModel,self).__init__()\n        self.num_classes = num_classes\n        self.num_queries = num_queries\n        \n        self.model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)\n        self.in_features = self.model.class_embed.in_features\n        \n        #for param in self.model.parameters():#\n        #    param.requires_grad = False#\n        \n        self.model.class_embed = nn.Linear(in_features=self.in_features, out_features=self.num_classes)\n        self.model.num_queries = self.num_queries\n        \n        #self.model.query_embed = nn.Embedding(self.num_queries, 256)\n        #self.model.bbox_embed = MLP(256, 256, 4, 3)\n        \n    def forward(self,images):\n        return self.model(images)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T12:49:49.294195Z","iopub.execute_input":"2022-04-04T12:49:49.294630Z","iopub.status.idle":"2022-04-04T12:49:49.301807Z","shell.execute_reply.started":"2022-04-04T12:49:49.294599Z","shell.execute_reply":"2022-04-04T12:49:49.300875Z"},"trusted":true},"execution_count":275,"outputs":[]},{"cell_type":"code","source":"# second way\nfrom detr.models.detr import MLP\n\nclass DETRModel(nn.Module):\n    def __init__(self, num_classes, num_queries):\n        super(DETRModel,self).__init__()\n        self.num_classes = num_classes\n        self.num_queries = num_queries\n        \n        self.model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=False, num_classes=50)\n        checkpoint = torch.hub.load_state_dict_from_url(\n                                    url='https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth',\n                                    map_location=device,\n                                    check_hash=True)\n        del checkpoint[\"model\"][\"class_embed.weight\"]\n        del checkpoint[\"model\"][\"class_embed.bias\"]\n        self.model.load_state_dict(checkpoint[\"model\"], strict=False)\n        for param in self.model.parameters():\n            param.requires_grad = False\n        \n        self.in_features = self.model.class_embed.in_features\n        self.model.class_embed = nn.Linear(in_features=self.in_features, out_features=self.num_classes)\n        self.model.bbox_embed = MLP(256, 256, 4, 3) # multilayer perceptron\n        \n    def forward(self,images):\n        return self.model(images)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:01:37.992568Z","iopub.execute_input":"2022-04-04T13:01:37.992907Z","iopub.status.idle":"2022-04-04T13:01:38.004968Z","shell.execute_reply.started":"2022-04-04T13:01:37.992874Z","shell.execute_reply":"2022-04-04T13:01:38.003821Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"matcher = HungarianMatcher()\n\nweight_dict = weight_dict = {'loss_ce': 1, 'loss_bbox': 1 , 'loss_giou': 1}\n\nlosses = ['labels', 'boxes', 'cardinality']","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:01:40.586259Z","iopub.execute_input":"2022-04-04T13:01:40.586911Z","iopub.status.idle":"2022-04-04T13:01:40.594693Z","shell.execute_reply.started":"2022-04-04T13:01:40.586858Z","shell.execute_reply":"2022-04-04T13:01:40.593559Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def train_fn(dataloader, model, criterion, optimizer, scheduler, epoch):\n    model.train()\n    criterion.train()\n    \n    summary_loss = AverageMeter()\n    \n    tk0 = tqdm(dataloader, total=len(dataloader), leave=True)\n    \n    for step, (images, targets, image_ids) in enumerate(tk0):\n        \n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        \n\n        output = model(images)\n\n        loss_dict = criterion(output, targets)\n        weight_dict = criterion.weight_dict\n        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n        \n        optimizer.zero_grad()\n\n        losses.backward()\n        optimizer.step()\n        if scheduler is not None:\n            scheduler.step()    \n        \n        summary_loss.update(losses.item(), BATCH_SIZE)\n        tk0.set_postfix(loss=summary_loss.avg) # print out average losses after each epoch\n    \n    return summary_loss","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:01:42.627408Z","iopub.execute_input":"2022-04-04T13:01:42.628092Z","iopub.status.idle":"2022-04-04T13:01:42.640345Z","shell.execute_reply.started":"2022-04-04T13:01:42.628054Z","shell.execute_reply":"2022-04-04T13:01:42.638803Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef eval_fn(dataloader, model, criterion):\n    model.eval()\n    criterion.eval()\n    summary_loss = AverageMeter()\n            \n    tk0 = tqdm(dataloader, total=len(dataloader), leave=True)\n    for step, (images, targets, image_ids) in enumerate(tk0):\n            \n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        output = model(images)\n\n        loss_dict = criterion(output, targets)\n        weight_dict = criterion.weight_dict\n        \n        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n        summary_loss.update(losses.item(),BATCH_SIZE)\n        tk0.set_postfix(loss=summary_loss.avg)\n        \n    return summary_loss","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:01:45.025541Z","iopub.execute_input":"2022-04-04T13:01:45.025906Z","iopub.status.idle":"2022-04-04T13:01:45.035361Z","shell.execute_reply.started":"2022-04-04T13:01:45.025873Z","shell.execute_reply":"2022-04-04T13:01:45.034220Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def run(fold):\n    \n    df_train = df_folds[df_folds['fold'] != fold]\n    df_valid = df_folds[df_folds['fold'] == fold]\n        \n    train_dataset = InsectDataset(\n                                  image_ids=df_train.index.values,\n                                  df=df,\n                                  transforms=get_train_transforms())\n\n    valid_dataset = InsectDataset(\n                                  image_ids=df_valid.index.values,\n                                  df=df,\n                                  transforms=get_valid_transforms())\n    \n    train_data_loader = DataLoader(\n                                   train_dataset,\n                                   batch_size=BATCH_SIZE,\n                                   shuffle=False,\n                                   num_workers=4,\n                                   collate_fn=train_dataset.collate_fn)\n\n    valid_data_loader = DataLoader(\n                                   valid_dataset,\n                                   batch_size=BATCH_SIZE,\n                                   shuffle=False,\n                                   num_workers=4,\n                                   collate_fn=valid_dataset.collate_fn)\n    \n    model = DETRModel(num_classes=num_classes, num_queries=num_queries).to(device)\n    criterion = SetCriterion(num_classes-1, matcher, weight_dict, eos_coef=1/num_classes, losses=losses).to(device)    \n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n    scheduler=torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n    \n    best_loss = 10**5\n    for epoch in range(EPOCHS):\n        train_loss = train_fn(train_data_loader, model, criterion, optimizer,scheduler=scheduler, epoch=epoch)\n        valid_loss = eval_fn(valid_data_loader, model, criterion)\n        \n        print('|EPOCH {}| TRAIN_LOSS {}| VALID_LOSS {}|'.format(epoch+1, train_loss.avg, valid_loss.avg))\n        if valid_loss.avg < best_loss:\n            best_loss = valid_loss.avg\n            print('Best model found for Fold {} in Epoch {}........Saving Model'.format(fold, epoch+1))\n            torch.save(model.state_dict(), f'detr_best_{fold}.pth')","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:01:49.806403Z","iopub.execute_input":"2022-04-04T13:01:49.806720Z","iopub.status.idle":"2022-04-04T13:01:49.821705Z","shell.execute_reply.started":"2022-04-04T13:01:49.806686Z","shell.execute_reply":"2022-04-04T13:01:49.820646Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"for fold in range(5):\n    run(fold)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:01:52.133708Z","iopub.execute_input":"2022-04-04T13:01:52.134151Z","iopub.status.idle":"2022-04-04T13:42:35.654870Z","shell.execute_reply.started":"2022-04-04T13:01:52.134100Z","shell.execute_reply":"2022-04-04T13:42:35.652794Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def view_sample(frame, model, device, threshold=0.7):\n\n    valid_dataset = InsectDataset(image_ids=frame.index.values,\n                                  df=df,\n                                  transforms=get_valid_transforms()\n                                  )\n     \n    valid_data_loader = DataLoader(\n                                    valid_dataset,\n                                    batch_size=BATCH_SIZE,\n                                    shuffle=False,\n                                    num_workers=4,\n                                    collate_fn=valid_dataset.collate_fn)\n    \n    images, targets, image_ids = next(iter(valid_data_loader))\n    _,h,w = images[0].shape\n    images = list(img.to(device) for img in images)\n    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n    \n    boxes = targets[0]['boxes'].cpu().numpy()\n    boxes = xcycwh_to_xywh(boxes)\n    boxes = [np.array(box).astype(np.int32) for box in A.augmentations.bbox_utils.denormalize_bboxes(boxes,h,w)]\n    sample = invTrans(images[0]).permute(1,2,0).cpu().numpy()\n    \n    model.eval()\n    model.to(device)\n    cpu_device = torch.device(\"cpu\")\n    \n    with torch.no_grad():\n        outputs = model(images)\n        \n    outputs = [{k: v.to(cpu_device) for k, v in outputs.items()}]\n    \n    plt.figure(figsize=(16,8))\n    ax = plt.gca()\n\n    for box in boxes:\n        ax.add_patch(plt.Rectangle((box[0], box[1]), box[2], box[3], fill=False, color='red', linewidth=2))\n        \n    probs = outputs[0]['pred_logits'].softmax(-1).detach().cpu().numpy()[0, :, :-1] \n    keep = probs.max(-1) > threshold\n    probs = probs[keep]\n\n    oboxes = outputs[0]['pred_boxes'].detach().cpu().numpy()[0, keep]\n    oboxes = xcycwh_to_xywh(oboxes)\n    oboxes = [np.array(box).astype(np.int32) for box in A.augmentations.bbox_utils.denormalize_bboxes(oboxes,h,w)]\n\n    labels = outputs[0]['pred_logits'][...,:-1].max(-1)[1].cpu().numpy()[0, keep]\n\n    for box, prob, label in zip(oboxes, probs, labels):\n        ax.add_patch(plt.Rectangle((box[0], box[1]), box[2], box[3], fill=False, color='blue', linewidth=2))\n        text = f'Class_id: {enc.inverse_transform(label)}'\n        ax.text(box[0], box[1], text, fontsize=10, bbox=dict(facecolor='yellow', alpha=0.5))\n    \n    ax.set_axis_off()\n    ax.imshow(sample)\n    \n    return outputs","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:46:12.760672Z","iopub.execute_input":"2022-04-04T13:46:12.761230Z","iopub.status.idle":"2022-04-04T13:46:12.781825Z","shell.execute_reply.started":"2022-04-04T13:46:12.761195Z","shell.execute_reply":"2022-04-04T13:46:12.780448Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"model = DETRModel(num_classes=num_classes,num_queries=num_queries)\nmodel.load_state_dict(torch.load(\"./detr_best_0.pth\"))","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:42:42.693723Z","iopub.execute_input":"2022-04-04T13:42:42.694010Z","iopub.status.idle":"2022-04-04T13:42:44.093911Z","shell.execute_reply.started":"2022-04-04T13:42:42.693978Z","shell.execute_reply":"2022-04-04T13:42:44.092934Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"view = view_sample(df_folds[df_folds['fold'] == 0], model=model, device=device, threshold=0.3)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:46:22.320282Z","iopub.execute_input":"2022-04-04T13:46:22.320596Z","iopub.status.idle":"2022-04-04T13:46:25.729880Z","shell.execute_reply.started":"2022-04-04T13:46:22.320553Z","shell.execute_reply":"2022-04-04T13:46:25.728635Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:43:45.883352Z","iopub.execute_input":"2022-04-04T13:43:45.884510Z","iopub.status.idle":"2022-04-04T13:43:45.918101Z","shell.execute_reply.started":"2022-04-04T13:43:45.884465Z","shell.execute_reply":"2022-04-04T13:43:45.917115Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}