{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport torch\nfrom torchvision import models\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport torch.nn.functional as F\nimport torchvision\n\nfrom PIL import Image, ImageDraw\nimport matplotlib.pyplot as plt\n\nimport os\nimport json\nimport re\nimport random\nfrom math import sqrt\nprint(os.listdir(\"../input\"))\n# https://www.kaggle.com/code/sdeagggg/ssd300-with-pytorch  # great original kernel, visit and vote up","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-08T10:58:34.755901Z","iopub.execute_input":"2022-11-08T10:58:34.756514Z","iopub.status.idle":"2022-11-08T10:58:36.359568Z","shell.execute_reply.started":"2022-11-08T10:58:34.756408Z","shell.execute_reply":"2022-11-08T10:58:36.357720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class names dictionary\nclass_names = ['background', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'CA', 'CJ', 'CK', 'CQ', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', \n               'DA', 'DJ', 'DK', 'DQ', 'H2', 'H3', 'H4', 'H5', 'H6', 'H7', 'H8', 'H9', 'H10', 'HA', 'HJ', 'HK', 'HQ', 'SA', 'S2', 'S3', 'S4', 'S5', \n               'S6', 'S7', 'S8', 'S9', 'S10', 'SJ', 'SK', 'SQ']\n\nid_to_class = {i: v for i, v in enumerate(class_names)}\nclass_to_id = {v: i for i, v in enumerate(class_names[1:], 1)}","metadata":{"execution":{"iopub.status.busy":"2022-11-08T10:58:38.053033Z","iopub.execute_input":"2022-11-08T10:58:38.053848Z","iopub.status.idle":"2022-11-08T10:58:38.061868Z","shell.execute_reply.started":"2022-11-08T10:58:38.053812Z","shell.execute_reply":"2022-11-08T10:58:38.060730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# display sample","metadata":{"_uuid":"55d7b1142a9ea4e285c015fa2cba666053deeec4"}},{"cell_type":"code","source":"def cxcy_to_xy(cxcy):\n    return np.concatenate([cxcy[:, :2] - (cxcy[:, 2:] / 2),  \n                           cxcy[:, :2] + (cxcy[:, 2:] / 2)], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-11-08T10:58:40.568147Z","iopub.execute_input":"2022-11-08T10:58:40.568854Z","iopub.status.idle":"2022-11-08T10:58:40.573895Z","shell.execute_reply.started":"2022-11-08T10:58:40.568819Z","shell.execute_reply":"2022-11-08T10:58:40.572891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('../input/playing-cards/train/labels/image_0000600.txt', 'r') as f:\n    reader = f.readlines()\n# the original coordinates are given in absolute form in yolo format (cx_cy_w_h)\nimg = Image.open('../input/playing-cards/train/images/image_0000600.jpg')\n\norigin_img = img.copy()\nwidth, height = img.size\n\ndraw = ImageDraw.Draw(origin_img)\n\nboxes, labels = [], []\nfor line in reader:\n    line = line.split()\n    line = [float(l) for l in line]\n    cxcy = np.array(line[1:]).reshape(1, -1)\n    cxcy[:, :2] *= width\n    cxcy[:, 2:] *= height\n    cxcy = cxcy_to_xy(cxcy)\n    label = line[0]\n    xmin, ymin, xmax, ymax = cxcy[0].tolist()\n    boxes.append([xmin, ymin, xmax, ymax])\n    labels.append(label)\n    draw.rectangle(xy=[(xmin,ymin), (xmax,ymax)])\n    draw.text(xy=[xmin, ymin], text=id_to_class[int(label)+1])\norigin_img","metadata":{"_uuid":"569e583e998b5fbcbcd96a0e1305125f5844d450","execution":{"iopub.status.busy":"2022-11-08T10:58:41.975342Z","iopub.execute_input":"2022-11-08T10:58:41.975705Z","iopub.status.idle":"2022-11-08T10:58:42.092132Z","shell.execute_reply.started":"2022-11-08T10:58:41.975676Z","shell.execute_reply":"2022-11-08T10:58:42.091306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dims = (300, 300)\nnew_dims = torch.FloatTensor([dims[1], dims[0], dims[1], dims[0]]).unsqueeze(0)\nold_dims = torch.FloatTensor([img.width, img.height, img.width, img.height]).unsqueeze(0)\n\nnew_boxes = []\nfor box in boxes:\n    new_box = torch.FloatTensor(box) / old_dims\n    new_box = new_box * new_dims\n    new_boxes.append(new_box)\nnew_boxes = torch.cat(new_boxes)","metadata":{"_uuid":"5ee52849db929db543f08de7d11240db4ba83761","execution":{"iopub.status.busy":"2022-11-08T10:58:45.472524Z","iopub.execute_input":"2022-11-08T10:58:45.472889Z","iopub.status.idle":"2022-11-08T10:58:45.484567Z","shell.execute_reply.started":"2022-11-08T10:58:45.472858Z","shell.execute_reply":"2022-11-08T10:58:45.483149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_boxes","metadata":{"_uuid":"18fa34d2c1136b6a39f9c3cd96901eca196c51a0","execution":{"iopub.status.busy":"2022-11-08T10:58:47.092634Z","iopub.execute_input":"2022-11-08T10:58:47.093023Z","iopub.status.idle":"2022-11-08T10:58:47.102463Z","shell.execute_reply.started":"2022-11-08T10:58:47.092992Z","shell.execute_reply":"2022-11-08T10:58:47.101297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = transforms.Resize(dims)(img)\ndraw = ImageDraw.Draw(img)\n\nfor new_box, label in zip(new_boxes, labels):\n    draw.rectangle(xy=[tuple(new_box.tolist())[:2], tuple(new_box.tolist())[2:]])\n    xmin, ymin = tuple(new_box.tolist())[:2]\n    draw.text(xy=[xmin, ymin], text=id_to_class[int(label)+1])\nimg","metadata":{"_uuid":"190998fb72fea619683015127bf28be2813376d8","execution":{"iopub.status.busy":"2022-11-08T10:58:48.222563Z","iopub.execute_input":"2022-11-08T10:58:48.222916Z","iopub.status.idle":"2022-11-08T10:58:48.275784Z","shell.execute_reply.started":"2022-11-08T10:58:48.222886Z","shell.execute_reply":"2022-11-08T10:58:48.274781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"}},{"cell_type":"code","source":"train_img_paths = sorted(os.listdir(\"../input/playing-cards/train/images/\"))\ntrain_annot_paths = sorted(os.listdir(\"../input/playing-cards/train/labels/\"))\n\nvalid_img_paths = sorted(os.listdir(\"../input/playing-cards/valid/images/\"))\nvalid_annot_paths = sorted(os.listdir(\"../input/playing-cards/valid/labels/\"))","metadata":{"execution":{"iopub.status.busy":"2022-11-08T10:58:51.169636Z","iopub.execute_input":"2022-11-08T10:58:51.170410Z","iopub.status.idle":"2022-11-08T10:58:52.423753Z","shell.execute_reply.started":"2022-11-08T10:58:51.170372Z","shell.execute_reply":"2022-11-08T10:58:52.422732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SSDDateset(Dataset):\n    def __init__(self, img_names, annotations, subset='train', transform=None):\n        self.img_folder_path = f'../input/playing-cards/{subset}/images/'\n        self.annotation_folder_path = f'../input/playing-cards/{subset}/labels/'\n        self.img_names = img_names\n        self.annotations = annotations\n        self.transform = transform\n        \n    def __getitem__(self, idx):\n        file = self.img_names[idx]\n        img_path = self.img_folder_path + file\n        img = Image.open(img_path)\n        img = img.convert('RGB')\n        \n        annotation_path = self.annotation_folder_path + self.annotations[idx]\n        with open(annotation_path) as f:\n            annotation = f.readlines()\n        \n        boxes, labels = [], []\n        for line in annotation:\n            line = line.split()\n            line = [float(l) for l in line]\n            label = line[0] + 1  # as we added background\n            boxes.append(line[1:])\n            labels.append(label)\n        \n        boxes = torch.cat([torch.FloatTensor(box)[None, ...] for box in boxes])\n        boxes = self.cxcy_to_xy(boxes)\n        \n        labels = torch.as_tensor(labels, dtype=torch.float32)\n        \n        if self.transform is not None:\n            img = self.transform(img)\n\n        return img, boxes, labels\n    \n    def __len__(self):\n        return len(self.img_names)\n    \n    def show_box(self):\n        image, boxes, labels = self[np.random.randint(len(self))]\n        image = image.detach().cpu()\n        boxes = boxes.detach().cpu().numpy()\n        labels = labels.detach().cpu().numpy().tolist()\n        \n        if self.transform is not None:\n            image = transforms.Compose([transforms.Normalize(mean = [ 0., 0., 0. ], std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n                                        transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ], std = [ 1., 1., 1. ]),\n                                        transforms.ToPILImage()])(image)\n        else:\n            image = transforms.ToPILImage()(image)\n        width, height = image.size\n        \n        boxes[:, [0,2]] *= width\n        boxes[:, [1,3]] *= height\n          \n        draw = ImageDraw.Draw(image)\n        for box, label in zip(boxes, labels):\n            draw.rectangle(xy=[tuple(box.tolist())[:2], tuple(box.tolist())[2:]])\n            xmin, ymin = tuple(box.tolist())[:2]\n            draw.text(xy=[xmin, ymin], text=id_to_class[int(label)])\n        \n        return image\n        \n    def cxcy_to_xy(self, cxcy):\n        return torch.cat([cxcy[:, :2] - (cxcy[:, 2:] / 2), cxcy[:, :2] + (cxcy[:, 2:] / 2)], dim=1) \n    \n    def collate_fn(self, batch):\n        \"\"\"\n        Since each image may have a different number of objects, we need a collate function (to be passed to the DataLoader).\n        This describes how to combine these tensors of different sizes. We use lists.\n        Note: this need not be defined in this Class, can be standalone.\n        :param batch: an iterable of N sets from __getitem__()\n        :return: a tensor of images, lists of varying-size tensors of bounding boxes, labels, and difficulties\n        \"\"\"\n\n        images = list()\n        boxes = list()\n        labels = list()\n\n        for b in batch:\n            images.append(b[0])\n            boxes.append(b[1])\n            labels.append(b[2])\n\n        images = torch.stack(images, dim=0)\n        boxes = torch.stack(boxes, dim=0)\n        labels = torch.stack(labels, dim=0)\n\n        return images, boxes, labels  # tensor (N, 3, 300, 300), 3 lists of N tensors each","metadata":{"_uuid":"442a9f50cb6b6156c240ec9438e24a298c699e43","execution":{"iopub.status.busy":"2022-11-08T10:59:09.035677Z","iopub.execute_input":"2022-11-08T10:59:09.036029Z","iopub.status.idle":"2022-11-08T10:59:09.053956Z","shell.execute_reply.started":"2022-11-08T10:59:09.035998Z","shell.execute_reply":"2022-11-08T10:59:09.052944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tsfm = transforms.Compose([\n                        transforms.Resize([300, 300]),\n                        transforms.ToTensor(),\n                        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n])\n# as improvement add to albumentations to train","metadata":{"_uuid":"b2f226a155aa4a28f3db1ff53297578a89c20b49","execution":{"iopub.status.busy":"2022-11-08T10:58:56.422322Z","iopub.execute_input":"2022-11-08T10:58:56.423341Z","iopub.status.idle":"2022-11-08T10:58:56.429084Z","shell.execute_reply.started":"2022-11-08T10:58:56.423301Z","shell.execute_reply":"2022-11-08T10:58:56.428202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds = SSDDateset(train_img_paths, train_annot_paths, transform=tsfm)","metadata":{"execution":{"iopub.status.busy":"2022-11-08T10:59:11.077622Z","iopub.execute_input":"2022-11-08T10:59:11.077982Z","iopub.status.idle":"2022-11-08T10:59:11.083031Z","shell.execute_reply.started":"2022-11-08T10:59:11.077952Z","shell.execute_reply":"2022-11-08T10:59:11.081840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds.show_box()","metadata":{"execution":{"iopub.status.busy":"2022-11-08T10:59:12.352757Z","iopub.execute_input":"2022-11-08T10:59:12.353120Z","iopub.status.idle":"2022-11-08T10:59:12.417869Z","shell.execute_reply.started":"2022-11-08T10:59:12.353088Z","shell.execute_reply":"2022-11-08T10:59:12.416819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SSD model","metadata":{"_uuid":"04e9a8f32873f8233a5d1afe956c65a3bfc539c8"}},{"cell_type":"markdown","source":"# utils","metadata":{"_uuid":"fad2dd3639d5a4b56491ddffe73701d8528adb7a"}},{"cell_type":"code","source":"def xy_to_cxcy(xy):\n    \"\"\"\n    Convert bounding boxes from boundary coordinates (x_min, y_min, x_max, y_max) to center-size coordinates (c_x, c_y, w, h).\n    :param xy: bounding boxes in boundary coordinates, a tensor of size (n_boxes, 4)\n    :return: bounding boxes in center-size coordinates, a tensor of size (n_boxes, 4)\n    \"\"\"\n    return torch.cat([(xy[:, 2:] + xy[:, :2]) / 2,  # c_x, c_y\n                      xy[:, 2:] - xy[:, :2]], 1)  # w, h\n\n\ndef cxcy_to_xy(cxcy):\n    \"\"\"\n    Convert bounding boxes from center-size coordinates (c_x, c_y, w, h) to boundary coordinates (x_min, y_min, x_max, y_max).\n    :param cxcy: bounding boxes in center-size coordinates, a tensor of size (n_boxes, 4)\n    :return: bounding boxes in boundary coordinates, a tensor of size (n_boxes, 4)\n    \"\"\"\n    return torch.cat([cxcy[:, :2] - (cxcy[:, 2:] / 2),  # x_min, y_min\n                      cxcy[:, :2] + (cxcy[:, 2:] / 2)], 1)  # x_max, y_max\n\n\ndef cxcy_to_gcxgcy(cxcy, priors_cxcy):\n    \"\"\"\n    Encode bounding boxes (that are in center-size form) w.r.t. the corresponding prior boxes (that are in center-size form).\n    For the center coordinates, find the offset with respect to the prior box, and scale by the size of the prior box.\n    For the size coordinates, scale by the size of the prior box, and convert to the log-space.\n    In the model, we are predicting bounding box coordinates in this encoded form.\n    :param cxcy: bounding boxes in center-size coordinates, a tensor of size (n_priors, 4)\n    :param priors_cxcy: prior boxes with respect to which the encoding must be performed, a tensor of size (n_priors, 4)\n    :return: encoded bounding boxes, a tensor of size (n_priors, 4)\n    \"\"\"\n\n    # The 10 and 5 below are referred to as 'variances' in the original Caffe repo, completely empirical\n    # They are for some sort of numerical conditioning, for 'scaling the localization gradient'\n    # See https://github.com/weiliu89/caffe/issues/155\n    return torch.cat([(cxcy[:, :2] - priors_cxcy[:, :2]) / (priors_cxcy[:, 2:] / 10),  # g_c_x, g_c_y\n                      torch.log(cxcy[:, 2:] / priors_cxcy[:, 2:]) * 5], 1)  # g_w, g_h\n\n\ndef gcxgcy_to_cxcy(gcxgcy, priors_cxcy):\n    \"\"\"\n    Decode bounding box coordinates predicted by the model, since they are encoded in the form mentioned above.\n    They are decoded into center-size coordinates.\n    This is the inverse of the function above.\n    :param gcxgcy: encoded bounding boxes, i.e. output of the model, a tensor of size (n_priors, 4)\n    :param priors_cxcy: prior boxes with respect to which the encoding is defined, a tensor of size (n_priors, 4)\n    :return: decoded bounding boxes in center-size form, a tensor of size (n_priors, 4)\n    \"\"\"\n\n    return torch.cat([gcxgcy[:, :2] * priors_cxcy[:, 2:] / 10 + priors_cxcy[:, :2],  # c_x, c_y\n                      torch.exp(gcxgcy[:, 2:] / 5) * priors_cxcy[:, 2:]], 1)  # w, h\n\ndef find_jaccard_overlap(set_1, set_2):\n    \"\"\"\n    Find the Jaccard Overlap (IoU) of every box combination between two sets of boxes that are in boundary coordinates.\n    :param set_1: set 1, a tensor of dimensions (n1, 4)\n    :param set_2: set 2, a tensor of dimensions (n2, 4)\n    :return: Jaccard Overlap of each of the boxes in set 1 with respect to each of the boxes in set 2, a tensor of dimensions (n1, n2)\n    \"\"\"\n\n    # Find intersections\n    intersection = find_intersection(set_1, set_2)  # (n1, n2)\n\n    # Find areas of each box in both sets\n    areas_set_1 = (set_1[:, 2] - set_1[:, 0]) * (set_1[:, 3] - set_1[:, 1])  # (n1)\n    areas_set_2 = (set_2[:, 2] - set_2[:, 0]) * (set_2[:, 3] - set_2[:, 1])  # (n2)\n\n    # Find the union\n    # PyTorch auto-broadcasts singleton dimensions\n    union = areas_set_1.unsqueeze(1) + areas_set_2.unsqueeze(0) - intersection  # (n1, n2)\n\n    return intersection / union  # (n1, n2)\n\ndef find_intersection(set_1, set_2):\n    \"\"\"\n    Find the intersection of every box combination between two sets of boxes that are in boundary coordinates.\n    :param set_1: set 1, a tensor of dimensions (n1, 4)\n    :param set_2: set 2, a tensor of dimensions (n2, 4)\n    :return: intersection of each of the boxes in set 1 with respect to each of the boxes in set 2, a tensor of dimensions (n1, n2)\n    \"\"\"\n\n    # PyTorch auto-broadcasts singleton dimensions\n    lower_bounds = torch.max(set_1[:, :2].unsqueeze(1), set_2[:, :2].unsqueeze(0))  # (n1, n2, 2)\n    upper_bounds = torch.min(set_1[:, 2:].unsqueeze(1), set_2[:, 2:].unsqueeze(0))  # (n1, n2, 2)\n    intersection_dims = torch.clamp(upper_bounds - lower_bounds, min=0)  # (n1, n2, 2)\n    return intersection_dims[:, :, 0] * intersection_dims[:, :, 1]  # (n1, n2)\n\ndef decimate(tensor, m):\n    \"\"\"\n    Decimate a tensor by a factor 'm', i.e. downsample by keeping every 'm'th value.\n    This is used when we convert FC layers to equivalent Convolutional layers, BUT of a smaller size.\n    :param tensor: tensor to be decimated\n    :param m: list of decimation factors for each dimension of the tensor; None if not to be decimated along a dimension\n    :return: decimated tensor\n    \"\"\"\n    assert tensor.dim() == len(m)\n    for d in range(tensor.dim()):\n        if m[d] is not None:\n            tensor = tensor.index_select(dim=d,\n                                         index=torch.arange(start=0, end=tensor.size(d), step=m[d]).long())\n\n    return tensor\n\ndef calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, true_difficulties):\n    \"\"\"\n    Calculate the Mean Average Precision (mAP) of detected objects.\n    See https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173 for an explanation\n    :param det_boxes: list of tensors, one tensor for each image containing detected objects' bounding boxes\n    :param det_labels: list of tensors, one tensor for each image containing detected objects' labels\n    :param det_scores: list of tensors, one tensor for each image containing detected objects' labels' scores\n    :param true_boxes: list of tensors, one tensor for each image containing actual objects' bounding boxes\n    :param true_labels: list of tensors, one tensor for each image containing actual objects' labels\n    :param true_difficulties: list of tensors, one tensor for each image containing actual objects' difficulty (0 or 1)\n    :return: list of average precisions for all classes, mean average precision (mAP)\n    \"\"\"\n    assert len(det_boxes) == len(det_labels) == len(det_scores) == len(true_boxes) == len(\n        true_labels) == len(\n        true_difficulties)  # these are all lists of tensors of the same length, i.e. number of images\n    n_classes = len(class_names)-1\n\n    # Store all (true) objects in a single continuous tensor while keeping track of the image it is from\n    true_images = list()\n    for i in range(len(true_labels)):\n        true_images.extend([i] * true_labels[i].size(0))\n    true_images = torch.LongTensor(true_images).to(device)  # (n_objects), n_objects is the total no. of objects across all images\n    true_boxes = torch.cat(true_boxes, dim=0)  # (n_objects, 4)\n    true_labels = torch.cat(true_labels, dim=0)  # (n_objects)\n    true_difficulties = torch.cat(true_difficulties, dim=0)  # (n_objects)\n\n    assert true_images.size(0) == true_boxes.size(0) == true_labels.size(0)\n\n    # Store all detections in a single continuous tensor while keeping track of the image it is from\n    det_images = list()\n    for i in range(len(det_labels)):\n        det_images.extend([i] * det_labels[i].size(0))\n    det_images = torch.LongTensor(det_images).to(device)  # (n_detections)\n    det_boxes = torch.cat(det_boxes, dim=0)  # (n_detections, 4)\n    det_labels = torch.cat(det_labels, dim=0)  # (n_detections)\n    det_scores = torch.cat(det_scores, dim=0)  # (n_detections)\n\n    assert det_images.size(0) == det_boxes.size(0) == det_labels.size(0) == det_scores.size(0)\n\n    # Calculate APs for each class (except background)\n    average_precisions = torch.zeros((n_classes - 1), dtype=torch.float)  # (n_classes - 1)\n    for c in range(1, n_classes):\n        # Extract only objects with this class\n        true_class_images = true_images[true_labels == c]  # (n_class_objects)\n        true_class_boxes = true_boxes[true_labels == c]  # (n_class_objects, 4)\n        true_class_difficulties = true_difficulties[true_labels == c]  # (n_class_objects)\n        n_easy_class_objects = (1 - true_class_difficulties).sum().item()  # ignore difficult objects\n\n        # Keep track of which true objects with this class have already been 'detected'\n        # So far, none\n        true_class_boxes_detected = torch.zeros((true_class_difficulties.size(0)), dtype=torch.uint8).to(device)  # (n_class_objects)\n\n        # Extract only detections with this class\n        det_class_images = det_images[det_labels == c]  # (n_class_detections)\n        det_class_boxes = det_boxes[det_labels == c]  # (n_class_detections, 4)\n        det_class_scores = det_scores[det_labels == c]  # (n_class_detections)\n        n_class_detections = det_class_boxes.size(0)\n        if n_class_detections == 0:\n            continue\n\n        # Sort detections in decreasing order of confidence/scores\n        det_class_scores, sort_ind = torch.sort(det_class_scores, dim=0, descending=True)  # (n_class_detections)\n        det_class_images = det_class_images[sort_ind]  # (n_class_detections)\n        det_class_boxes = det_class_boxes[sort_ind]  # (n_class_detections, 4)\n\n        # In the order of decreasing scores, check if true or false positive\n        true_positives = torch.zeros((n_class_detections), dtype=torch.float).to(device)  # (n_class_detections)\n        false_positives = torch.zeros((n_class_detections), dtype=torch.float).to(device)  # (n_class_detections)\n        for d in range(n_class_detections):\n            this_detection_box = det_class_boxes[d].unsqueeze(0)  # (1, 4)\n            this_image = det_class_images[d]  # (), scalar\n\n            # Find objects in the same image with this class, their difficulties, and whether they have been detected before\n            object_boxes = true_class_boxes[true_class_images == this_image]  # (n_class_objects_in_img)\n            object_difficulties = true_class_difficulties[true_class_images == this_image]  # (n_class_objects_in_img)\n            # If no such object in this image, then the detection is a false positive\n            if object_boxes.size(0) == 0:\n                false_positives[d] = 1\n                continue\n\n            # Find maximum overlap of this detection with objects in this image of this class\n            overlaps = find_jaccard_overlap(this_detection_box, object_boxes)  # (1, n_class_objects_in_img)\n            max_overlap, ind = torch.max(overlaps.squeeze(0), dim=0)  # (), () - scalars\n\n            # 'ind' is the index of the object in these image-level tensors 'object_boxes', 'object_difficulties'\n            # In the original class-level tensors 'true_class_boxes', etc., 'ind' corresponds to object with index...\n            original_ind = torch.LongTensor(range(true_class_boxes.size(0)))[true_class_images == this_image][ind]\n            # We need 'original_ind' to update 'true_class_boxes_detected'\n\n            # If the maximum overlap is greater than the threshold of 0.5, it's a match\n            if max_overlap.item() > 0.5:\n                # If the object it matched with is 'difficult', ignore it\n                if object_difficulties[ind] == 0:\n                    # If this object has already not been detected, it's a true positive\n                    if true_class_boxes_detected[original_ind] == 0:\n                        true_positives[d] = 1\n                        true_class_boxes_detected[original_ind] = 1  # this object has now been detected/accounted for\n                    # Otherwise, it's a false positive (since this object is already accounted for)\n                    else:\n                        false_positives[d] = 1\n            # Otherwise, the detection occurs in a different location than the actual object, and is a false positive\n            else:\n                false_positives[d] = 1\n\n        # Compute cumulative precision and recall at each detection in the order of decreasing scores\n        cumul_true_positives = torch.cumsum(true_positives, dim=0)  # (n_class_detections)\n        cumul_false_positives = torch.cumsum(false_positives, dim=0)  # (n_class_detections)\n        cumul_precision = cumul_true_positives / (\n                cumul_true_positives + cumul_false_positives + 1e-10)  # (n_class_detections)\n        cumul_recall = cumul_true_positives / n_easy_class_objects  # (n_class_detections)\n\n        # Find the mean of the maximum of the precisions corresponding to recalls above the threshold 't'\n        recall_thresholds = torch.arange(start=0, end=1.1, step=.1).tolist()  # (11)\n        precisions = torch.zeros((len(recall_thresholds)), dtype=torch.float).to(device)  # (11)\n        for i, t in enumerate(recall_thresholds):\n            recalls_above_t = cumul_recall >= t\n            if recalls_above_t.any():\n                precisions[i] = cumul_precision[recalls_above_t].max()\n            else:\n                precisions[i] = 0.\n        average_precisions[c - 1] = precisions.mean()  # c is in [1, n_classes - 1]\n\n    # Calculate Mean Average Precision (mAP)\n    mean_average_precision = average_precisions.mean().item()\n\n    # Keep class-wise average precisions in a dictionary\n    average_precisions = {id_to_class[c + 1]: v for c, v in enumerate(average_precisions.tolist())}\n\n    return average_precisions, mean_average_precision","metadata":{"_uuid":"ae40b7cd21d340e3d97fa64e60c527741371c73c","execution":{"iopub.status.busy":"2022-11-08T10:59:26.951355Z","iopub.execute_input":"2022-11-08T10:59:26.951735Z","iopub.status.idle":"2022-11-08T10:59:26.992524Z","shell.execute_reply.started":"2022-11-08T10:59:26.951702Z","shell.execute_reply":"2022-11-08T10:59:26.991644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# model","metadata":{"_uuid":"e298fde0b3ebc1e5ce5f0bad612c145f3193348a","trusted":true}},{"cell_type":"code","source":"\nclass VGGBase(nn.Module):\n    \"\"\"\n    VGG base convolutions to produce lower-level feature maps.\n    \"\"\"\n\n    def __init__(self):\n        super(VGGBase, self).__init__()\n\n        # Standard convolutional layers in VGG16\n        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)  # stride = 1, by default\n        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)  # ceiling (not floor) here for even dims\n\n        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)  # retains size because stride is 1 (and padding)\n\n        # Replacements for FC6 and FC7 in VGG16\n        self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)  # atrous convolution\n\n        self.conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n\n        # Load pretrained layers\n        self.load_pretrained_layers()\n\n    def forward(self, image):\n        \"\"\"\n        Forward propagation.\n        :param image: images, a tensor of dimensions (N, 3, 300, 300)\n        :return: lower-level feature maps conv4_3 and conv7\n        \"\"\"\n        out = F.relu(self.conv1_1(image))  # (N, 64, 300, 300)\n        out = F.relu(self.conv1_2(out))  # (N, 64, 300, 300)\n        out = self.pool1(out)  # (N, 64, 150, 150)\n\n        out = F.relu(self.conv2_1(out))  # (N, 128, 150, 150)\n        out = F.relu(self.conv2_2(out))  # (N, 128, 150, 150)\n        out = self.pool2(out)  # (N, 128, 75, 75)\n\n        out = F.relu(self.conv3_1(out))  # (N, 256, 75, 75)\n        out = F.relu(self.conv3_2(out))  # (N, 256, 75, 75)\n        out = F.relu(self.conv3_3(out))  # (N, 256, 75, 75)\n        out = self.pool3(out)  # (N, 256, 38, 38), it would have been 37 if not for ceil_mode = True\n\n        out = F.relu(self.conv4_1(out))  # (N, 512, 38, 38)\n        out = F.relu(self.conv4_2(out))  # (N, 512, 38, 38)\n        out = F.relu(self.conv4_3(out))  # (N, 512, 38, 38)\n        conv4_3_feats = out  # (N, 512, 38, 38)\n        out = self.pool4(out)  # (N, 512, 19, 19)\n\n        out = F.relu(self.conv5_1(out))  # (N, 512, 19, 19)\n        out = F.relu(self.conv5_2(out))  # (N, 512, 19, 19)\n        out = F.relu(self.conv5_3(out))  # (N, 512, 19, 19)\n        out = self.pool5(out)  # (N, 512, 19, 19), pool5 does not reduce dimensions\n\n        out = F.relu(self.conv6(out))  # (N, 1024, 19, 19)\n\n        conv7_feats = F.relu(self.conv7(out))  # (N, 1024, 19, 19)\n\n        # Lower-level feature maps\n        return conv4_3_feats, conv7_feats\n\n    def load_pretrained_layers(self):\n        \"\"\"\n        As in the paper, we use a VGG-16 pretrained on the ImageNet task as the base network.\n        There's one available in PyTorch, see https://pytorch.org/docs/stable/torchvision/models.html#torchvision.models.vgg16\n        We copy these parameters into our network. It's straightforward for conv1 to conv5.\n        However, the original VGG-16 does not contain the conv6 and con7 layers.\n        Therefore, we convert fc6 and fc7 into convolutional layers, and subsample by decimation. See 'decimate' in utils.py.\n        \"\"\"\n        # Current state of base\n        state_dict = self.state_dict()\n        param_names = list(state_dict.keys())\n\n        # Pretrained VGG base\n        pretrained_state_dict = torchvision.models.vgg16(pretrained=True).state_dict()\n        pretrained_param_names = list(pretrained_state_dict.keys())\n\n        # Transfer conv. parameters from pretrained model to current model\n        for i, param in enumerate(param_names[:-4]):  # excluding conv6 and conv7 parameters\n            state_dict[param] = pretrained_state_dict[pretrained_param_names[i]]\n\n        # Convert fc6, fc7 to convolutional layers, and subsample (by decimation) to sizes of conv6 and conv7\n        # fc6\n        conv_fc6_weight = pretrained_state_dict['classifier.0.weight'].view(4096, 512, 7, 7)  # (4096, 512, 7, 7)\n        conv_fc6_bias = pretrained_state_dict['classifier.0.bias']  # (4096)\n        state_dict['conv6.weight'] = decimate(conv_fc6_weight, m=[4, None, 3, 3])  # (1024, 512, 3, 3)\n        state_dict['conv6.bias'] = decimate(conv_fc6_bias, m=[4])  # (1024)\n        # fc7\n        conv_fc7_weight = pretrained_state_dict['classifier.3.weight'].view(4096, 4096, 1, 1)  # (4096, 4096, 1, 1)\n        conv_fc7_bias = pretrained_state_dict['classifier.3.bias']  # (4096)\n        state_dict['conv7.weight'] = decimate(conv_fc7_weight, m=[4, 4, None, None])  # (1024, 1024, 1, 1)\n        state_dict['conv7.bias'] = decimate(conv_fc7_bias, m=[4])  # (1024)\n\n        # Note: an FC layer of size (K) operating on a flattened version (C*H*W) of a 2D image of size (C, H, W)...\n        # ...is equivalent to a convolutional layer with kernel size (H, W), input channels C, output channels K...\n        # ...operating on the 2D image of size (C, H, W) without padding\n\n        self.load_state_dict(state_dict)\n\n        print(\"\\nLoaded base model.\\n\")\n        \n\nclass AuxiliaryConvolutions(nn.Module):\n    def __init__(self):\n        super(AuxiliaryConvolutions, self).__init__()\n        self.conv8_1 = nn.Conv2d(1024, 256, kernel_size=1, padding=0)  # stride = 1, by default\n        self.conv8_2 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)  # dim. reduction because stride > 1\n\n        self.conv9_1 = nn.Conv2d(512, 128, kernel_size=1, padding=0)\n        self.conv9_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)  # dim. reduction because stride > 1\n\n        self.conv10_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)\n        self.conv10_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)  # dim. reduction because padding = 0\n\n        self.conv11_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)\n        self.conv11_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)  # dim. reduction because padding = 0\n        \n        self.init_conv2d()\n        \n    def init_conv2d(self):\n        \"\"\"\n        Initialize convolution parameters.\n        \"\"\"\n        for c in self.children():\n            if isinstance(c, nn.Conv2d):\n                nn.init.xavier_uniform_(c.weight)\n                nn.init.constant_(c.bias, 0.)\n        \n    def forward(self, conv7_feats):\n        out = F.relu(self.conv8_1(conv7_feats))  # (N, 256, 19, 19)\n        out = F.relu(self.conv8_2(out))  # (N, 512, 10, 10)\n        conv8_2_feats = out  # (N, 512, 10, 10)\n\n        out = F.relu(self.conv9_1(out))  # (N, 128, 10, 10)\n        out = F.relu(self.conv9_2(out))  # (N, 256, 5, 5)\n        conv9_2_feats = out  # (N, 256, 5, 5)\n\n        out = F.relu(self.conv10_1(out))  # (N, 128, 5, 5)\n        out = F.relu(self.conv10_2(out))  # (N, 256, 3, 3)\n        conv10_2_feats = out  # (N, 256, 3, 3)\n\n        out = F.relu(self.conv11_1(out))  # (N, 128, 3, 3)\n        conv11_2_feats = F.relu(self.conv11_2(out))  # (N, 256, 1, 1)\n\n        # Higher-level feature maps\n        return conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats\n    \nclass PredictionConvolutions(nn.Module):\n    def __init__(self, n_classes):\n        super(PredictionConvolutions, self).__init__()\n        self.n_classes = n_classes\n        \n        n_boxes = {'conv4_3': 4,\n                   'conv7': 6,\n                   'conv8_2': 6,\n                   'conv9_2': 6,\n                   'conv10_2': 4,\n                   'conv11_2': 4}\n        \n        # Localization prediction convolutions (predict offsets w.r.t prior-boxes)\n        self.loc_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * 4, kernel_size=3, padding=1)\n        self.loc_conv7 = nn.Conv2d(1024, n_boxes['conv7'] * 4, kernel_size=3, padding=1)\n        self.loc_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * 4, kernel_size=3, padding=1)\n        self.loc_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * 4, kernel_size=3, padding=1)\n        self.loc_conv10_2 = nn.Conv2d(256, n_boxes['conv10_2'] * 4, kernel_size=3, padding=1)\n        self.loc_conv11_2 = nn.Conv2d(256, n_boxes['conv11_2'] * 4, kernel_size=3, padding=1)\n\n        # Class prediction convolutions (predict classes in localization boxes)\n        self.cl_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * n_classes, kernel_size=3, padding=1)\n        self.cl_conv7 = nn.Conv2d(1024, n_boxes['conv7'] * n_classes, kernel_size=3, padding=1)\n        self.cl_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * n_classes, kernel_size=3, padding=1)\n        self.cl_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * n_classes, kernel_size=3, padding=1)\n        self.cl_conv10_2 = nn.Conv2d(256, n_boxes['conv10_2'] * n_classes, kernel_size=3, padding=1)\n        self.cl_conv11_2 = nn.Conv2d(256, n_boxes['conv11_2'] * n_classes, kernel_size=3, padding=1)\n        \n        self.init_conv2d()\n        \n    def init_conv2d(self):\n        for c in self.children():\n            if isinstance(c, nn.Conv2d):\n                nn.init.xavier_uniform_(c.weight)\n                nn.init.constant_(c.bias, 0.)\n\n    def forward(self, conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats):\n        batch_size = conv4_3_feats.size(0)\n\n        # Predict localization boxes' bounds (as offsets w.r.t prior-boxes)\n        l_conv4_3 = self.loc_conv4_3(conv4_3_feats)  # (N, 16, 38, 38)\n        l_conv4_3 = l_conv4_3.permute(0, 2, 3, 1).contiguous()  # (N, 38, 38, 16), to match prior-box order (after .view())\n        # (.contiguous() ensures it is stored in a contiguous chunk of memory, needed for .view() below)\n        l_conv4_3 = l_conv4_3.view(batch_size, -1, 4)  # (N, 5776, 4), there are a total 5776 boxes on this feature map\n\n        l_conv7 = self.loc_conv7(conv7_feats)  # (N, 24, 19, 19)\n        l_conv7 = l_conv7.permute(0, 2, 3, 1).contiguous()  # (N, 19, 19, 24)\n        l_conv7 = l_conv7.view(batch_size, -1, 4)  # (N, 2166, 4), there are a total 2116 boxes on this feature map\n\n        l_conv8_2 = self.loc_conv8_2(conv8_2_feats)  # (N, 24, 10, 10)\n        l_conv8_2 = l_conv8_2.permute(0, 2, 3, 1).contiguous()  # (N, 10, 10, 24)\n        l_conv8_2 = l_conv8_2.view(batch_size, -1, 4)  # (N, 600, 4)\n\n        l_conv9_2 = self.loc_conv9_2(conv9_2_feats)  # (N, 24, 5, 5)\n        l_conv9_2 = l_conv9_2.permute(0, 2, 3, 1).contiguous()  # (N, 5, 5, 24)\n        l_conv9_2 = l_conv9_2.view(batch_size, -1, 4)  # (N, 150, 4)\n\n        l_conv10_2 = self.loc_conv10_2(conv10_2_feats)  # (N, 16, 3, 3)\n        l_conv10_2 = l_conv10_2.permute(0, 2, 3, 1).contiguous()  # (N, 3, 3, 16)\n        l_conv10_2 = l_conv10_2.view(batch_size, -1, 4)  # (N, 36, 4)\n\n        l_conv11_2 = self.loc_conv11_2(conv11_2_feats)  # (N, 16, 1, 1)\n        l_conv11_2 = l_conv11_2.permute(0, 2, 3, 1).contiguous()  # (N, 1, 1, 16)\n        l_conv11_2 = l_conv11_2.view(batch_size, -1, 4)  # (N, 4, 4)\n\n        # Predict classes in localization boxes\n        c_conv4_3 = self.cl_conv4_3(conv4_3_feats)  # (N, 4 * n_classes, 38, 38)\n        c_conv4_3 = c_conv4_3.permute(0, 2, 3, 1).contiguous()  # (N, 38, 38, 4 * n_classes), to match prior-box order (after .view())\n        c_conv4_3 = c_conv4_3.view(batch_size, -1, self.n_classes)  # (N, 5776, n_classes), there are a total 5776 boxes on this feature map\n\n        c_conv7 = self.cl_conv7(conv7_feats)  # (N, 6 * n_classes, 19, 19)\n        c_conv7 = c_conv7.permute(0, 2, 3, 1).contiguous()  # (N, 19, 19, 6 * n_classes)\n        c_conv7 = c_conv7.view(batch_size, -1, self.n_classes)  # (N, 2166, n_classes), there are a total 2116 boxes on this feature map\n\n        c_conv8_2 = self.cl_conv8_2(conv8_2_feats)  # (N, 6 * n_classes, 10, 10)\n        c_conv8_2 = c_conv8_2.permute(0, 2, 3, 1).contiguous()  # (N, 10, 10, 6 * n_classes)\n        c_conv8_2 = c_conv8_2.view(batch_size, -1, self.n_classes)  # (N, 600, n_classes)\n\n        c_conv9_2 = self.cl_conv9_2(conv9_2_feats)  # (N, 6 * n_classes, 5, 5)\n        c_conv9_2 = c_conv9_2.permute(0, 2, 3, 1).contiguous()  # (N, 5, 5, 6 * n_classes)\n        c_conv9_2 = c_conv9_2.view(batch_size, -1, self.n_classes)  # (N, 150, n_classes)\n\n        c_conv10_2 = self.cl_conv10_2(conv10_2_feats)  # (N, 4 * n_classes, 3, 3)\n        c_conv10_2 = c_conv10_2.permute(0, 2, 3, 1).contiguous()  # (N, 3, 3, 4 * n_classes)\n        c_conv10_2 = c_conv10_2.view(batch_size, -1, self.n_classes)  # (N, 36, n_classes)\n\n        c_conv11_2 = self.cl_conv11_2(conv11_2_feats)  # (N, 4 * n_classes, 1, 1)\n        c_conv11_2 = c_conv11_2.permute(0, 2, 3, 1).contiguous()  # (N, 1, 1, 4 * n_classes)\n        c_conv11_2 = c_conv11_2.view(batch_size, -1, self.n_classes)  # (N, 4, n_classes)\n\n        # A total of 8732 boxes\n        # Concatenate in this specific order (i.e. must match the order of the prior-boxes)\n        locs = torch.cat([l_conv4_3, l_conv7, l_conv8_2, l_conv9_2, l_conv10_2, l_conv11_2], dim=1)  # (N, 8732, 4)\n        classes_scores = torch.cat([c_conv4_3, c_conv7, c_conv8_2, c_conv9_2, c_conv10_2, c_conv11_2], dim=1)  # (N, 8732, n_classes)\n\n        return locs, classes_scores\n    \nclass SSD(nn.Module):\n    def __init__(self, n_classes=2):\n        super(SSD, self).__init__()\n        self.n_classes = n_classes\n        \n        self.base = VGGBase().to(device)\n        self.aux_convs = AuxiliaryConvolutions().to(device)\n        self.pred_convs = PredictionConvolutions(n_classes).to(device)\n        \n        self.rescale_factors = nn.Parameter(torch.FloatTensor(1, 512, 1, 1))  # there are 512 channels in conv4_3_feats\n        nn.init.constant_(self.rescale_factors, 20)\n\n        # Prior boxes\n        self.priors_cxcy = self.create_prior_boxes()\n        \n    def forward(self, image):\n        \"\"\"\n        Forward propagation.\n        :param image: images, a tensor of dimensions (N, 3, 300, 300)\n        :return: 8732 locations and class scores (i.e. w.r.t each prior box) for each image\n        \"\"\"\n        # Run VGG base network convolutions (lower level feature map generators)\n        conv4_3_feats, conv7_feats = self.base(image)  # (N, 512, 38, 38), (N, 1024, 19, 19)\n\n        # Rescale conv4_3 after L2 norm\n        norm = conv4_3_feats.pow(2).sum(dim=1, keepdim=True).sqrt()  # (N, 1, 38, 38)\n        conv4_3_feats = conv4_3_feats / norm  # (N, 512, 38, 38)\n        conv4_3_feats = conv4_3_feats * self.rescale_factors  # (N, 512, 38, 38)\n        # (PyTorch autobroadcasts singleton dimensions during arithmetic)\n\n        # Run auxiliary convolutions (higher level feature map generators)\n        conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats = \\\n            self.aux_convs(conv7_feats)  # (N, 512, 10, 10),  (N, 256, 5, 5), (N, 256, 3, 3), (N, 256, 1, 1)\n\n        # Run prediction convolutions (predict offsets w.r.t prior-boxes and classes in each resulting localization box)\n        locs, classes_scores = self.pred_convs(conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats,\n                                               conv11_2_feats)  # (N, 8732, 4), (N, 8732, n_classes)\n\n        return locs, classes_scores\n\n    \n    def create_prior_boxes(self):\n        \"\"\"\n        Create the 8732 prior (default) boxes for the SSD300, as defined in the paper.\n        :return: prior boxes in center-size coordinates, a tensor of dimensions (8732, 4)\n        \"\"\"\n        fmap_dims = {'conv4_3': 38,\n                     'conv7': 19,\n                     'conv8_2': 10,\n                     'conv9_2': 5,\n                     'conv10_2': 3,\n                     'conv11_2': 1}\n\n        obj_scales = {'conv4_3': 0.1,\n                      'conv7': 0.2,\n                      'conv8_2': 0.375,\n                      'conv9_2': 0.55,\n                      'conv10_2': 0.725,\n                      'conv11_2': 0.9}\n\n        aspect_ratios = {'conv4_3': [1., 2., 0.5],\n                         'conv7': [1., 2., 3., 0.5, .333],\n                         'conv8_2': [1., 2., 3., 0.5, .333],\n                         'conv9_2': [1., 2., 3., 0.5, .333],\n                         'conv10_2': [1., 2., 0.5],\n                         'conv11_2': [1., 2., 0.5]}\n\n        fmaps = list(fmap_dims.keys())\n\n        prior_boxes = []\n\n        for k, fmap in enumerate(fmaps):\n            for i in range(fmap_dims[fmap]):\n                for j in range(fmap_dims[fmap]):\n                    cx = (j + 0.5) / fmap_dims[fmap]\n                    cy = (i + 0.5) / fmap_dims[fmap]\n\n                    for ratio in aspect_ratios[fmap]:\n                        prior_boxes.append([cx, cy, obj_scales[fmap] * sqrt(ratio), obj_scales[fmap] / sqrt(ratio)])\n\n                        # For an aspect ratio of 1, use an additional prior whose scale is the geometric mean of the\n                        # scale of the current feature map and the scale of the next feature map\n                        if ratio == 1.:\n                            try:\n                                additional_scale = sqrt(obj_scales[fmap] * obj_scales[fmaps[k + 1]])\n                            # For the last feature map, there is no \"next\" feature map\n                            except IndexError:\n                                additional_scale = 1.\n                            prior_boxes.append([cx, cy, additional_scale, additional_scale])\n\n        prior_boxes = torch.FloatTensor(prior_boxes).to(device)  # (8732, 4)\n        prior_boxes.clamp_(0, 1)  # (8732, 4)\n\n        return prior_boxes\n    \n    def detect_objects(self, predicted_locs, predicted_scores, min_score, max_overlap, top_k):\n        \"\"\"\n        Decipher the 8732 locations and class scores (output of ths SSD300) to detect objects.\n        For each class, perform Non-Maximum Suppression (NMS) on boxes that are above a minimum threshold.\n        :param predicted_locs: predicted locations/boxes w.r.t the 8732 prior boxes, a tensor of dimensions (N, 8732, 4)\n        :param predicted_scores: class scores for each of the encoded locations/boxes, a tensor of dimensions (N, 8732, n_classes)\n        :param min_score: minimum threshold for a box to be considered a match for a certain class\n        :param max_overlap: maximum overlap two boxes can have so that the one with the lower score is not suppressed via NMS\n        :param top_k: if there are a lot of resulting detection across all classes, keep only the top 'k'\n        :return: detections (boxes, labels, and scores), lists of length batch_size\n        \"\"\"\n        batch_size = predicted_locs.size(0)\n        n_priors = self.priors_cxcy.size(0)\n#         print(n_priors)\n        predicted_scores = F.softmax(predicted_scores, dim=2)  # (N, 8732, n_classes)\n\n        # Lists to store final predicted boxes, labels, and scores for all images\n        all_images_boxes = list()\n        all_images_labels = list()\n        all_images_scores = list()\n\n        assert n_priors == predicted_locs.size(1) == predicted_scores.size(1)\n\n        for i in range(batch_size):\n            # Decode object coordinates from the form we regressed predicted boxes to\n            decoded_locs = cxcy_to_xy(\n                gcxgcy_to_cxcy(predicted_locs[i], self.priors_cxcy))  # (8732, 4), these are fractional pt. coordinates\n\n            # Lists to store boxes and scores for this image\n            image_boxes = list()\n            image_labels = list()\n            image_scores = list()\n\n            max_scores, best_label = predicted_scores[i].max(dim=1)  # (8732)\n\n            # Check for each class\n            for c in range(1, self.n_classes):\n                # Keep only predicted boxes and scores where scores for this class are above the minimum score\n                class_scores = predicted_scores[i][:, c]  # (8732)\n                score_above_min_score = class_scores > min_score  # torch.uint8 (byte) tensor, for indexing\n                n_above_min_score = score_above_min_score.sum().item()\n                if n_above_min_score == 0:\n                    continue\n                class_scores = class_scores[score_above_min_score]  # (n_qualified), n_min_score <= 8732\n                class_decoded_locs = decoded_locs[score_above_min_score]  # (n_qualified, 4)\n\n                # Sort predicted boxes and scores by scores\n                class_scores, sort_ind = class_scores.sort(dim=0, descending=True)  # (n_qualified), (n_min_score)\n                class_decoded_locs = class_decoded_locs[sort_ind]  # (n_min_score, 4)\n\n                # Find the overlap between predicted boxes\n                overlap = find_jaccard_overlap(class_decoded_locs, class_decoded_locs)  # (n_qualified, n_min_score)\n\n                # Non-Maximum Suppression (NMS)\n\n                # A torch.uint8 (byte) tensor to keep track of which predicted boxes to suppress\n                # 1 implies suppress, 0 implies don't suppress\n                suppress = torch.zeros((n_above_min_score), dtype=torch.uint8).to(device)  # (n_qualified)\n\n                # Consider each box in order of decreasing scores\n                for box in range(class_decoded_locs.size(0)):\n                    # If this box is already marked for suppression\n                    if suppress[box] == 1:\n                        continue\n\n                    # Suppress boxes whose overlaps (with this box) are greater than maximum overlap\n                    # Find such boxes and update suppress indices\n                    suppress = torch.max(suppress, overlap[box] > max_overlap)\n                    # The max operation retains previously suppressed boxes, like an 'OR' operation\n\n                    # Don't suppress this box, even though it has an overlap of 1 with itself\n                    suppress[box] = 0\n\n                # Store only unsuppressed boxes for this class\n                image_boxes.append(class_decoded_locs[1 - suppress])\n                image_labels.append(torch.LongTensor((1 - suppress).sum().item() * [c]).to(device))\n                image_scores.append(class_scores[1 - suppress])\n\n            # If no object in any class is found, store a placeholder for 'background'\n            if len(image_boxes) == 0:\n                image_boxes.append(torch.FloatTensor([[0., 0., 1., 1.]]).to(device))\n                image_labels.append(torch.LongTensor([0]).to(device))\n                image_scores.append(torch.FloatTensor([0.]).to(device))\n\n            # Concatenate into single tensors\n            image_boxes = torch.cat(image_boxes, dim=0)  # (n_objects, 4)\n            image_labels = torch.cat(image_labels, dim=0)  # (n_objects)\n            image_scores = torch.cat(image_scores, dim=0)  # (n_objects)\n            n_objects = image_scores.size(0)\n\n            # Keep only the top k objects\n            if n_objects > top_k:\n                image_scores, sort_ind = image_scores.sort(dim=0, descending=True)\n                image_scores = image_scores[:top_k]  # (top_k)\n                image_boxes = image_boxes[sort_ind][:top_k]  # (top_k, 4)\n                image_labels = image_labels[sort_ind][:top_k]  # (top_k)\n\n            # Append to lists that store predicted boxes and scores for all images\n            all_images_boxes.append(image_boxes)\n            all_images_labels.append(image_labels)\n            all_images_scores.append(image_scores)\n\n        return all_images_boxes, all_images_labels, all_images_scores  # lists of length batch_size","metadata":{"_uuid":"161e60105f3905088bbb63d249b40bc387e92bf1","execution":{"iopub.status.busy":"2022-11-08T10:59:29.529599Z","iopub.execute_input":"2022-11-08T10:59:29.529947Z","iopub.status.idle":"2022-11-08T10:59:29.601172Z","shell.execute_reply.started":"2022-11-08T10:59:29.529916Z","shell.execute_reply":"2022-11-08T10:59:29.600165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiBoxLoss(nn.Module):\n    \"\"\"\n    The MultiBox loss, a loss function for object detection.\n    This is a combination of:\n    (1) a localization loss for the predicted locations of the boxes, and\n    (2) a confidence loss for the predicted class scores.\n    \"\"\"\n\n    def __init__(self, priors_cxcy, threshold=0.5, neg_pos_ratio=3, alpha=1.):\n        super(MultiBoxLoss, self).__init__()\n        self.priors_cxcy = priors_cxcy\n        self.priors_xy = cxcy_to_xy(priors_cxcy)\n        self.threshold = threshold\n        self.neg_pos_ratio = neg_pos_ratio\n        self.alpha = alpha\n\n        self.smooth_l1 = nn.L1Loss()  # SmoothL1Loss(), MSELoss\n        self.cross_entropy = nn.CrossEntropyLoss(reduction='none')  # reduction?\n#         self.cross_entropy = nn.BCELoss()\n\n    def forward(self, predicted_locs, predicted_scores, boxes, labels):\n        \"\"\"\n        Forward propagation.\n        :param predicted_locs: predicted locations/boxes w.r.t the 8732 prior boxes, a tensor of dimensions (N, 8732, 4)\n        :param predicted_scores: class scores for each of the encoded locations/boxes, a tensor of dimensions (N, 8732, n_classes)\n        :param boxes: true  object bounding boxes in boundary coordinates, a list of N tensors\n        :param labels: true object labels, a list of N tensors\n        :return: multibox loss, a scalar\n        \"\"\"\n        batch_size = predicted_locs.size(0)\n        n_priors = self.priors_cxcy.size(0)\n        n_classes = predicted_scores.size(2)\n\n        assert n_priors == predicted_locs.size(1) == predicted_scores.size(1)\n\n        true_locs = torch.zeros((batch_size, n_priors, 4), dtype=torch.float).to(device)  # (N, 8732, 4)\n        true_classes = torch.zeros((batch_size, n_priors), dtype=torch.long).to(device)  # (N, 8732)\n\n        # For each image\n        for i in range(batch_size):\n            n_objects = boxes[i].size(0)\n\n            overlap = find_jaccard_overlap(boxes[i], self.priors_xy)  # (n_objects, 8732)\n\n            # For each prior, find the object that has the maximum overlap\n            overlap_for_each_prior, object_for_each_prior = overlap.max(dim=0)  # (8732)\n\n            # We don't want a situation where an object is not represented in our positive (non-background) priors -\n            # 1. An object might not be the best object for all priors, and is therefore not in object_for_each_prior.\n            # 2. All priors with the object may be assigned as background based on the threshold (0.5).\n\n            # To remedy this -\n            # First, find the prior that has the maximum overlap for each object.\n            _, prior_for_each_object = overlap.max(dim=1)  # (N_o)\n\n            # Then, assign each object to the corresponding maximum-overlap-prior. (This fixes 1.)\n            object_for_each_prior[prior_for_each_object] = torch.LongTensor(range(n_objects)).to(device)\n\n            # To ensure these priors qualify, artificially give them an overlap of greater than 0.5. (This fixes 2.)\n            overlap_for_each_prior[prior_for_each_object] = 1.\n\n            # Labels for each prior\n            label_for_each_prior = labels[i][object_for_each_prior]  # (8732)\n            # Set priors whose overlaps with objects are less than the threshold to be background (no object)\n            label_for_each_prior[overlap_for_each_prior < self.threshold] = 0  # (8732)\n\n            # Store\n            true_classes[i] = label_for_each_prior\n\n            # Encode center-size object coordinates into the form we regressed predicted boxes to\n            true_locs[i] = cxcy_to_gcxgcy(xy_to_cxcy(boxes[i][object_for_each_prior]), self.priors_cxcy)  # (8732, 4)\n\n        # Identify priors that are positive (object/non-background)\n        positive_priors = true_classes != 0  # (N, 8732)\n\n        # LOCALIZATION LOSS\n\n        # Localization loss is computed only over positive (non-background) priors\n        loc_loss = self.smooth_l1(predicted_locs[positive_priors], true_locs[positive_priors])  # (), scalar\n\n        # Note: indexing with a torch.uint8 (byte) tensor flattens the tensor when indexing is across multiple dimensions (N & 8732)\n        # So, if predicted_locs has the shape (N, 8732, 4), predicted_locs[positive_priors] will have (total positives, 4)\n\n        # CONFIDENCE LOSS\n\n        # Confidence loss is computed over positive priors and the most difficult (hardest) negative priors in each image\n        # That is, FOR EACH IMAGE,\n        # we will take the hardest (neg_pos_ratio * n_positives) negative priors, i.e where there is maximum loss\n        # This is called Hard Negative Mining - it concentrates on hardest negatives in each image, and also minimizes pos/neg imbalance\n\n        # Number of positive and hard-negative priors per image\n        n_positives = positive_priors.sum(dim=1)  # (N)\n        n_hard_negatives = self.neg_pos_ratio * n_positives  # (N)\n\n        # First, find the loss for all priors\n        conf_loss_all = self.cross_entropy(predicted_scores.view(-1, n_classes), true_classes.view(-1))  # (N * 8732)\n        conf_loss_all = conf_loss_all.view(batch_size, n_priors)  # (N, 8732)\n\n        # We already know which priors are positive\n        conf_loss_pos = conf_loss_all[positive_priors]  # (sum(n_positives))\n\n        # Next, find which priors are hard-negative\n        # To do this, sort ONLY negative priors in each image in order of decreasing loss and take top n_hard_negatives\n        conf_loss_neg = conf_loss_all.clone()  # (N, 8732)\n        conf_loss_neg[positive_priors] = 0.  # (N, 8732), positive priors are ignored (never in top n_hard_negatives)\n        conf_loss_neg, _ = conf_loss_neg.sort(dim=1, descending=True)  # (N, 8732), sorted by decreasing hardness\n        hardness_ranks = torch.LongTensor(range(n_priors)).unsqueeze(0).expand_as(conf_loss_neg).to(device)  # (N, 8732)\n        hard_negatives = hardness_ranks < n_hard_negatives.unsqueeze(1)  # (N, 8732)\n        conf_loss_hard_neg = conf_loss_neg[hard_negatives]  # (sum(n_hard_negatives))\n\n        # As in the paper, averaged over positive priors only, although computed over both positive and hard-negative priors\n        conf_loss = (conf_loss_hard_neg.sum() + conf_loss_pos.sum()) / n_positives.sum().float()  # (), scalar\n\n        # TOTAL LOSS\n\n        return conf_loss + self.alpha * loc_loss","metadata":{"_uuid":"55c5e16198c64706f5a418912cc38283238bf230","execution":{"iopub.status.busy":"2022-11-08T10:59:30.500937Z","iopub.execute_input":"2022-11-08T10:59:30.501317Z","iopub.status.idle":"2022-11-08T10:59:30.518836Z","shell.execute_reply.started":"2022-11-08T10:59:30.501284Z","shell.execute_reply":"2022-11-08T10:59:30.517725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{"_uuid":"6fa7d8a1dc6015be49f14dc45be504acce0e1636"}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"_uuid":"c98365724d8529f412ffb966a1841796a2ffdc23","execution":{"iopub.status.busy":"2022-11-08T10:59:32.336037Z","iopub.execute_input":"2022-11-08T10:59:32.336708Z","iopub.status.idle":"2022-11-08T10:59:32.394272Z","shell.execute_reply.started":"2022-11-08T10:59:32.336672Z","shell.execute_reply":"2022-11-08T10:59:32.393181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCH = 3\nLR = 1e-3\nBS = 8\nmomentum = 0.9  # momentum\nweight_decay = 5e-4  # weight decay\nprint_feq = 100","metadata":{"_uuid":"856eef5bb0248bfc58a7c5cd251e1ccd40abc7f7","execution":{"iopub.status.busy":"2022-11-08T10:59:33.691838Z","iopub.execute_input":"2022-11-08T10:59:33.692199Z","iopub.status.idle":"2022-11-08T10:59:33.697277Z","shell.execute_reply.started":"2022-11-08T10:59:33.692170Z","shell.execute_reply":"2022-11-08T10:59:33.696298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SSD(n_classes=len(class_names)).to(device)  # all cards + background\ncriterion = MultiBoxLoss(priors_cxcy=model.priors_cxcy).to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=momentum, weight_decay=weight_decay)  # AdamW with LR = 1e-4\n# scheduler (StepLR or Plateau with high patience), early stopping...","metadata":{"_uuid":"545e26c0e61f7bd0d76ba3474489e174057f7ecf","execution":{"iopub.status.busy":"2022-11-08T10:59:35.605125Z","iopub.execute_input":"2022-11-08T10:59:35.606153Z","iopub.status.idle":"2022-11-08T11:00:32.475983Z","shell.execute_reply.started":"2022-11-08T10:59:35.606104Z","shell.execute_reply":"2022-11-08T11:00:32.474977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = SSDDateset(train_img_paths, train_annot_paths, transform=tsfm)\ntrain_dl = DataLoader(train_ds, batch_size=BS, shuffle=True, collate_fn=train_ds.collate_fn)\n\nvalid_ds = SSDDateset(valid_img_paths, valid_annot_paths, subset='valid', transform=tsfm)\nvalid_dl = DataLoader(valid_ds, batch_size=BS, shuffle=True, collate_fn=valid_ds.collate_fn)","metadata":{"_uuid":"d49cfc6d2e2840cb48c467deeb522244554694ff","execution":{"iopub.status.busy":"2022-11-08T11:00:34.446820Z","iopub.execute_input":"2022-11-08T11:00:34.447174Z","iopub.status.idle":"2022-11-08T11:00:34.452907Z","shell.execute_reply.started":"2022-11-08T11:00:34.447145Z","shell.execute_reply":"2022-11-08T11:00:34.451892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport time\n\nfor epoch in range(1, EPOCH+1):\n    model.train()\n    train_loss = []\n    for step, (img, boxes, labels) in enumerate(train_dl):\n        time_1 = time.time()\n        img = img.to(device)\n\n        boxes = [box.to(device) for box in boxes]\n\n        labels = [label.to(device) for label in labels]\n        \n        pred_loc, pred_sco = model(img)\n        \n        loss = criterion(pred_loc, pred_sco, boxes, labels)\n        \n        # Backward propagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_loss.append(loss.item())\n        if step % print_feq == 0:\n            print('epoch:', epoch, \n                  '\\tstep:', step+1, '/', len(train_dl) + 1,\n                  '\\ttrain loss:', '{:.4f}'.format(loss.item()),\n                  '\\ttime:', '{:.4f}'.format((time.time()-time_1)*print_feq), 's')\n    \n    model.eval()\n    valid_loss = []\n    with torch.no_grad():\n        for step, (img, boxes, labels) in enumerate(tqdm(valid_dl)):\n            img = img.to(device)\n            boxes = [box.to(device) for box in boxes]\n            labels = [label.to(device) for label in labels]\n            pred_loc, pred_sco = model(img)\n            loss = criterion(pred_loc, pred_sco, boxes, labels)\n            valid_loss.append(loss.item())\n        \n    print('epoch:', epoch, '/', EPOCH+1,\n            '\\ttrain loss:', '{:.4f}'.format(np.mean(train_loss)),\n            '\\tvalid loss:', '{:.4f}'.format(np.mean(valid_loss)))","metadata":{"_uuid":"099af9f1a406ffb1ef3d40fe1c0067b68a956d67","scrolled":true,"execution":{"iopub.status.busy":"2022-11-08T11:00:36.376216Z","iopub.execute_input":"2022-11-08T11:00:36.376618Z","iopub.status.idle":"2022-11-08T11:47:10.834669Z","shell.execute_reply.started":"2022-11-08T11:00:36.376585Z","shell.execute_reply":"2022-11-08T11:47:10.833155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# test","metadata":{"_uuid":"ffd76e5ce2e1bb585dcf533e19d7730b07db5061","trusted":true}},{"cell_type":"code","source":"model.eval()\ndel train_ds, train_dl, valid_ds, valid_dl","metadata":{"_uuid":"382a8fe8845b3bafd01e25297c79818f4c1415f1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from random import randint\n\ndef test(n=5):\n    d = []\n    for i in range(5):\n        origin_img = Image.open('../input/playing-cards/valid/images/' + valid_img_paths[np.random.randint(len(valid_img_paths))]).convert('RGB')\n        img = tsfm(origin_img)\n\n        img = img.to(device)\n        predicted_locs, predicted_scores = model(img.unsqueeze(0))\n        det_boxes, det_labels, det_scores = model.detect_objects(predicted_locs, predicted_scores, min_score=0.2,\n                                                                 max_overlap=0.5, top_k=200)\n        det_boxes = det_boxes[0].to('cpu')\n        det_labels = det_labels[0].to('cpu')\n\n        origin_dims = torch.FloatTensor([origin_img.width, origin_img.height, origin_img.width, origin_img.height]).unsqueeze(0)\n        det_boxes = det_boxes * origin_dims\n\n        annotated_image = origin_img\n        draw = ImageDraw.Draw(annotated_image)\n        \n        for i in range(det_boxes.shape[0]):\n            box_location = det_boxes[i].tolist()\n            label = det_labels[i].item()\n            draw.rectangle(xy=box_location, outline='red')\n            draw.rectangle(xy=list(map(lambda x:x+1, box_location)), outline='red')\n            draw.text(xy=box_location[:2], text=id_to_class[int(label)])\n        d.append(annotated_image)\n    return d","metadata":{"_uuid":"f48b93a09cf71224c87fa8a28b8e4ba6e2dc36e0","execution":{"iopub.status.busy":"2022-11-08T11:47:28.862966Z","iopub.execute_input":"2022-11-08T11:47:28.863371Z","iopub.status.idle":"2022-11-08T11:47:28.876338Z","shell.execute_reply.started":"2022-11-08T11:47:28.863337Z","shell.execute_reply":"2022-11-08T11:47:28.875119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in test():\n    plt.figure(figsize=(7, 7))\n    plt.imshow(i)","metadata":{"_uuid":"51ec6cfdf83fa5bd5356921ea1174dfe4d3b61fb","execution":{"iopub.status.busy":"2022-11-08T11:47:49.578524Z","iopub.execute_input":"2022-11-08T11:47:49.578879Z","iopub.status.idle":"2022-11-08T11:47:51.476894Z","shell.execute_reply.started":"2022-11-08T11:47:49.578848Z","shell.execute_reply":"2022-11-08T11:47:51.476086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [Results](https://www.kaggle.com/code/pankratozzi/ssd300-with-pytorch?scriptVersionId=110393952)","metadata":{}}]}