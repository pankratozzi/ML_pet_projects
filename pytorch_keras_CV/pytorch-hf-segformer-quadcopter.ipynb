{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom tqdm.autonotebook import tqdm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nfrom glob import glob\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import SegformerFeatureExtractor\nfrom transformers import SegformerForSemanticSegmentation\nfrom huggingface_hub import cached_download, hf_hub_url\nfrom datasets import load_metric\n\nseed = 42\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f'Currently using \"{device.upper()}\" device.')\n\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\nbatch_size = 2\nnum_classes = 24\nepochs = 20\npath = r'segform_model.pth'","metadata":{"execution":{"iopub.status.busy":"2022-07-23T08:29:00.990046Z","iopub.execute_input":"2022-07-23T08:29:00.990853Z","iopub.status.idle":"2022-07-23T08:29:04.429028Z","shell.execute_reply.started":"2022-07-23T08:29:00.990360Z","shell.execute_reply":"2022-07-23T08:29:04.427854Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"[Link to example tutorial](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb#scrollTo=MbNeV9xdw7rm)","metadata":{}},{"cell_type":"code","source":"images_path = r'../input/semantic-drone-dataset/dataset/semantic_drone_dataset/original_images/'\ncolor_masks_path = r'../input/semantic-drone-dataset/RGB_color_image_masks/RGB_color_image_masks/'\nmasks_path = r'../input/semantic-drone-dataset/dataset/semantic_drone_dataset/label_images_semantic/'\n\nimages = glob(images_path + '*.jpg')\ncolored_masks = glob(color_masks_path + '*.png')\nmasks = glob(masks_path + '*.png')\n\nimages = sorted([str(p) for p in images])\ncolored_masks = sorted([str(p) for p in colored_masks])\nmasks = sorted([str(p) for p in masks])\n\npath_df = pd.DataFrame({'image': images, 'color_mask': colored_masks, 'mask': masks})\npath_df.sample(2)","metadata":{"execution":{"iopub.status.busy":"2022-07-23T08:29:04.431418Z","iopub.execute_input":"2022-07-23T08:29:04.432617Z","iopub.status.idle":"2022-07-23T08:29:04.939150Z","shell.execute_reply.started":"2022-07-23T08:29:04.432577Z","shell.execute_reply":"2022-07-23T08:29:04.938076Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"labels = pd.read_csv('../input/semantic-drone-dataset/class_dict_seg.csv')\nlabel_to_id = {v: k for k, v in enumerate(labels['name'].unique())}\nid_to_label = {v: k for k, v in label_to_id.items()}\n\npalette = []\nfor i in range(num_classes):\n    color = labels.iloc[i, 1:].values.tolist()\n    palette.append(color)","metadata":{"execution":{"iopub.status.busy":"2022-07-23T08:29:04.940719Z","iopub.execute_input":"2022-07-23T08:29:04.941328Z","iopub.status.idle":"2022-07-23T08:29:04.968729Z","shell.execute_reply.started":"2022-07-23T08:29:04.941292Z","shell.execute_reply":"2022-07-23T08:29:04.967886Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train, test = train_test_split(path_df, test_size=10, shuffle=True, random_state=42)\ntrain, valid = train_test_split(train, test_size=40, shuffle=True, random_state=42)\n\ntrain.reset_index(drop=True, inplace=True)\nvalid.reset_index(drop=True, inplace=True)\ntest.reset_index(drop=True, inplace=True)\n\nprint(f'Train size: {len(train)}, validation size: {len(valid)} and test size: {len(test)}')","metadata":{"execution":{"iopub.status.busy":"2022-07-23T08:29:04.971027Z","iopub.execute_input":"2022-07-23T08:29:04.971635Z","iopub.status.idle":"2022-07-23T08:29:04.982495Z","shell.execute_reply.started":"2022-07-23T08:29:04.971599Z","shell.execute_reply":"2022-07-23T08:29:04.981358Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"sample_img = path_df.sample(1, random_state=42)\n\nimage = Image.open(sample_img['image'].values[0])\ncmask = Image.open(sample_img['color_mask'].values[0])\nmask = Image.open(sample_img['mask'].values[0])\n\nplt.figure(figsize=(8, 4))\nplt.subplot(131)\nplt.imshow(image)\nplt.subplot(132)\nplt.imshow(cmask)\nplt.subplot(133)\nplt.imshow(mask)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-23T08:29:04.984124Z","iopub.execute_input":"2022-07-23T08:29:04.984486Z","iopub.status.idle":"2022-07-23T08:29:10.614151Z","shell.execute_reply.started":"2022-07-23T08:29:04.984452Z","shell.execute_reply":"2022-07-23T08:29:10.613214Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class DroneDataset(Dataset):\n    def __init__(self, dataframe, feature_extractor):\n        self.dataframe = dataframe\n        self.feature_extractor = feature_extractor\n        \n    def __len__(self):\n        return len(self.dataframe)\n    \n    def __getitem__(self, ix):\n        row = self.dataframe.loc[ix].squeeze()\n        image = Image.open(row['image'])        \n        mask = Image.open(row['mask'])\n        \n        encoded_inputs = self.feature_extractor(image, mask, return_tensors=\"pt\")\n        for k,v in encoded_inputs.items():\n            encoded_inputs[k].squeeze_()\n        \n        return encoded_inputs","metadata":{"execution":{"iopub.status.busy":"2022-07-23T08:29:10.616398Z","iopub.execute_input":"2022-07-23T08:29:10.616765Z","iopub.status.idle":"2022-07-23T08:29:10.625125Z","shell.execute_reply.started":"2022-07-23T08:29:10.616727Z","shell.execute_reply":"2022-07-23T08:29:10.624231Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"feature_extractor = SegformerFeatureExtractor(reduce_labels=True) # remove background class\n\ntrain_dataset = DroneDataset(train, feature_extractor)\nvalid_dataset = DroneDataset(valid, feature_extractor)","metadata":{"execution":{"iopub.status.busy":"2022-07-23T08:29:10.626540Z","iopub.execute_input":"2022-07-23T08:29:10.627093Z","iopub.status.idle":"2022-07-23T08:29:10.634227Z","shell.execute_reply.started":"2022-07-23T08:29:10.627056Z","shell.execute_reply":"2022-07-23T08:29:10.633251Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-07-23T08:29:10.635799Z","iopub.execute_input":"2022-07-23T08:29:10.636509Z","iopub.status.idle":"2022-07-23T08:29:10.642264Z","shell.execute_reply.started":"2022-07-23T08:29:10.636471Z","shell.execute_reply":"2022-07-23T08:29:10.641288Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b0\",\n                                                         num_labels=num_classes, \n                                                         id2label=id_to_label, \n                                                         label2id=label_to_id,)","metadata":{"execution":{"iopub.status.busy":"2022-07-23T08:29:10.644205Z","iopub.execute_input":"2022-07-23T08:29:10.644586Z","iopub.status.idle":"2022-07-23T08:29:16.428801Z","shell.execute_reply.started":"2022-07-23T08:29:10.644548Z","shell.execute_reply":"2022-07-23T08:29:16.427782Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"metric_train = load_metric(\"mean_iou\")\nmetric_valid = load_metric(\"mean_iou\")\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.00006)\nmodel.to(device)\nprint()","metadata":{"execution":{"iopub.status.busy":"2022-07-23T08:29:16.431666Z","iopub.execute_input":"2022-07-23T08:29:16.431992Z","iopub.status.idle":"2022-07-23T08:29:21.993035Z","shell.execute_reply.started":"2022-07-23T08:29:16.431964Z","shell.execute_reply":"2022-07-23T08:29:21.991892Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef evaluate_test(model=model, test=test):\n    model.eval()\n    idx = np.random.randint(len(test))\n    image_p = test.loc[idx, 'image']\n    gt_mask_p = test.loc[idx, 'mask']\n    image = Image.open(image_p)\n    gt_mask = Image.open(gt_mask_p)\n    \n    encoding = feature_extractor(image, return_tensors=\"pt\")\n    pixel_values = encoding.pixel_values.to(device)\n    \n    outputs = model(pixel_values=pixel_values)\n    logits = outputs.logits.cpu()\n    upsampled_logits = nn.functional.interpolate(logits,\n                                                 size=image.size[::-1],\n                                                 mode='bilinear',\n                                                 align_corners=False)\n    seg = upsampled_logits.argmax(dim=1)[0]\n    color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8)\n    \n    np_palette = np.array(palette)\n    for label, color in enumerate(np_palette):\n        color_seg[seg == label, :] = color\n        \n    color_seg = color_seg[..., ::-1]\n\n    img = np.array(image) * 0.5 + color_seg * 0.5\n    img = img.astype(np.uint8)\n    \n    # GT\n    gt_map = np.array(gt_mask)\n    gt_map[gt_map == 0] = 255\n    gt_map = gt_map - 1\n    gt_map[gt_map == 254] = 255\n\n    classes_map = np.unique(gt_map).tolist()\n    unique_classes = [model.config.id2label[idx] if idx!=255 else None for idx in classes_map]\n\n    color_seg = np.zeros((gt_map.shape[0], gt_map.shape[1], 3), dtype=np.uint8)\n    for label, color in enumerate(np_palette):\n        color_seg[gt_map == label, :] = color\n\n    color_seg = color_seg[..., ::-1]\n    \n    gt_mask = np.array(image) * 0.5 + color_seg * 0.5\n    gt_mask = gt_mask.astype(np.uint8)\n\n    plt.figure(figsize=(12, 8))\n    plt.subplot(131)\n    plt.title('Predicted Image')\n    plt.imshow(img)\n    \n    plt.subplot(132)\n    plt.title('GT segmentation mask')\n    plt.imshow(gt_mask)\n    \n    plt.subplot(133)\n    plt.title('Original Image')\n    plt.imshow(image)\n    \n    plt.tight_layout()\n    plt.show()\n    plt.pause(0.01)","metadata":{"execution":{"iopub.status.busy":"2022-07-23T09:26:48.363144Z","iopub.execute_input":"2022-07-23T09:26:48.363511Z","iopub.status.idle":"2022-07-23T09:26:48.379247Z","shell.execute_reply.started":"2022-07-23T09:26:48.363479Z","shell.execute_reply":"2022-07-23T09:26:48.378307Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"for epoch in range(epochs):\n    print(\"Epoch:\", epoch+1)\n    model.train()\n    for idx, batch in enumerate(tqdm(train_dataloader, leave=False)):\n        pixel_values = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(pixel_values=pixel_values, labels=labels)\n        loss, logits = outputs.loss, outputs.logits\n        \n        loss.backward()\n        optimizer.step()\n\n        with torch.no_grad():\n            upsampled_logits = nn.functional.interpolate(logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n            predicted = upsampled_logits.argmax(dim=1)\n          \n            metric_train.add_batch(predictions=predicted.detach().cpu().numpy(), references=labels.detach().cpu().numpy())\n\n        if idx % 170 == 0:\n            metrics = metric_train.compute(num_labels=len(id_to_label), \n                                           ignore_index=255,\n                                           reduce_labels=False,\n          )\n\n            print(\"Loss:\", loss.item())\n            print(\"Mean_iou:\", metrics[\"mean_iou\"])\n            print(\"Mean accuracy:\", metrics[\"mean_accuracy\"])\n            print('-'*50)\n            \n    model.eval()\n    print('-'*30, 'Validation', '-'*30)\n    for idx, batch in enumerate(tqdm(valid_dataloader, leave=False)):\n        pixel_values = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n        \n        outputs = model(pixel_values=pixel_values, labels=labels)\n        loss, logits = outputs.loss, outputs.logits\n        \n        with torch.no_grad():\n            upsampled_logits = nn.functional.interpolate(logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n            predicted = upsampled_logits.argmax(dim=1)\n          \n            metric_valid.add_batch(predictions=predicted.detach().cpu().numpy(), references=labels.detach().cpu().numpy())\n\n        if idx % 19 == 0:\n            metrics = metric_valid.compute(num_labels=len(id_to_label), \n                                           ignore_index=255,\n                                           reduce_labels=False,\n          )\n\n            print(\"Loss:\", loss.item())\n            print(\"Mean_iou:\", metrics[\"mean_iou\"])\n            print(\"Mean accuracy:\", metrics[\"mean_accuracy\"])\n            print('-'*50)\n    try:\n        evaluate_test()\n    except:\n        pass","metadata":{"execution":{"iopub.status.busy":"2022-07-23T08:29:21.994770Z","iopub.execute_input":"2022-07-23T08:29:21.995198Z","iopub.status.idle":"2022-07-23T08:46:31.825295Z","shell.execute_reply.started":"2022-07-23T08:29:21.995159Z","shell.execute_reply":"2022-07-23T08:46:31.823305Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"[Link to the training results](https://www.kaggle.com/code/pankratozzi/pytorch-hf-segformer-quadcopter?scriptVersionId=101556804)","metadata":{}}]}