{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "346b72b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "410e8a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884155e1",
   "metadata": {},
   "source": [
    "<img src=clip.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b43d87c",
   "metadata": {},
   "source": [
    "<a href=https://github.com/openai/CLIP>CLIP torch model usage</a><br>\n",
    "<a href=https://arxiv.org/pdf/2103.00020.pdf>CLIP paper for deeper understanding</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7345644",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\"a diagram\", \"a dog\", \"a cat\"]\n",
    "image = preprocess(Image.open(\"clip.png\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize(inputs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8920cde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    \n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f2179fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20465613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: [[0.9927   0.004253 0.003016]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Label probs:\", probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "396f3fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[25.5312, 20.0781, 19.7344]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_per_image  # similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b757661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top predictions:\n",
      "a diagram: 100.00%\n",
      "a cat: 0.00%\n",
      "a dog: 0.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "values, indices = similarity[0].topk(3)\n",
    "\n",
    "print(\"\\nTop predictions:\")\n",
    "for value, index in zip(values, indices):\n",
    "    print(f\"{inputs[index]:>5s}: {100 * value.item():.2f}%\")\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaab50c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a diagram [25.53125 20.0625  19.71875]\n"
     ]
    }
   ],
   "source": [
    "similarity = np.empty((3, ))\n",
    "for i in range(3):\n",
    "    sim = 100.0 * nn.functional.cosine_similarity(image_features, text_features[i])\n",
    "    similarity[i] = sim\n",
    "\n",
    "print(inputs[np.argmax(similarity)], similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e99fda",
   "metadata": {},
   "source": [
    "**Great usage of CLIP model** \n",
    "<br><a href=https://cdn.openai.com/papers/dall-e-2.pdf>dalle original paper</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d49652",
   "metadata": {},
   "source": [
    "<img src=dalle.png width=800 height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8579f933",
   "metadata": {},
   "source": [
    "### Main script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d06b1ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Daria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os, re, gc\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "import spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from transformers import LongformerTokenizerFast, LongformerModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "seed = 42\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "087f1e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de8fa952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text, stop=stop):\n",
    "    text = text.lower()\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text)\n",
    "    text = re.sub(r'[^\\w]', ' ', text)\n",
    "    text = [w for w in text.split() if w not in stop]\n",
    "    text = [re.sub(r\"[^a-zA-Z0-9]+\", '', k) for k in text]\n",
    "    return re.sub(' +', ' ', ' '.join(text))\n",
    "\n",
    "def snow_stem(text):\n",
    "    snow_stemmer = SnowballStemmer(language='english')\n",
    "    new_text = ''\n",
    "    for word in text.split():\n",
    "        new_text += snow_stemmer.stem(word) + ' '\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32dcee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce1971ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD5CAYAAADLL+UrAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAVhklEQVR4nO3df4xd5X3n8fd37WAI07UNRCPLttbO1mpF8TaFKRBRReOwSviRxvyRZolQY1OvrN2QNi2ugtloS9rdSKYtS4hUpfIGGtPNZkJIViAgS7wOs2nU4gQnBBvYlAGcYsvYJdhuJ6FJnXz3j/vYXE9m7Dv319z4eb+kq3nOc557zveee/2ZM8+59zoyE0lSHf7FXBcgSeofQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSLzTzcgIu4B3gUcysyLSt+fAL8O/Ah4HrgxM4+UdbcCG4AfA7+TmY+W/quAu4B5wKcyc8vp9n3BBRfkihUrWn4w3//+9zn33HNbHt8Pg1gTWNdsDGJNMJh1DWJNUF9du3bteiUz3zTtysw85Q14G3AxsKep7x3A/NK+Hbi9tC8Evg0sAFbS+IUwr9yeB94MnFXGXHi6fV9yySU5G4899tisxvfDINaUaV2zMYg1ZQ5mXYNYU2Z9dQFP5Ay5etrpncz8KvDqlL4vZ+axsvg4sKy01wJjmfnDzHwRmAAuLbeJzHwhM38EjJWxkqQ+6sac/m8BXyrtpcBLTev2lb6Z+iVJfXTaOf1TiYiPAMeAz3SnHIiIjcBGgOHhYcbHx1u+7+Tk5KzG98Mg1gTWNRuDWBMMZl2DWBNY10lmmvfJk+f1V9A0p1/61gN/A7yxqe9W4Nam5UeBt5bbozONm+nmnH7vWFfrBrGmzMGsaxBryqyvLjqZ059OeSfOh4F3Z+YPmlY9CFwfEQsiYiWwCvg68A1gVUSsjIizgOvLWElSH7Xyls3PAqPABRGxD7iNxpn6AmB7RAA8npn/ITOfjoj7gGdoTPvclJk/Ltv5II0z/3nAPZn5dA8ejyTpFE4b+pn5vmm67z7F+I8BH5um/xHgkVlVJ0nqKj+RK0kVMfQlqSIdvWVTg2fF5odbGrdp9THWtzi2FXu3XNu1bUnqHc/0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTlt6EfEPRFxKCL2NPWdFxHbI+K58nNx6Y+I+ERETETEUxFxcdN91pXxz0XEut48HEnSqbRypv9p4KopfZuBHZm5CthRlgGuBlaV20bgk9D4JQHcBlwGXArcdvwXhSSpf04b+pn5VeDVKd1rgW2lvQ24rqn/3mx4HFgUEUuAdwLbM/PVzDwMbOenf5FIknqs3Tn94cw8UNovA8OlvRR4qWncvtI3U78kqY8iM08/KGIF8FBmXlSWj2Tmoqb1hzNzcUQ8BGzJzK+V/h3ALcAocHZm/tfS/5+B1zLzT6fZ10YaU0MMDw9fMjY21vKDmZycZGhoqOXx/dDvmnbvP9rSuOFz4OBr3dvv6qULu7Idn8PWDWJdg1gT1FfXmjVrdmXmyHTr5re5zYMRsSQzD5Tpm0Olfz+wvGncstK3n0bwN/ePT7fhzNwKbAUYGRnJ0dHR6YZNa3x8nNmM74d+17R+88Mtjdu0+hh37G736f9pe28Y7cp2fA5bN4h1DWJNYF3N2p3eeRA4/g6cdcADTf3vL+/iuRw4WqaBHgXeERGLywXcd5Q+SVIfnfZULyI+S+Ms/YKI2EfjXThbgPsiYgPwXeC9ZfgjwDXABPAD4EaAzHw1Iv4L8I0y7o8yc+rFYUlSj5029DPzfTOsunKasQncNMN27gHumVV1kqSu8hO5klQRQ1+SKmLoS1JFuveePVVtRYtvFT2dTauPtfy2U4C9W67tyn6lWnimL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKtJR6EfE70XE0xGxJyI+GxFnR8TKiNgZERMR8bmIOKuMXVCWJ8r6FV15BJKklrUd+hGxFPgdYCQzLwLmAdcDtwN3ZubPA4eBDeUuG4DDpf/OMk6S1EedTu/MB86JiPnAG4EDwNuB+8v6bcB1pb22LFPWXxkR0eH+JUmzML/dO2bm/oj4U+DvgNeALwO7gCOZeawM2wcsLe2lwEvlvsci4ihwPvBKuzUMqhWbHz7R3rT6GOubliVpLkVmtnfHiMXAF4B/BxwBPk/jDP6jZQqHiFgOfCkzL4qIPcBVmbmvrHseuCwzX5my3Y3ARoDh4eFLxsbGWq5pcnKSoaGhth5PN+3ef/REe/gcOPjaHBYzgzOlrtVLF/aumGJQXldTDWJdg1gT1FfXmjVrdmXmyHTr2j7TB/4t8GJm/j1ARHwRuAJYFBHzy9n+MmB/Gb8fWA7sK9NBC4HvTd1oZm4FtgKMjIzk6OhoywWNj48zm/G9sn7Kmf4duzs5zL1xptS194bR3hVTDMrraqpBrGsQawLratbJnP7fAZdHxBvL3PyVwDPAY8B7yph1wAOl/WBZpqz/Srb7Z4YkqS1th35m7qQxnfNNYHfZ1lbgFuDmiJigMWd/d7nL3cD5pf9mYHMHdUuS2tDR3/eZeRtw25TuF4BLpxn7T8BvdLI/SVJn/ESuJFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klSRjkI/IhZFxP0R8f8i4tmIeGtEnBcR2yPiufJzcRkbEfGJiJiIiKci4uLuPARJUqs6PdO/C/jfmfmLwC8DzwKbgR2ZuQrYUZYBrgZWldtG4JMd7luSNEtth35ELATeBtwNkJk/yswjwFpgWxm2DbiutNcC92bD48CiiFjS7v4lSbPXyZn+SuDvgb+IiG9FxKci4lxgODMPlDEvA8OlvRR4qen++0qfJKlPIjPbu2PECPA4cEVm7oyIu4B/AH47Mxc1jTucmYsj4iFgS2Z+rfTvAG7JzCembHcjjekfhoeHLxkbG2u5psnJSYaGhtp6PN20e//RE+3hc+Dga3NYzAzOlLpWL13Yu2KKQXldTTWIdQ1iTVBfXWvWrNmVmSPTrZvfwXb3Afsyc2dZvp/G/P3BiFiSmQfK9M2hsn4/sLzp/stK30kycyuwFWBkZCRHR0dbLmh8fJzZjO+V9ZsfPtHetPoYd+zu5DD3xplS194bRntXTDEor6upBrGuQawJrKtZ29M7mfky8FJE/ELpuhJ4BngQWFf61gEPlPaDwPvLu3guB442TQNJkvqg01O93wY+ExFnAS8AN9L4RXJfRGwAvgu8t4x9BLgGmAB+UMZKkvqoo9DPzCeB6eaNrpxmbAI3dbI/SVJn/ESuJFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKD9+Ur0iysaPqeo17ZtPrYSd+ndNzeLdf2fN9St3mmL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIh2HfkTMi4hvRcRDZXllROyMiImI+FxEnFX6F5TlibJ+Raf7liTNTjfO9D8EPNu0fDtwZ2b+PHAY2FD6NwCHS/+dZZwkqY86Cv2IWAZcC3yqLAfwduD+MmQbcF1pry3LlPVXlvGSpD7p9Ez/48CHgZ+U5fOBI5l5rCzvA5aW9lLgJYCy/mgZL0nqk8jM9u4Y8S7gmsz8QESMAr8PrAceL1M4RMRy4EuZeVFE7AGuysx9Zd3zwGWZ+cqU7W4ENgIMDw9fMjY21nJNk5OTDA0NtfV4umn3/qMn2sPnwMHX5rCYGVhX62aqafXShf0vpsmgvN6bDWJNUF9da9as2ZWZI9Otm9/Bdq8A3h0R1wBnA/8SuAtYFBHzy9n8MmB/Gb8fWA7si4j5wELge1M3mplbga0AIyMjOTo62nJB4+PjzGZ8r6zf/PCJ9qbVx7hjdyeHuTesq3Uz1bT3htH+F9NkUF7vzQaxJrCuZm1P72TmrZm5LDNXANcDX8nMG4DHgPeUYeuAB0r7wbJMWf+VbPfPDElSW3rxPv1bgJsjYoLGnP3dpf9u4PzSfzOwuQf7liSdQlf+js7McWC8tF8ALp1mzD8Bv9GN/UmS2uMnciWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFWk7dCPiOUR8VhEPBMRT0fEh0r/eRGxPSKeKz8Xl/6IiE9ExEREPBURF3frQUiSWtPJmf4xYFNmXghcDtwUERcCm4EdmbkK2FGWAa4GVpXbRuCTHexbktSGtkM/Mw9k5jdL+x+BZ4GlwFpgWxm2DbiutNcC92bD48CiiFjS7v4lSbM3vxsbiYgVwK8AO4HhzDxQVr0MDJf2UuClprvtK30HkH4Grdj88Jzsd++Wa+dkvzozRGZ2toGIIeD/Ah/LzC9GxJHMXNS0/nBmLo6Ih4Atmfm10r8DuCUzn5iyvY00pn8YHh6+ZGxsrOVaJicnGRoa6ujxdMPu/UdPtIfPgYOvzWExM7Cu1g1aTauXLgQG5/XebBBrgvrqWrNmza7MHJluXUdn+hHxBuALwGcy84ul+2BELMnMA2X65lDp3w8sb7r7stJ3kszcCmwFGBkZydHR0ZbrGR8fZzbje2V90xngptXHuGN3V/6g6irrat2g1bT3hlFgcF7vzQaxJrCuZp28eyeAu4FnM/O/Na16EFhX2uuAB5r631/exXM5cLRpGkiS1AednL5cAfwmsDsinix9/wnYAtwXERuA7wLvLeseAa4BJoAfADd2sG9JUhvaDv0yNx8zrL5ymvEJ3NTu/iRJnfMTuZJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFVkcP4POEktOf4fsm9afeyk/5qzH/xP2X/2ndGhv6LP/yAkadA5vSNJFTH0Jakihr4kVeSMntOX1F2nu07Wq4vLXkDuHs/0Jakihr4kVcTQl6SKGPqSVBFDX5Iq4rt3JA28Tj9d38m7is60dw71/Uw/Iq6KiO9ExEREbO73/iWpZn0N/YiYB/wZcDVwIfC+iLiwnzVIUs36faZ/KTCRmS9k5o+AMWBtn2uQpGr1e05/KfBS0/I+4LI+1yBJLevlt/We6lpDr64lRGb2ZMPT7iziPcBVmfnvy/JvApdl5gebxmwENpbFXwC+M4tdXAC80qVyu2UQawLrmo1BrAkGs65BrAnqq+tfZeabplvR7zP9/cDypuVlpe+EzNwKbG1n4xHxRGaOtF9e9w1iTWBdszGINcFg1jWINYF1Nev3nP43gFURsTIizgKuBx7scw2SVK2+nuln5rGI+CDwKDAPuCczn+5nDZJUs75/OCszHwEe6dHm25oW6rFBrAmsazYGsSYYzLoGsSawrhP6eiFXkjS3/O4dSarIGRH6c/3VDhGxNyJ2R8STEfFE6TsvIrZHxHPl5+LSHxHxiVLrUxFxcRfruCciDkXEnqa+WdcREevK+OciYl0PavpoROwvx+vJiLimad2tpabvRMQ7m/q79hxHxPKIeCwinomIpyPiQ6V/ro/VTHXN9fE6OyK+HhHfLnX9YelfGRE7yz4+V96cQUQsKMsTZf2K09XbxZo+HREvNh2rt5T+vjyHTducFxHfioiHyvKcHaufkpk/0zcaF4SfB94MnAV8G7iwzzXsBS6Y0vfHwObS3gzcXtrXAF8CArgc2NnFOt4GXAzsabcO4DzghfJzcWkv7nJNHwV+f5qxF5bnbwGwsjyv87r9HANLgItL++eAvy37nutjNVNdc328Ahgq7TcAO8txuA+4vvT/OfAfS/sDwJ+X9vXA505Vb5dr+jTwnmnG9+U5bNrfzcD/BB4qy3N2rKbezoQz/UH9aoe1wLbS3gZc19R/bzY8DiyKiCXd2GFmfhV4tcM63glsz8xXM/MwsB24qss1zWQtMJaZP8zMF4EJGs9vV5/jzDyQmd8s7X8EnqXxafG5PlYz1TWTfh2vzMzJsviGckvg7cD9pX/q8Tp+HO8HroyIOEW93axpJn15DgEiYhlwLfCpshzM4bGa6kwI/em+2uFU/1B6IYEvR8SuaHyiGGA4Mw+U9svAcGn3u97Z1tGv+j5Y/sy+5/g0ylzUVP6c/hUaZ4oDc6ym1AVzfLzKdMWTwCEawfg8cCQzj02zjxP7L+uPAud3u66pNWXm8WP1sXKs7oyIBVNrmrLvXjyHHwc+DPykLJ/PHB+rZmdC6A+CX8vMi2l8e+hNEfG25pXZ+Httzt8mNSh1AJ8E/jXwFuAAcMdcFBERQ8AXgN/NzH9oXjeXx2qauub8eGXmjzPzLTQ+RX8p8Iv9rmGqqTVFxEXArTRq+1UaUza39LOmiHgXcCgzd/Vzv7NxJoT+ab/aodcyc3/5eQj4XzT+URw8Pm1Tfh4qw/td72zr6Hl9mXmw/IP9CfDfef3P1r7VFBFvoBGsn8nML5buOT9W09U1CMfruMw8AjwGvJXGFMnxz/o07+PE/sv6hcD3elVXU01XlSmyzMwfAn9B/4/VFcC7I2IvjWm1twN3MSDHCjgjLuTOp3HxZSWvX7T6pT7u/1zg55raf01jTvBPOPmi4B+X9rWcfEHp612uZwUnXzSdVR00zo5epHFRa3Fpn9flmpY0tX+PxtwlwC9x8sWrF2hclOzqc1we873Ax6f0z+mxOkVdc3283gQsKu1zgL8C3gV8npMvTn6gtG/i5IuT952q3i7XtKTpWH4c2NLv13tTjaO8fiF3zo7VT9XVjY3M9Y3Glfm/pTHP+JE+7/vN5cn5NvD08f3TmJfbATwH/J/jL6TyovuzUutuYKSLtXyWxp///0xjDnBDO3UAv0XjwtEEcGMPavrLss+naHz3UnOofaTU9B3g6l48x8Cv0Zi6eQp4styuGYBjNVNdc328/g3wrbL/PcAfNL32v14e++eBBaX/7LI8Uda/+XT1drGmr5RjtQf4H7z+Dp++PIdTahzl9dCfs2M19eYnciWpImfCnL4kqUWGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFfn/B8tjGX/IkE8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train.full_text.apply(preprocessor).str.len().hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5524e0f",
   "metadata": {},
   "source": [
    "**Use CLIP context encoder for feature extraction / alt: use BERT/word2vec encoding for same purpose**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f842ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipDataset(Dataset):\n",
    "    def __init__(self, data, max_len=None, preprocess=False):\n",
    "        self.data = data\n",
    "        if max_len is None:\n",
    "            self.max_len = model.context_length\n",
    "        self.preprocess = preprocess\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        raw_text = self.data.iloc[ix, 1]\n",
    "        labels = self.data.iloc[ix, 2:]\n",
    "        raw_text = raw_text[:self.max_len]\n",
    "        if self.preprocess:\n",
    "            raw_text = snow_stem(preprocessor(raw_text))\n",
    "        \n",
    "        return raw_text, labels\n",
    "    \n",
    "    def _encode(self, text: list):\n",
    "        text = clip.tokenize(text, context_length=self.max_len).to(device)\n",
    "        with torch.no_grad():\n",
    "            text_features = model.encode_text(text)\n",
    "        return text_features.type(torch.float32)\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        text, labels = list(zip(*batch))\n",
    "        text = self._encode(text)\n",
    "        labels = torch.as_tensor([label.values for label in labels], dtype=torch.float32).to(device)\n",
    "        return text, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "590e05b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid = train_test_split(train, shuffle=True, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f79f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ClipDataset(x_train, preprocess=True)\n",
    "valid_ds = ClipDataset(x_valid, preprocess=True)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=16, shuffle=True, collate_fn=train_ds.collate_fn)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=16, shuffle=False, collate_fn=valid_ds.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4211f9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextModel(nn.Module):\n",
    "    def __init__(self, short=True):\n",
    "        super(TextModel, self).__init__()\n",
    "        if short:\n",
    "            self.regressor = nn.Sequential(\n",
    "                                    nn.Dropout(p=0.5),\n",
    "                                    nn.Linear(512, 6))\n",
    "        else:\n",
    "            self.regressor = nn.Sequential(\n",
    "                    nn.Dropout(p=0.5),\n",
    "                    nn.Linear(512, 256),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Dropout(p=0.5),\n",
    "                    nn.Linear(256, 6)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.regressor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19883364",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = TextModel(short=True).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "# criterion = nn.SmoothL1Loss()\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, min_lr=1e-6, factor=0.1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03300720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCRMSE(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    rmse = np.sqrt(np.mean(np.square(y_true - y_pred), axis=1))\n",
    "    rcrmse = np.mean(rmse)\n",
    "    return rcrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45475cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, valid_losses = [], []\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} / {epochs}\")\n",
    "    \n",
    "    losses, scores = [], []\n",
    "    for batch in train_dl:\n",
    "        net.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, labels = batch\n",
    "        logits = net(text)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        scores.append(MCRMSE(labels.detach().cpu().numpy(), logits.detach().cpu().numpy()))\n",
    "    epoch_train_scores = np.mean(scores)\n",
    "    epoch_train_loss = np.mean(losses)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    \n",
    "    losses, scores = [], []\n",
    "    for batch in valid_dl:\n",
    "        net.eval()\n",
    "        text, labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = net(text)\n",
    "            loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "        scores.append(MCRMSE(labels.detach().cpu().numpy(), logits.detach().cpu().numpy()))\n",
    "    epoch_valid_loss = np.mean(losses)\n",
    "    epoch_valid_scores = np.mean(scores)\n",
    "    valid_losses.append(epoch_valid_loss)\n",
    "    \n",
    "    scheduler.step(epoch_valid_loss)\n",
    "    \n",
    "    print(f\"Train loss: {epoch_train_loss:.5f}, valid loss: {epoch_valid_loss:.5f}\")\n",
    "    print(f\"Train scores: {epoch_train_scores:.5f}, valid scores: {epoch_valid_scores:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2a566bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5c27fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_columns = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "submission = pd.DataFrame(data={\"text_id\": test.text_id})\n",
    "submission[test_columns] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67457e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "\n",
    "net.eval()\n",
    "for i in range(len(test)):\n",
    "    text = test.iloc[i, 1][:model.context_length]\n",
    "    text = clip.tokenize(text).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text).type(torch.float32).unsqueeze(0)\n",
    "        output = net(text_features)\n",
    "    output = output.cpu().detach().numpy().squeeze()\n",
    "    submission.iloc[i, 1:] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a853496a",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9fcd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "!kaggle competitions submit -c feedback-prize-english-language-learning -f submission.csv -m \"Message\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e084d8b3",
   "metadata": {},
   "source": [
    "## Bert Encoder\n",
    "<br><a href=https://arxiv.org/pdf/2004.05150.pdf>Longformer original paper</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90c7e63",
   "metadata": {},
   "source": [
    "<img src=bert.png width=500 height=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c089269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8834d8abe654cdfb1ffb9393beab77c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d908fe99b0e24b9a9a9c5fdd10181684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7229d0cf98654b7cac8d5648a56400bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/597M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096', max_length=4096)\n",
    "bert = LongformerModel.from_pretrained('allenai/longformer-base-4096').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10adbb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_for_bert(data):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sent in tqdm(data):\n",
    "        encoded_sent = tokenizer(snow_stem(preprocessor(sent)), padding='max_length', truncation=True, max_length=4096)\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7342181d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa871a086afc4e2a9f24d543911374b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65dcf343258d45339dbea27b425e06ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/783 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_inputs, train_masks = preprocessing_for_bert(x_train['full_text'].values)\n",
    "val_inputs, val_masks = preprocessing_for_bert(x_valid['full_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "387693e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.as_tensor(x_train.iloc[: , 2:].values, dtype=torch.float32)\n",
    "valid_labels = torch.as_tensor(x_valid.iloc[: , 2:].values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c24af1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "valid_ds = TensorDataset(val_inputs, val_masks, valid_labels)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=4, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85fedc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "    def __init__(self, mode=\"short\"):\n",
    "        super(BertModel, self).__init__()\n",
    "        self.mode = mode\n",
    "        if mode == \"short\":\n",
    "            self.predictor = nn.Sequential(\n",
    "                                           nn.Dropout(p=0.5),\n",
    "                                           nn.Linear(768, 6)\n",
    "                                          )\n",
    "        elif mode == \"long\":\n",
    "            self.predictor = nn.Sequential(\n",
    "                                           nn.Linear(768, 768),\n",
    "                                           nn.LeakyReLU(0.01, inplace=True),\n",
    "                                           nn.Dropout(p=0.5),\n",
    "                                           nn.Linear(768, 6)\n",
    "            )\n",
    "        elif mode == \"conv\":\n",
    "            self.predictor = nn.Sequential(\n",
    "                                           nn.Conv1d(in_channels=4096, out_channels=1024, kernel_size=3, stride=2, padding=0),\n",
    "                                           nn.LeakyReLU(0.01, inplace=True),\n",
    "                                           nn.Dropout(p=0.5),\n",
    "                                           nn.Conv1d(in_channels=1024, out_channels=256, kernel_size=3, stride=2, padding=0),\n",
    "                                           nn.LeakyReLU(0.01, inplace=True),\n",
    "                                           nn.Dropout(p=0.5),\n",
    "                                           nn.Conv1d(in_channels=256, out_channels=64, kernel_size=3, stride=2, padding=0),\n",
    "                                           nn.LeakyReLU(0.01, inplace=True),\n",
    "                                           nn.Dropout(p=0.5),\n",
    "                                           nn.AdaptiveAvgPool1d(32),\n",
    "                                           nn.Flatten(),\n",
    "                                           nn.Dropout(p=0.5),\n",
    "                                           nn.Linear(2048, 256),\n",
    "                                           nn.ReLU(),\n",
    "                                           nn.Linear(256, 6)\n",
    "                                           \n",
    "            )\n",
    "            \n",
    "    def forward(self, input_ids, attention_masks):\n",
    "        with torch.no_grad():\n",
    "            outputs = bert(input_ids=input_ids.to(device), attention_mask=attention_masks.to(device))\n",
    "            if self.mode == \"short\" or self.mode == \"long\":\n",
    "                hidden = outputs.pooler_output  # outputs[0][:, 0, :] # last layer hidden-state of CLS token\n",
    "            elif self.mode == \"conv\":\n",
    "                hidden = outputs.last_hidden_state  # outputs[0]  # output hidden-states sequence\n",
    "            else:\n",
    "                raise ValueError(f\"mode is expected to be 'short' or 'long', given {mode}\")\n",
    "        return self.predictor(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c25a91d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BertModel(mode=\"conv\").to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 15\n",
    "total_steps = total_steps = len(train_dl) * epochs\n",
    "optimizer = AdamW(net.parameters(), lr=5e-5, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=20, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cfb73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, valid_losses = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} / {epochs}\")\n",
    "    \n",
    "    losses, scores = [], []\n",
    "    for batch in tqdm(train_dl, total=len(train_dl), leave=False):\n",
    "        net.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids, att_masks, labels = batch\n",
    "        logits = net(input_ids.to(device), att_masks.to(device))\n",
    "        loss = criterion(logits, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        scores.append(MCRMSE(labels.detach().cpu().numpy(), logits.detach().cpu().numpy()))\n",
    "    epoch_train_scores = np.mean(scores)\n",
    "    epoch_train_loss = np.mean(losses)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    \n",
    "    losses, scores = [], []\n",
    "    for batch in tqdm(valid_dl, total=len(valid_dl), leave=False):\n",
    "        net.eval()\n",
    "        input_ids, att_masks, labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = net(input_ids.to(device), att_masks.to(device))\n",
    "            loss = criterion(logits, labels.to(device))\n",
    "        losses.append(loss.item())\n",
    "        scores.append(MCRMSE(labels.detach().cpu().numpy(), logits.detach().cpu().numpy()))\n",
    "    epoch_valid_loss = np.mean(losses)\n",
    "    epoch_valid_scores = np.mean(scores)\n",
    "    valid_losses.append(epoch_valid_loss)\n",
    "    \n",
    "    scheduler.step(epoch_valid_loss)\n",
    "    \n",
    "    print(f\"Train loss: {epoch_train_loss:.5f}, valid loss: {epoch_valid_loss:.5f}\")\n",
    "    print(f\"Train scores: {epoch_train_scores:.5f}, valid scores: {epoch_valid_scores:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045d2447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train: short,Train loss: 0.52810, valid loss: 0.47120\n",
    "#Train scores: 0.65202, valid scores: 0.61893 -> train more\n",
    "# long, conv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4224bdce",
   "metadata": {},
   "source": [
    "## DeBERTa\n",
    "<br><a href=https://arxiv.org/pdf/2111.09543.pdf>DeBERTa v3 original paper</a><br><b>here first deberta used as for conflicts in packages</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ccb5d9",
   "metadata": {},
   "source": [
    "<img src=deberta.jpg>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bf4ddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DebertaTokenizer, DebertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4773716b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')\n",
    "model = DebertaModel.from_pretrained('microsoft/deberta-base', return_dict=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b84a0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_for_deberta(text):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for sent in tqdm(text):\n",
    "        encoded_sent = tokenizer(snow_stem(preprocessor(sent)), padding='max_length', truncation=True, max_length=512)\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4aef157e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0642ed752e9d4fcfaeb72496ab83c73a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f8067cf306448ba9ee6fcf3e5d424b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/783 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_inputs, train_masks = tokenize_for_deberta(x_train['full_text'].values)\n",
    "val_inputs, val_masks = tokenize_for_deberta(x_valid['full_text'].values)\n",
    "\n",
    "train_labels = torch.as_tensor(x_train.iloc[: , 2:].values, dtype=torch.float32)\n",
    "valid_labels = torch.as_tensor(x_valid.iloc[: , 2:].values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5627c0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "valid_ds = TensorDataset(val_inputs, val_masks, valid_labels)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=4, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d0dc375",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeBERTaModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeBERTaModel, self).__init__()\n",
    "        self.predictor = nn.Sequential(\n",
    "                                       nn.Conv1d(in_channels=512, out_channels=512, kernel_size=3, stride=2, padding=0),\n",
    "                                       nn.LeakyReLU(0.01, inplace=True),\n",
    "                                       nn.Dropout(p=0.5),\n",
    "                                       nn.Conv1d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=0),\n",
    "                                       nn.LeakyReLU(0.01, inplace=True),\n",
    "                                       nn.Dropout(p=0.5),\n",
    "                                       nn.Conv1d(in_channels=256, out_channels=64, kernel_size=3, stride=2, padding=0),\n",
    "                                       nn.LeakyReLU(0.01, inplace=True),\n",
    "                                       nn.Dropout(p=0.5),\n",
    "                                       nn.AdaptiveAvgPool1d(32),\n",
    "                                       nn.Flatten(),\n",
    "                                       nn.Dropout(p=0.5),\n",
    "                                       nn.Linear(2048, 256),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(256, 6)\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_masks):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids.to(device), attention_mask=attention_masks.to(device))\n",
    "            hidden = outputs.last_hidden_state\n",
    "\n",
    "        return self.predictor(hidden)\n",
    "    \n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "    \n",
    "class FeedBackModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeedBackModel, self).__init__()\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        self.pooler = MeanPooling()\n",
    "        self.fc = nn.Linear(768, 6)\n",
    "        \n",
    "    def forward(self, ids, mask):        \n",
    "        out = model(input_ids=ids, attention_mask=mask, output_hidden_states=False)\n",
    "        out = self.pooler(out.last_hidden_state, mask)\n",
    "        out = self.drop(out)\n",
    "        outputs = self.fc(out)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e386687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = AttModel().to(device)  # FeedBackModel is most stable\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 12\n",
    "total_steps = total_steps = len(train_dl) * epochs\n",
    "optimizer = AdamW(net.parameters(), lr=5e-5, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=10, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "761a6654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 12.58959, valid loss: 12.26212\n",
      "Train scores: 3.49834, valid scores: 3.45859\n",
      "Epoch 2 / 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.87987, valid loss: 0.45121\n",
      "Train scores: 1.13322, valid scores: 0.61248\n",
      "Epoch 3 / 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.68205, valid loss: 0.44936\n",
      "Train scores: 0.76849, valid scores: 0.61121\n",
      "Epoch 4 / 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.68058, valid loss: 0.44673\n",
      "Train scores: 0.77054, valid scores: 0.60954\n",
      "Epoch 5 / 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.66944, valid loss: 0.44410\n",
      "Train scores: 0.76413, valid scores: 0.60791\n",
      "Epoch 6 / 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.65862, valid loss: 0.44151\n",
      "Train scores: 0.75596, valid scores: 0.60622\n",
      "Epoch 7 / 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.65784, valid loss: 0.43891\n",
      "Train scores: 0.75607, valid scores: 0.60456\n",
      "Epoch 8 / 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.64357, valid loss: 0.43626\n",
      "Train scores: 0.74725, valid scores: 0.60287\n",
      "Epoch 9 / 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.63897, valid loss: 0.43349\n",
      "Train scores: 0.74503, valid scores: 0.60113\n",
      "Epoch 10 / 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.63141, valid loss: 0.43094\n",
      "Train scores: 0.73822, valid scores: 0.59951\n",
      "Epoch 11 / 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.62215, valid loss: 0.42875\n",
      "Train scores: 0.73539, valid scores: 0.59803\n",
      "Epoch 12 / 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.62414, valid loss: 0.42695\n",
      "Train scores: 0.73452, valid scores: 0.59679\n"
     ]
    }
   ],
   "source": [
    "train_losses, valid_losses = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} / {epochs}\")\n",
    "    \n",
    "    losses, scores = [], []\n",
    "    for batch in tqdm(train_dl, total=len(train_dl), leave=False):\n",
    "        net.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids, att_masks, labels = batch\n",
    "        logits = net(input_ids.to(device), att_masks.to(device))\n",
    "        loss = criterion(logits, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        scores.append(MCRMSE(labels.detach().cpu().numpy(), logits.detach().cpu().numpy()))\n",
    "    epoch_train_scores = np.mean(scores)\n",
    "    epoch_train_loss = np.mean(losses)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    \n",
    "    losses, scores = [], []\n",
    "    for batch in tqdm(valid_dl, total=len(valid_dl), leave=False):\n",
    "        net.eval()\n",
    "        input_ids, att_masks, labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = net(input_ids.to(device), att_masks.to(device))\n",
    "            loss = criterion(logits, labels.to(device))\n",
    "        losses.append(loss.item())\n",
    "        scores.append(MCRMSE(labels.detach().cpu().numpy(), logits.detach().cpu().numpy()))\n",
    "    epoch_valid_loss = np.mean(losses)\n",
    "    epoch_valid_scores = np.mean(scores)\n",
    "    valid_losses.append(epoch_valid_loss)\n",
    "    \n",
    "    scheduler.step(epoch_valid_loss)\n",
    "    \n",
    "    print(f\"Train loss: {epoch_train_loss:.5f}, valid loss: {epoch_valid_loss:.5f}\")\n",
    "    print(f\"Train scores: {epoch_train_scores:.5f}, valid scores: {epoch_valid_scores:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab79f010",
   "metadata": {},
   "source": [
    "## Word2vec encoder\n",
    "<br><a href=https://arxiv.org/pdf/1301.3781.pdf>word2vec original paper</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecba6a5",
   "metadata": {},
   "source": [
    "<img src=w2v.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db521136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1c36d888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_enc = train[\"full_text\"].apply(lambda x: snow_stem(preprocessor(x)).split())\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5018ff82",
   "metadata": {},
   "outputs": [],
   "source": [
    "wmodel = Word2Vec(window=10, sg=1, hs=0, negative=10, alpha=0.03, min_alpha=0.0007, seed=seed)\n",
    "\n",
    "wmodel.build_vocab(train_enc, progress_per=200)\n",
    "\n",
    "wmodel.train(train_enc, total_examples=wmodel.corpus_count, epochs=20, report_delay=1)\n",
    "wmodel.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5da5136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_vectors(items, agg_func=np.mean):\n",
    "    n = len(items)\n",
    "    item_vec = np.zeros((n, 100))\n",
    "    for i in range(n):\n",
    "        try:\n",
    "            item_vec[i, :] = wmodel.wv.get_vector(items[i], norm=True)\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    return agg_func(item_vec, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "077a78ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.49568167e-02,  6.50992220e-02, -1.23898177e-02,  9.88180647e-02,\n",
       "       -6.41737839e-02,  6.10573513e-03,  8.55784495e-03, -2.80213943e-02,\n",
       "        2.91608556e-02,  6.62954544e-03, -1.13670222e-01,  2.32459710e-03,\n",
       "        9.45151654e-02,  9.29290331e-03, -3.23485249e-02, -2.66641401e-02,\n",
       "        1.42495557e-01,  3.10273544e-03, -8.20647131e-02,  1.54746731e-02,\n",
       "       -4.73497517e-02,  4.13829624e-02, -4.35033586e-02, -2.46894202e-02,\n",
       "       -1.44293687e-02,  1.14565853e-01,  1.22476762e-02, -1.98909554e-02,\n",
       "        8.23148069e-02, -2.87900323e-02, -3.76960055e-03,  2.79519358e-02,\n",
       "        1.00643943e-01,  5.35840235e-02, -7.78463967e-02,  8.87313737e-02,\n",
       "       -1.56221022e-02,  1.30273731e-02,  2.10993452e-02,  2.23867668e-02,\n",
       "       -2.02543362e-02, -1.11475476e-01,  8.13004091e-02, -5.87143740e-02,\n",
       "       -1.36891345e-02,  1.83664076e-02, -5.92603437e-03, -6.83435262e-03,\n",
       "        9.08860167e-04, -5.66390734e-02, -6.53079851e-02, -3.82534389e-02,\n",
       "       -7.18860300e-02,  4.14159298e-02, -4.32523598e-02,  1.27157054e-01,\n",
       "        1.63353158e-02,  4.23016438e-02, -1.01215411e-01,  3.96312072e-02,\n",
       "        1.06701281e-04,  6.94023056e-02, -1.40458060e-01,  1.98304238e-02,\n",
       "       -8.16216304e-03, -2.71787829e-02,  2.12361163e-02,  3.11272126e-02,\n",
       "       -1.03902204e-01,  4.84367586e-02, -8.38973037e-03, -5.48815299e-02,\n",
       "       -2.44615225e-02, -9.70605538e-02, -8.00846414e-02,  6.35102647e-02,\n",
       "        1.43192342e-02,  1.03044597e-02,  5.63538943e-02, -3.64083216e-02,\n",
       "        3.37891587e-02, -1.22170907e-02,  8.21593030e-03,  3.11983069e-02,\n",
       "        6.41115118e-04, -4.73335208e-02, -7.75408758e-03,  4.68139930e-02,\n",
       "       -5.13175354e-02, -4.11734025e-02, -1.36750548e-01,  8.16809743e-03,\n",
       "       -9.58994667e-03, -4.95008864e-02,  8.91043324e-02, -6.00439812e-02,\n",
       "       -2.62423510e-02,  3.90438620e-02, -4.86651568e-02, -1.85323884e-02])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregate_vectors(train_enc.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82548652",
   "metadata": {},
   "source": [
    "## 6 LGBMRegressor models with text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1bf3513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from lightgbm import LGBMRegressor\n",
    "from lutils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5ed2c3",
   "metadata": {},
   "source": [
    "Embeddings:\n",
    "\n",
    "* CLIP context short text (77) embedding, size 512;\n",
    "* LongFormer pooler_output text (4096) embedding, size 768;\n",
    "* DeBEDRTa custom MeanPooling text (512) output, size 768;\n",
    "* Word2Vec embedding, size 100.\n",
    "\n",
    "Dimensionality reduction with PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6ab3f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'n_estimators': 2000, \n",
    "                'learning_rate': 0.005134,\n",
    "                'num_leaves': 54,\n",
    "                'max_depth': 5,\n",
    "                'subsample_for_bin': 240000,\n",
    "                'reg_alpha': 0.436193,\n",
    "                'reg_lambda': 0.479169,\n",
    "                'colsample_bytree': 0.508716,\n",
    "                'min_split_gain': 0.024766,\n",
    "                'subsample': 0.7,\n",
    "                'random_state': seed,\n",
    "                'verbose': -1,\n",
    "                'min_child_samples': 200,\n",
    "                'max_bin': 255\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3158336",
   "metadata": {},
   "source": [
    "**DeBERTa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a63d39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deberta_embeddings(text, batch_size=32):\n",
    "    model.eval()\n",
    "    inp, msk = tokenize_for_deberta(text)\n",
    "    pooler = MeanPooling()\n",
    "    embedding = np.zeros((len(text), 768))\n",
    "    dl = DataLoader(TensorDataset(inp, msk), shuffle=True, batch_size=batch_size)\n",
    "    for i, batch in enumerate(dl, 1):\n",
    "        input_ids, masks = batch\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=input_ids.to(device), attention_mask=masks.to(device), output_hidden_states=False)\n",
    "        embed = pooler(out.last_hidden_state, masks.to(device))\n",
    "        embed = embed.cpu().detach().numpy().squeeze()\n",
    "        embedding[(i-1)*batch_size:i*batch_size, :] = embed\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38fa49c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3911/3911 [00:12<00:00, 312.97it/s]\n"
     ]
    }
   ],
   "source": [
    "X = get_deberta_embeddings(train.iloc[:, 1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b170238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(data=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fca52e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep 20 17:43:28 2022, Cross-Validation, 3911 rows, 768 cols\n",
      "Fold 1, Train score = 0.58814, Valid score = 0.58957\n",
      "Fold 2, Train score = 0.57856, Valid score = 0.59692\n",
      "Fold 3, Train score = 0.59415, Valid score = 0.59736\n",
      "Fold 4, Train score = 0.59403, Valid score = 0.59667\n",
      "Fold 5, Train score = 0.59196, Valid score = 0.57953\n",
      "Fold 6, Train score = 0.58626, Valid score = 0.60234\n",
      "Fold 7, Train score = 0.59268, Valid score = 0.60889\n",
      "Fold 8, Train score = 0.58901, Valid score = 0.61555\n",
      "Fold 9, Train score = 0.58163, Valid score = 0.56771\n",
      "Fold 10, Train score = 0.58051, Valid score = 0.59196\n",
      "Train score by each fold: [0.58814, 0.57856, 0.59415, 0.59403, 0.59196, 0.58626, 0.59268, 0.58901, 0.58163, 0.58051]\n",
      "Valid score by each fold: [0.58957, 0.59692, 0.59736, 0.59667, 0.57953, 0.60234, 0.60889, 0.61555, 0.56771, 0.59196]\n",
      "Train mean score by each fold:0.58769 +/- 0.00548\n",
      "Valid mean score by each fold:0.59465 +/- 0.01305\n",
      "**************************************************\n",
      "OOF-score: 0.59465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "874"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "lgbm_estimators, oof_scores = lightgbm_cross_validation(lgbm_params, \n",
    "                                                        X, \n",
    "                                                        train.iloc[:, 2:], \n",
    "                                                        cv=fold, \n",
    "                                                        rounds=50)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "58238945",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(StandardScaler(), PCA(n_components=None, random_state=seed))\n",
    "\n",
    "pca_factors = pipe.fit_transform(X.values)\n",
    "pca_factors = pd.DataFrame(pca_factors, index=train.index)\n",
    "# did not work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ac1495",
   "metadata": {},
   "source": [
    "**Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5b9348f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(data=np.zeros((len(train), 100)), columns=[f\"feat_{i+1}\" for i in range(100)])\n",
    "for idx, row in X.iterrows():\n",
    "    X.iloc[idx, :] = aggregate_vectors(train_enc.iloc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "172748a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = {\n",
    "                'boosting_type': 'gbdt', # goss\n",
    "                'n_estimators': 1000, \n",
    "                'learning_rate': 0.005134,\n",
    "                'num_leaves': 84,\n",
    "                'max_depth': 5,\n",
    "                'subsample_for_bin': 240000,\n",
    "                'reg_alpha': 0.436193,\n",
    "                'reg_lambda': 0.479169,\n",
    "                'colsample_bytree': 0.508716,\n",
    "                'min_split_gain': 0.024766,\n",
    "                'subsample': 0.51,\n",
    "                'random_state': seed,\n",
    "                'verbose': -1,\n",
    "                'min_child_samples': 200,\n",
    "                'max_bin': 20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "854232a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep 20 18:10:41 2022, Cross-Validation, 3911 rows, 100 cols\n",
      "Fold 1, Train score = 0.48199, Valid score = 0.53074\n",
      "Fold 2, Train score = 0.48102, Valid score = 0.53580\n",
      "Fold 3, Train score = 0.48274, Valid score = 0.52561\n",
      "Fold 4, Train score = 0.47804, Valid score = 0.55084\n",
      "Fold 5, Train score = 0.48400, Valid score = 0.52260\n",
      "Train score by each fold: [0.48199, 0.48102, 0.48274, 0.47804, 0.484]\n",
      "Valid score by each fold: [0.53074, 0.5358, 0.52561, 0.55084, 0.5226]\n",
      "Train mean score by each fold:0.48156 +/- 0.00201\n",
      "Valid mean score by each fold:0.53312 +/- 0.00994\n",
      "**************************************************\n",
      "OOF-score: 0.53312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1064"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "lgbm_estimators, oof_scores = lightgbm_cross_validation(lgbm_params, \n",
    "                                                        X, \n",
    "                                                        train.iloc[:, 2:], \n",
    "                                                        cv=fold, \n",
    "                                                        rounds=50)\n",
    "gc.collect()  # take average from estimators or fit full model with more conservative parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7246fe",
   "metadata": {},
   "source": [
    "### Additional model with additional attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4540ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttModel, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc = nn.Linear(768, 6)\n",
    "        self._init_weights(self.fc)\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(768, 6)\n",
    "        )\n",
    "        self._init_weights(self.attention)\n",
    "\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def feature(self, ids, mask):\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, output_hidden_states=False)\n",
    "\n",
    "        sequence_output = outputs[0]      \n",
    "        weights = self.attention(sequence_output)\n",
    "        pooled_output = torch.sum(weights * sequence_output, dim=1)\n",
    "\n",
    "        return pooled_output\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        feature = self.feature(ids, mask)\n",
    "        output = self.regressor(self.dropout(feature))\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test1] *",
   "language": "python",
   "name": "conda-env-test1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
