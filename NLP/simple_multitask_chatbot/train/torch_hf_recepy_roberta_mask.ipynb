{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc0b5e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful\n",
      "Your token has been saved to C:\\Users\\Daria/.huggingface/token\n",
      "\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n",
      "\n",
      "git config --global credential.helper store\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer\n",
    "from transformers import pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re \n",
    "\n",
    "import torch\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d523a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"sberbank-ai/ruRoberta-large\"\n",
    "block_size = 128\n",
    "batch_size = 2\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "317ad46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc4a66a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "106f7511",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"all_recepies_inter.csv\", sep=\"\\t\", usecols=[\"name\", \"Инструкции\"]).rename(columns={\"Инструкции\": \"Instructions\"})\n",
    "\n",
    "df[\"Instructions\"] = df[\"Instructions\"].apply(lambda x: x.replace(u'\\xa0', u' '))\n",
    "df[\"Instructions\"] = df[\"Instructions\"].apply(lambda x: re.sub(\"\\n|\\r|\\t\", \" \",  x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f02dcb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_pandas(df[[\"Instructions\"]]).train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fc56fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"Instructions\"], truncation=True, max_length=512, padding=\"max_length\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d84b0b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22307 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5577 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_ds = ds.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=ds[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eba7435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8495fc24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22307 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5577 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = tokenized_ds.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d070d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2195b9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_steps = len(ds[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-for-chat\",\n",
    "    overwrite_output_dir=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=num_epochs,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=logging_steps,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    # optim=torch.optim.AdamW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38041019",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aad8793f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daria\\anaconda3\\envs\\test1\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 89228\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 16728\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16728' max='16728' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16728/16728 6:02:37, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-500\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-500\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-500\\pytorch_model.bin\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-1000\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-1000\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-1000\\pytorch_model.bin\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-1500\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-1500\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-1500\\pytorch_model.bin\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-2000\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-2000\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-2000\\pytorch_model.bin\n",
      "Deleting older checkpoint [ruRoberta-large-finetuned-for-chat\\checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-2500\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-2500\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-2500\\pytorch_model.bin\n",
      "Deleting older checkpoint [ruRoberta-large-finetuned-for-chat\\checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-3000\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-3000\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-3000\\pytorch_model.bin\n",
      "Deleting older checkpoint [ruRoberta-large-finetuned-for-chat\\checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-3500\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-3500\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-3500\\pytorch_model.bin\n",
      "Deleting older checkpoint [ruRoberta-large-finetuned-for-chat\\checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-4000\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-4000\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-4000\\pytorch_model.bin\n",
      "Deleting older checkpoint [ruRoberta-large-finetuned-for-chat\\checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-4500\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-4500\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-4500\\pytorch_model.bin\n",
      "Deleting older checkpoint [ruRoberta-large-finetuned-for-chat\\checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-5000\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-5000\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-5000\\pytorch_model.bin\n",
      "Deleting older checkpoint [ruRoberta-large-finetuned-for-chat\\checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-5500\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-5500\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-5500\\pytorch_model.bin\n",
      "Deleting older checkpoint [ruRoberta-large-finetuned-for-chat\\checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 22308\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-6000\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-6000\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-6000\\pytorch_model.bin\n",
      "Deleting older checkpoint [ruRoberta-large-finetuned-for-chat\\checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-6500\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-6500\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-6500\\pytorch_model.bin\n",
      "Deleting older checkpoint [ruRoberta-large-finetuned-for-chat\\checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-7000\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-7000\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-7000\\pytorch_model.bin\n",
      "Deleting older checkpoint [ruRoberta-large-finetuned-for-chat\\checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-7500\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-7500\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-7500\\pytorch_model.bin\n",
      "Deleting older checkpoint [ruRoberta-large-finetuned-for-chat\\checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-8000\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-8000\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-8000\\pytorch_model.bin\n",
      "Deleting older checkpoint [ruRoberta-large-finetuned-for-chat\\checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-8500\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-8500\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-8500\\pytorch_model.bin\n",
      "Deleting older checkpoint [ruRoberta-large-finetuned-for-chat\\checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-9000\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-9000\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-9000\\pytorch_model.bin\n",
      "Deleting older checkpoint [ruRoberta-large-finetuned-for-chat\\checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-9500\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-9500\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-9500\\pytorch_model.bin\n",
      "Deleting older checkpoint [ruRoberta-large-finetuned-for-chat\\checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-10000\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-10000\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-10000\\pytorch_model.bin\n",
      "Deleting older checkpoint [ruRoberta-large-finetuned-for-chat\\checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-10500\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-10500\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-10500\\pytorch_model.bin\n",
      "Deleting older checkpoint [ruRoberta-large-finetuned-for-chat\\checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-11000\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-11000\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-11000\\pytorch_model.bin\n",
      "Deleting older checkpoint [ruRoberta-large-finetuned-for-chat\\checkpoint-9500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 22308\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-11500\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-11500\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-11500\\pytorch_model.bin\n",
      "Deleting older checkpoint [ruRoberta-large-finetuned-for-chat\\checkpoint-10000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-12000\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-12000\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-12000\\pytorch_model.bin\n",
      "Deleting older checkpoint [ruRoberta-large-finetuned-for-chat\\checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-12500\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-12500\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-12500\\pytorch_model.bin\n",
      "Deleting older checkpoint [ruRoberta-large-finetuned-for-chat\\checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-13000\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-13000\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-13000\\pytorch_model.bin\n",
      "Deleting older checkpoint [ruRoberta-large-finetuned-for-chat\\checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-13500\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-13500\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-13500\\pytorch_model.bin\n",
      "Deleting older checkpoint [ruRoberta-large-finetuned-for-chat\\checkpoint-12000] due to args.save_total_limit\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-14000\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-14000\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-14000\\pytorch_model.bin\n",
      "Deleting older checkpoint [ruRoberta-large-finetuned-for-chat\\checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-14500\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-14500\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-14500\\pytorch_model.bin\n",
      "Deleting older checkpoint [ruRoberta-large-finetuned-for-chat\\checkpoint-13000] due to args.save_total_limit\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-15000\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-15000\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-15000\\pytorch_model.bin\n",
      "Deleting older checkpoint [ruRoberta-large-finetuned-for-chat\\checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-15500\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-15500\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-15500\\pytorch_model.bin\n",
      "Deleting older checkpoint [ruRoberta-large-finetuned-for-chat\\checkpoint-14000] due to args.save_total_limit\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-16000\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-16000\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-16000\\pytorch_model.bin\n",
      "Deleting older checkpoint [ruRoberta-large-finetuned-for-chat\\checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to ruRoberta-large-finetuned-for-chat\\checkpoint-16500\n",
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\checkpoint-16500\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\checkpoint-16500\\pytorch_model.bin\n",
      "Deleting older checkpoint [ruRoberta-large-finetuned-for-chat\\checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 22308\n",
      "  Batch size = 2\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16728, training_loss=538006.8943089431, metrics={'train_runtime': 21762.8498, 'train_samples_per_second': 12.3, 'train_steps_per_second': 0.769, 'total_flos': 6.237331564144435e+16, 'train_loss': 538006.8943089431, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25138abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ruRoberta-large-finetuned-for-chat\\config.json\n",
      "Model weights saved in ruRoberta-large-finetuned-for-chat\\pytorch_model.bin\n",
      "Uploading the following files to pankratozzi/ruRoberta-large-finetuned-for-chat: config.json,pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.push_to_hub(f\"{model_name}-finetuned-for-chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f69bf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Добавить ложку <mask>, полить сливочным соусом\"\n",
    "text1 = \"Обжарьте <mask> на легком огне\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d485c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(f\"pankratozzi/{model_name}-finetuned-for-chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f57a925",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mask_filler = pipeline(\"fill-mask\", f\"pankratozzi/{model_name}-finetuned-for-chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25e5c1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.50724196434021,\n",
       "  'token': 36734,\n",
       "  'token_str': ' меда',\n",
       "  'sequence': 'Добавить ложку меда, полить сливочным соусом'},\n",
       " {'score': 0.15127043426036835,\n",
       "  'token': 45517,\n",
       "  'token_str': ' сметаны',\n",
       "  'sequence': 'Добавить ложку сметаны, полить сливочным соусом'},\n",
       " {'score': 0.09975725412368774,\n",
       "  'token': 17841,\n",
       "  'token_str': ' сахара',\n",
       "  'sequence': 'Добавить ложку сахара, полить сливочным соусом'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_filler(text, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8368ead2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.10000195354223251,\n",
       "  'token': 651,\n",
       "  'token_str': ' их',\n",
       "  'sequence': 'Обжарьте их на легком огне'},\n",
       " {'score': 0.08920028060674667,\n",
       "  'token': 9816,\n",
       "  'token_str': ' мясо',\n",
       "  'sequence': 'Обжарьте мясо на легком огне'},\n",
       " {'score': 0.06923796236515045,\n",
       "  'token': 24585,\n",
       "  'token_str': ' овощи',\n",
       "  'sequence': 'Обжарьте овощи на легком огне'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_filler(text1, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5eb4eb5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Обжарьте их на легком огне'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_filler(text1, top_k=3)[0][\"sequence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "278f1e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub(f\"pankratozzi/{model_name}-finetuned-for-chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59ac41d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Добавить ложку  меда, полить  меда соусом\n",
      "Добавить ложку  сметаны, полить  сметаны соусом\n",
      "Добавить ложку  сахара, полить  сахара соусом\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(f\"pankratozzi/{model_name}-finetuned-for-chat\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "\n",
    "# model = AutoModelForMaskedLM.from_pretrained(\"pankratozzi/{model_name}-finetuned-for-chat\")\n",
    "logits = model(**inputs).logits\n",
    "mask_token_logits = logits[0, mask_token_index, :]\n",
    "\n",
    "top_3_tokens = torch.topk(mask_token_logits, 3, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_3_tokens:\n",
    "    print(text.replace(tokenizer.mask_token, tokenizer.decode([token])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test1] *",
   "language": "python",
   "name": "conda-env-test1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
