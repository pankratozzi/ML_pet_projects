{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq keras_nlp"
      ],
      "metadata": {
        "id": "cAqyR8eueTOu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=115avJgRM8P-rQf9ER2MnSEhxIvTlHJhS\" -O book.txt"
      ],
      "metadata": {
        "id": "xkFsNxgZeg3n"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6UWWaUgeUGoB"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import re, string\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras_nlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "SEQ_LEN = 16\n",
        "MIN_TRAINING_SEQ_LEN = 3\n",
        "\n",
        "EMBED_DIM = 64\n",
        "FEED_FORWARD_DIM = 256\n",
        "NUM_HEADS = 4\n",
        "NUM_LAYERS = 2\n",
        "VOCAB_SIZE = 10000\n",
        "\n",
        "EPOCHS = 80\n",
        "\n",
        "NUM_TOKENS_TO_GENERATE = 30"
      ],
      "metadata": {
        "id": "BZI52PgMeSQ3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"book.txt\", \"r\", encoding=\"utf-8\") as inbook:\n",
        "    book = inbook.read()\n",
        "    book = re.sub(' +',' ', book)\n",
        "    book = \" \".join([w.lower() for w in book.split()])\n",
        "    book = book.replace(\"\\n\", \"\")"
      ],
      "metadata": {
        "id": "4vXXeuj2ooNY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"lower_book.txt\", \"w\", encoding=\"utf-8\") as inbook:\n",
        "    lbook = book.split()\n",
        "    for i in range(0, len(lbook), SEQ_LEN):\n",
        "        inbook.write(\" \".join(lbook[i:i+SEQ_LEN]+[\"\\n\"]))"
      ],
      "metadata": {
        "id": "nTXx83WpwMQZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = (\n",
        "    tf.data.TextLineDataset(\"lower_book.txt\")\n",
        "    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
        "    .batch(1)\n",
        "  #  .shuffle(buffer_size=10000)\n",
        ")"
      ],
      "metadata": {
        "id": "IVqcV-tmgR3z"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
        "    train_ds,\n",
        "    vocabulary_size=VOCAB_SIZE,\n",
        "    lowercase=True,\n",
        "    reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\"],\n",
        ")"
      ],
      "metadata": {
        "id": "jDIKgMTJteZH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=vocab,\n",
        "    sequence_length=SEQ_LEN,\n",
        "    lowercase=True,\n",
        ")"
      ],
      "metadata": {
        "id": "fJ_Wo-ZfhhqS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_packer = keras_nlp.layers.StartEndPacker(\n",
        "    sequence_length=SEQ_LEN,\n",
        "    start_value=tokenizer.token_to_id(\"[BOS]\"),\n",
        ")"
      ],
      "metadata": {
        "id": "HjOhBvUekPv4"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = (\n",
        "    tf.data.TextLineDataset(\"lower_book.txt\")\n",
        "    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .shuffle(buffer_size=10000)\n",
        ")"
      ],
      "metadata": {
        "id": "m_JOEOnG3NkB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(inputs):\n",
        "    outputs = tokenizer(inputs)\n",
        "    features = start_packer(outputs)\n",
        "    labels = outputs\n",
        "    return features, labels\n",
        "\n",
        "train_ds = train_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n",
        "    tf.data.AUTOTUNE\n",
        ")"
      ],
      "metadata": {
        "id": "HzSGQogt3Nhi"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.layers.Input(shape=(None,), dtype=tf.int32)\n",
        "\n",
        "embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding( \n",
        "    vocabulary_size=VOCAB_SIZE,\n",
        "    sequence_length=SEQ_LEN,\n",
        "    embedding_dim=EMBED_DIM,\n",
        "    mask_zero=True,\n",
        ")\n",
        "x = embedding_layer(inputs)\n",
        "\n",
        "for _ in range(NUM_LAYERS):\n",
        "    decoder_layer = keras_nlp.layers.TransformerDecoder(\n",
        "        num_heads=NUM_HEADS,\n",
        "        intermediate_dim=FEED_FORWARD_DIM,\n",
        "    )\n",
        "    x = decoder_layer(x)  \n",
        "\n",
        "outputs = tf.keras.layers.Dense(VOCAB_SIZE)(x)\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)  \n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[perplexity])"
      ],
      "metadata": {
        "id": "IIIf81PZ3m5F"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_ds, epochs=EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBu890Hk3v1q",
        "outputId": "89b5526b-1c4b-4cb6-dd49-5524e1961cb5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80\n",
            "93/93 [==============================] - 4s 17ms/step - loss: 6.7020 - perplexity: 813.9924\n",
            "Epoch 2/80\n",
            "93/93 [==============================] - 2s 16ms/step - loss: 5.7415 - perplexity: 311.5184\n",
            "Epoch 3/80\n",
            "93/93 [==============================] - 1s 13ms/step - loss: 5.6554 - perplexity: 285.8412\n",
            "Epoch 4/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 5.3807 - perplexity: 217.1843\n",
            "Epoch 5/80\n",
            "93/93 [==============================] - 1s 13ms/step - loss: 5.0945 - perplexity: 163.1240\n",
            "Epoch 6/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 4.8266 - perplexity: 124.7871\n",
            "Epoch 7/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 4.6082 - perplexity: 100.3039\n",
            "Epoch 8/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 4.4117 - perplexity: 82.4094\n",
            "Epoch 9/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 4.2329 - perplexity: 68.9197\n",
            "Epoch 10/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 4.0645 - perplexity: 58.2331\n",
            "Epoch 11/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 3.8949 - perplexity: 49.1489\n",
            "Epoch 12/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 3.7361 - perplexity: 41.9324\n",
            "Epoch 13/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 3.5876 - perplexity: 36.1455\n",
            "Epoch 14/80\n",
            "93/93 [==============================] - 2s 18ms/step - loss: 3.4291 - perplexity: 30.8492\n",
            "Epoch 15/80\n",
            "93/93 [==============================] - 1s 12ms/step - loss: 3.2781 - perplexity: 26.5244\n",
            "Epoch 16/80\n",
            "93/93 [==============================] - 1s 12ms/step - loss: 3.1219 - perplexity: 22.6903\n",
            "Epoch 17/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 2.9858 - perplexity: 19.8021\n",
            "Epoch 18/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 2.8318 - perplexity: 16.9762\n",
            "Epoch 19/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 2.6917 - perplexity: 14.7574\n",
            "Epoch 20/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 2.5574 - perplexity: 12.9020\n",
            "Epoch 21/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 2.4226 - perplexity: 11.2753\n",
            "Epoch 22/80\n",
            "93/93 [==============================] - 1s 13ms/step - loss: 2.2963 - perplexity: 9.9375\n",
            "Epoch 23/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 2.1627 - perplexity: 8.6944\n",
            "Epoch 24/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 2.0615 - perplexity: 7.8578\n",
            "Epoch 25/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 1.9343 - perplexity: 6.9189\n",
            "Epoch 26/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 1.8253 - perplexity: 6.2046\n",
            "Epoch 27/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 1.7049 - perplexity: 5.5008\n",
            "Epoch 28/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 1.6163 - perplexity: 5.0346\n",
            "Epoch 29/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 1.5148 - perplexity: 4.5483\n",
            "Epoch 30/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 1.4374 - perplexity: 4.2097\n",
            "Epoch 31/80\n",
            "93/93 [==============================] - 2s 17ms/step - loss: 1.3638 - perplexity: 3.9112\n",
            "Epoch 32/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 1.2927 - perplexity: 3.6426\n",
            "Epoch 33/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 1.2188 - perplexity: 3.3832\n",
            "Epoch 34/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 1.1534 - perplexity: 3.1690\n",
            "Epoch 35/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 1.0874 - perplexity: 2.9666\n",
            "Epoch 36/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 1.0357 - perplexity: 2.8171\n",
            "Epoch 37/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 1.0013 - perplexity: 2.7219\n",
            "Epoch 38/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.9601 - perplexity: 2.6120\n",
            "Epoch 39/80\n",
            "93/93 [==============================] - 1s 12ms/step - loss: 0.9140 - perplexity: 2.4942\n",
            "Epoch 40/80\n",
            "93/93 [==============================] - 2s 18ms/step - loss: 0.8729 - perplexity: 2.3938\n",
            "Epoch 41/80\n",
            "93/93 [==============================] - 1s 12ms/step - loss: 0.8431 - perplexity: 2.3236\n",
            "Epoch 42/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.8246 - perplexity: 2.2810\n",
            "Epoch 43/80\n",
            "93/93 [==============================] - 1s 12ms/step - loss: 0.8079 - perplexity: 2.2433\n",
            "Epoch 44/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.7750 - perplexity: 2.1707\n",
            "Epoch 45/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.7600 - perplexity: 2.1382\n",
            "Epoch 46/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.7324 - perplexity: 2.0800\n",
            "Epoch 47/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.7169 - perplexity: 2.0481\n",
            "Epoch 48/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.7117 - perplexity: 2.0375\n",
            "Epoch 49/80\n",
            "93/93 [==============================] - 2s 17ms/step - loss: 0.6985 - perplexity: 2.0107\n",
            "Epoch 50/80\n",
            "93/93 [==============================] - 1s 13ms/step - loss: 0.6909 - perplexity: 1.9956\n",
            "Epoch 51/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.6780 - perplexity: 1.9700\n",
            "Epoch 52/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.6687 - perplexity: 1.9517\n",
            "Epoch 53/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.6783 - perplexity: 1.9706\n",
            "Epoch 54/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.6838 - perplexity: 1.9814\n",
            "Epoch 55/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.6911 - perplexity: 1.9958\n",
            "Epoch 56/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.7123 - perplexity: 2.0387\n",
            "Epoch 57/80\n",
            "93/93 [==============================] - 1s 14ms/step - loss: 0.7194 - perplexity: 2.0532\n",
            "Epoch 58/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.7260 - perplexity: 2.0668\n",
            "Epoch 59/80\n",
            "93/93 [==============================] - 1s 12ms/step - loss: 0.7191 - perplexity: 2.0526\n",
            "Epoch 60/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.6843 - perplexity: 1.9823\n",
            "Epoch 61/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.6520 - perplexity: 1.9193\n",
            "Epoch 62/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.6346 - perplexity: 1.8864\n",
            "Epoch 63/80\n",
            "93/93 [==============================] - 1s 12ms/step - loss: 0.6152 - perplexity: 1.8500\n",
            "Epoch 64/80\n",
            "93/93 [==============================] - 1s 12ms/step - loss: 0.6052 - perplexity: 1.8317\n",
            "Epoch 65/80\n",
            "93/93 [==============================] - 1s 13ms/step - loss: 0.5999 - perplexity: 1.8219\n",
            "Epoch 66/80\n",
            "93/93 [==============================] - 1s 13ms/step - loss: 0.5924 - perplexity: 1.8083\n",
            "Epoch 67/80\n",
            "93/93 [==============================] - 1s 12ms/step - loss: 0.5927 - perplexity: 1.8089\n",
            "Epoch 68/80\n",
            "93/93 [==============================] - 1s 12ms/step - loss: 0.5873 - perplexity: 1.7990\n",
            "Epoch 69/80\n",
            "93/93 [==============================] - 1s 12ms/step - loss: 0.5831 - perplexity: 1.7916\n",
            "Epoch 70/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.5833 - perplexity: 1.7919\n",
            "Epoch 71/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.5906 - perplexity: 1.8051\n",
            "Epoch 72/80\n",
            "93/93 [==============================] - 2s 17ms/step - loss: 0.5928 - perplexity: 1.8090\n",
            "Epoch 73/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.6246 - perplexity: 1.8674\n",
            "Epoch 74/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.7688 - perplexity: 2.1573\n",
            "Epoch 75/80\n",
            "93/93 [==============================] - 1s 12ms/step - loss: 1.0256 - perplexity: 2.7887\n",
            "Epoch 76/80\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.9729 - perplexity: 2.6456\n",
            "Epoch 77/80\n",
            "93/93 [==============================] - 1s 12ms/step - loss: 0.7778 - perplexity: 2.1767\n",
            "Epoch 78/80\n",
            "93/93 [==============================] - 1s 12ms/step - loss: 0.6465 - perplexity: 1.9089\n",
            "Epoch 79/80\n",
            "93/93 [==============================] - 1s 15ms/step - loss: 0.5923 - perplexity: 1.8082\n",
            "Epoch 80/80\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.5796 - perplexity: 1.7854\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4c42b17ca0>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_tokens = tf.convert_to_tensor([tokenizer.token_to_id(\"[BOS]\")])\n",
        "\n",
        "def token_logits_fn(inputs):\n",
        "    cur_len = inputs.shape[1]\n",
        "    output = model(inputs)\n",
        "    return output[:, cur_len - 1, :] "
      ],
      "metadata": {
        "id": "lFVWdNU24Faq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_tokens = keras_nlp.utils.top_p_search(\n",
        "    token_logits_fn,\n",
        "    prompt_tokens,\n",
        "    max_length=15,\n",
        "    p=0.7,\n",
        "    from_logits=True,\n",
        ")\n",
        "txt = tokenizer.detokenize(output_tokens)\n",
        "txt = re.sub(\"(?:[^|]*\\|)?([^\\]|]*)\\]\", '', txt.numpy().decode(\"utf-8\"))\n",
        "txt = re.sub(' +', ' ', txt).strip()\n",
        "print(f\"Top-P search generated text: \\n{txt}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ay29gJi644wI",
        "outputId": "7d9805f8-f7db-4d03-e1e8-f9435eb5f19b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-P search generated text: \n",
            "на язык , от коих я теперь отвык\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_tokens = keras_nlp.utils.top_p_search(\n",
        "    token_logits_fn,\n",
        "    prompt_tokens,\n",
        "    max_length=15,\n",
        "    p=0.7,\n",
        "    from_logits=True,\n",
        ")\n",
        "txt = tokenizer.detokenize(output_tokens)\n",
        "txt = re.sub(\"(?:[^|]*\\|)?([^\\]|]*)\\]\", '', txt.numpy().decode(\"utf-8\"))\n",
        "txt = re.sub(' +', ' ', txt).strip()\n",
        "print(f\"Top-P search generated text: \\n{txt}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rKZkVxka77y",
        "outputId": "f897194a-656b-45f5-ef35-4018d8febc4f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-P search generated text: \n",
            "разлуку , татьяна ропщет на ручей\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Casual Vocabulary"
      ],
      "metadata": {
        "id": "qCnRLSNabJMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = (\n",
        "    tf.data.TextLineDataset(\"lower_book.txt\")\n",
        "    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
        "    .batch(1)\n",
        ")"
      ],
      "metadata": {
        "id": "Sss-UMXAcA3y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 10000\n",
        "sequence_length = 25\n",
        "\n",
        "vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "    standardize=\"lower\",\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n",
        "\n",
        "def vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    text = vectorize_layer(text)\n",
        "    return text, label\n",
        "\n",
        "vectorize_layer.adapt(train_ds)"
      ],
      "metadata": {
        "id": "9H7Y6wMnbL-w"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "train_ds = (\n",
        "    tf.data.TextLineDataset(\"lower_book.txt\")\n",
        "    .batch(BATCH_SIZE)\n",
        "    .shuffle(buffer_size=10000)\n",
        ")\n",
        "\n",
        "def preprocess(inputs):\n",
        "    outputs = vectorize_layer(inputs)\n",
        "    input_text = outputs[:, :-1]\n",
        "    target_text = outputs[:, 1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "train_ds = train_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n",
        "    tf.data.AUTOTUNE\n",
        ")"
      ],
      "metadata": {
        "id": "ydsEGd-_cIhh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.layers.Input(shape=(None,), dtype=tf.int32)\n",
        "\n",
        "embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding( \n",
        "    vocabulary_size=max_features,\n",
        "    sequence_length=sequence_length,\n",
        "    embedding_dim=EMBED_DIM,\n",
        "    mask_zero=True,\n",
        ")\n",
        "x = embedding_layer(inputs)\n",
        "\n",
        "for _ in range(NUM_LAYERS):\n",
        "    decoder_layer = keras_nlp.layers.TransformerDecoder(\n",
        "        num_heads=NUM_HEADS,\n",
        "        intermediate_dim=FEED_FORWARD_DIM,\n",
        "    )\n",
        "    x = decoder_layer(x)  \n",
        "\n",
        "outputs = tf.keras.layers.Dense(max_features)(x)\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)  \n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[perplexity])\n",
        "callbacks = [\n",
        "             tf.keras.callbacks.ReduceLROnPlateau(patience=3, monitor=\"loss\"),\n",
        "             tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, restore_best_weights=True),\n",
        "            ]"
      ],
      "metadata": {
        "id": "o-Wc-C8TcUXI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_ds, epochs=EPOCHS, callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEO3PGEucUS1",
        "outputId": "a651e63f-0305-40b3-f56e-0a195321bac7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80\n",
            "47/47 [==============================] - 6s 16ms/step - loss: 5.7733 - perplexity: 6535.5029 - lr: 0.0010\n",
            "Epoch 2/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 5.0841 - perplexity: 2804.1404 - lr: 0.0010\n",
            "Epoch 3/80\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 4.8585 - perplexity: 2292.3171 - lr: 0.0010\n",
            "Epoch 4/80\n",
            "47/47 [==============================] - 1s 19ms/step - loss: 4.6096 - perplexity: 1583.2578 - lr: 0.0010\n",
            "Epoch 5/80\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 4.3614 - perplexity: 1069.1787 - lr: 0.0010\n",
            "Epoch 6/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 4.1060 - perplexity: 711.6854 - lr: 0.0010\n",
            "Epoch 7/80\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 3.8204 - perplexity: 450.9723 - lr: 0.0010\n",
            "Epoch 8/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 3.4915 - perplexity: 266.5721 - lr: 0.0010\n",
            "Epoch 9/80\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 3.1837 - perplexity: 162.9529 - lr: 0.0010\n",
            "Epoch 10/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 2.8601 - perplexity: 97.1129 - lr: 0.0010\n",
            "Epoch 11/80\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 2.5384 - perplexity: 58.0508 - lr: 0.0010\n",
            "Epoch 12/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 2.2409 - perplexity: 36.0646 - lr: 0.0010\n",
            "Epoch 13/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 1.9640 - perplexity: 23.1596 - lr: 0.0010\n",
            "Epoch 14/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 1.7066 - perplexity: 15.3409 - lr: 0.0010\n",
            "Epoch 15/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 1.4344 - perplexity: 9.9239 - lr: 0.0010\n",
            "Epoch 16/80\n",
            "47/47 [==============================] - 1s 19ms/step - loss: 1.1936 - perplexity: 6.7507 - lr: 0.0010\n",
            "Epoch 17/80\n",
            "47/47 [==============================] - 1s 21ms/step - loss: 0.9747 - perplexity: 4.7565 - lr: 0.0010\n",
            "Epoch 18/80\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 0.7796 - perplexity: 3.4810 - lr: 0.0010\n",
            "Epoch 19/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 0.6177 - perplexity: 2.6867 - lr: 0.0010\n",
            "Epoch 20/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 0.4816 - perplexity: 2.1609 - lr: 0.0010\n",
            "Epoch 21/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 0.3816 - perplexity: 1.8415 - lr: 0.0010\n",
            "Epoch 22/80\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 0.3040 - perplexity: 1.6263 - lr: 0.0010\n",
            "Epoch 23/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 0.2454 - perplexity: 1.4809 - lr: 0.0010\n",
            "Epoch 24/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 0.2017 - perplexity: 1.3808 - lr: 0.0010\n",
            "Epoch 25/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 0.1705 - perplexity: 1.3135 - lr: 0.0010\n",
            "Epoch 26/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 0.1481 - perplexity: 1.2674 - lr: 0.0010\n",
            "Epoch 27/80\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 0.1296 - perplexity: 1.2305 - lr: 0.0010\n",
            "Epoch 28/80\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.1168 - perplexity: 1.2054 - lr: 0.0010\n",
            "Epoch 29/80\n",
            "47/47 [==============================] - 1s 19ms/step - loss: 0.1069 - perplexity: 1.1865 - lr: 0.0010\n",
            "Epoch 30/80\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.1015 - perplexity: 1.1762 - lr: 0.0010\n",
            "Epoch 31/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 0.0931 - perplexity: 1.1605 - lr: 0.0010\n",
            "Epoch 32/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 0.0888 - perplexity: 1.1526 - lr: 0.0010\n",
            "Epoch 33/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 0.0842 - perplexity: 1.1442 - lr: 0.0010\n",
            "Epoch 34/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 0.0819 - perplexity: 1.1400 - lr: 0.0010\n",
            "Epoch 35/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 0.0786 - perplexity: 1.1340 - lr: 0.0010\n",
            "Epoch 36/80\n",
            "47/47 [==============================] - 1s 18ms/step - loss: 0.0766 - perplexity: 1.1305 - lr: 0.0010\n",
            "Epoch 37/80\n",
            "47/47 [==============================] - 1s 20ms/step - loss: 0.0751 - perplexity: 1.1276 - lr: 0.0010\n",
            "Epoch 38/80\n",
            "47/47 [==============================] - 1s 21ms/step - loss: 0.0724 - perplexity: 1.1229 - lr: 0.0010\n",
            "Epoch 39/80\n",
            "47/47 [==============================] - 2s 35ms/step - loss: 0.0711 - perplexity: 1.1205 - lr: 0.0010\n",
            "Epoch 40/80\n",
            "47/47 [==============================] - 1s 28ms/step - loss: 0.0708 - perplexity: 1.1199 - lr: 0.0010\n",
            "Epoch 41/80\n",
            "47/47 [==============================] - 1s 30ms/step - loss: 0.0684 - perplexity: 1.1156 - lr: 0.0010\n",
            "Epoch 42/80\n",
            "47/47 [==============================] - 1s 27ms/step - loss: 0.0661 - perplexity: 1.1115 - lr: 0.0010\n",
            "Epoch 43/80\n",
            "47/47 [==============================] - 1s 26ms/step - loss: 0.0659 - perplexity: 1.1112 - lr: 0.0010\n",
            "Epoch 44/80\n",
            "47/47 [==============================] - 1s 28ms/step - loss: 0.0650 - perplexity: 1.1095 - lr: 0.0010\n",
            "Epoch 45/80\n",
            "47/47 [==============================] - 1s 28ms/step - loss: 0.0651 - perplexity: 1.1097 - lr: 0.0010\n",
            "Epoch 46/80\n",
            "47/47 [==============================] - 1s 29ms/step - loss: 0.0637 - perplexity: 1.1073 - lr: 0.0010\n",
            "Epoch 47/80\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 0.0624 - perplexity: 1.1050 - lr: 0.0010\n",
            "Epoch 48/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 0.0625 - perplexity: 1.1051 - lr: 0.0010\n",
            "Epoch 49/80\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 0.0608 - perplexity: 1.1021 - lr: 0.0010\n",
            "Epoch 50/80\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 0.0609 - perplexity: 1.1024 - lr: 0.0010\n",
            "Epoch 51/80\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 0.0599 - perplexity: 1.1006 - lr: 0.0010\n",
            "Epoch 52/80\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 0.0588 - perplexity: 1.0986 - lr: 0.0010\n",
            "Epoch 53/80\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 0.0596 - perplexity: 1.1001 - lr: 0.0010\n",
            "Epoch 54/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 0.0585 - perplexity: 1.0981 - lr: 0.0010\n",
            "Epoch 55/80\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 0.0582 - perplexity: 1.0975 - lr: 0.0010\n",
            "Epoch 56/80\n",
            "47/47 [==============================] - 1s 19ms/step - loss: 0.0582 - perplexity: 1.0976 - lr: 0.0010\n",
            "Epoch 57/80\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 0.0572 - perplexity: 1.0958 - lr: 0.0010\n",
            "Epoch 58/80\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 0.0574 - perplexity: 1.0963 - lr: 0.0010\n",
            "Epoch 59/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 0.0566 - perplexity: 1.0948 - lr: 0.0010\n",
            "Epoch 60/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 0.0568 - perplexity: 1.0950 - lr: 0.0010\n",
            "Epoch 61/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 0.0561 - perplexity: 1.0939 - lr: 0.0010\n",
            "Epoch 62/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 0.0559 - perplexity: 1.0935 - lr: 0.0010\n",
            "Epoch 63/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 0.0554 - perplexity: 1.0927 - lr: 0.0010\n",
            "Epoch 64/80\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 0.0560 - perplexity: 1.0937 - lr: 0.0010\n",
            "Epoch 65/80\n",
            "47/47 [==============================] - 1s 19ms/step - loss: 0.0561 - perplexity: 1.0938 - lr: 0.0010\n",
            "Epoch 66/80\n",
            "47/47 [==============================] - 1s 20ms/step - loss: 0.0547 - perplexity: 1.0915 - lr: 0.0010\n",
            "Epoch 67/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 0.0546 - perplexity: 1.0914 - lr: 0.0010\n",
            "Epoch 68/80\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.0550 - perplexity: 1.0919 - lr: 0.0010\n",
            "Epoch 69/80\n",
            "47/47 [==============================] - 1s 20ms/step - loss: 0.0554 - perplexity: 1.0927 - lr: 0.0010\n",
            "Epoch 70/80\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.0474 - perplexity: 1.0787 - lr: 1.0000e-04\n",
            "Epoch 71/80\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 0.0457 - perplexity: 1.0759 - lr: 1.0000e-04\n",
            "Epoch 72/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 0.0452 - perplexity: 1.0750 - lr: 1.0000e-04\n",
            "Epoch 73/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 0.0450 - perplexity: 1.0747 - lr: 1.0000e-04\n",
            "Epoch 74/80\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 0.0448 - perplexity: 1.0743 - lr: 1.0000e-04\n",
            "Epoch 75/80\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 0.0447 - perplexity: 1.0741 - lr: 1.0000e-04\n",
            "Epoch 76/80\n",
            "47/47 [==============================] - 1s 24ms/step - loss: 0.0446 - perplexity: 1.0739 - lr: 1.0000e-04\n",
            "Epoch 77/80\n",
            "47/47 [==============================] - 1s 24ms/step - loss: 0.0446 - perplexity: 1.0739 - lr: 1.0000e-04\n",
            "Epoch 78/80\n",
            "47/47 [==============================] - 1s 18ms/step - loss: 0.0445 - perplexity: 1.0737 - lr: 1.0000e-04\n",
            "Epoch 79/80\n",
            "47/47 [==============================] - 1s 18ms/step - loss: 0.0444 - perplexity: 1.0736 - lr: 1.0000e-04\n",
            "Epoch 80/80\n",
            "47/47 [==============================] - 1s 25ms/step - loss: 0.0445 - perplexity: 1.0737 - lr: 1.0000e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ffa2c0a1040>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_tokens = tf.expand_dims(vectorize_layer(\"Иду \")[:1], axis=0)\n",
        "\n",
        "def token_logits_fn(inputs):\n",
        "    output = model(inputs)\n",
        "    return output[:, -1, :] "
      ],
      "metadata": {
        "id": "-P2NqUNHcUQq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "vocabulary = vectorize_layer.get_vocabulary()\n",
        "vocab_arr = np.asarray(vocabulary) "
      ],
      "metadata": {
        "id": "ki-DJG1TiqvC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_tokens = keras_nlp.utils.top_p_search(\n",
        "    token_logits_fn,\n",
        "    prompt_tokens,\n",
        "    max_length=15,\n",
        "    p=0.7,\n",
        "    from_logits=True,\n",
        ")\n",
        "\n",
        "\" \".join(vocab_arr[output_tokens])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ss9jKFY-cUOg",
        "outputId": "560b21e6-cd00-4da4-cbe1-4b0798ba64ba"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[UNK] смотрит он и отвечает: [UNK] x татьяна, по совету няни сбираясь ночью ворожить, тихонько'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_tokens = keras_nlp.utils.top_p_search(\n",
        "    token_logits_fn,\n",
        "    prompt_tokens,\n",
        "    max_length=20,\n",
        "    p=0.8,\n",
        "    from_logits=True,\n",
        ")\n",
        "\n",
        "\" \".join(vocab_arr[output_tokens])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "xu3kJPR-dDpx",
        "outputId": "10279cb7-a23c-4c28-d69f-d97f523e6bdd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[UNK] вихорь шумный; чета мелькает за четой. к минуте мщенья приближаясь, онегин, втайне усмехаясь, подходит к   с [UNK]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_tokens = keras_nlp.utils.top_p_search(\n",
        "    token_logits_fn,\n",
        "    prompt_tokens,\n",
        "    max_length=24,\n",
        "    p=0.4,\n",
        "    from_logits=True,\n",
        ")\n",
        "\n",
        "\" \".join(vocab_arr[output_tokens])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-Rcn0qOFjgTy",
        "outputId": "a4a3b461-c529-40d3-b5d5-010e4f5a273e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[UNK] мужички-то все [UNK] гребут лопатой серебро; кому поем, тому добро и слава!\" но сулит утраты  как ты, [UNK] вот что  и'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRU with char-level"
      ],
      "metadata": {
        "id": "eI-3qXXi7hHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "pUIdPzhQMtKT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"book.txt\", \"r\", encoding=\"utf-8\") as inbook:\n",
        "    text = inbook.read()\n",
        "\n",
        "text = text + text\n",
        "vocab = sorted(set(text))\n",
        "\n",
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ocodfi_r4FYC",
        "outputId": "7b55dbe4-ab98-4846-d981-a834dfe2aef4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "metadata": {
        "id": "BY8oJgeMMfdd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "scboq4XGNe9X"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\\\n",
        "                              .batch(seq_length+1, drop_remainder=True)\\\n",
        "                              .map(split_input_target)\\\n",
        "                              .shuffle(BUFFER_SIZE)\\\n",
        "                              .batch(BATCH_SIZE, drop_remainder=True)\\\n",
        "                              .prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "l3rROCsbMv4a"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 128\n",
        "hidden = 1024"
      ],
      "metadata": {
        "id": "_UrUogq48r9B"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, hidden):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim),                    \n",
        "        tf.keras.layers.GRU(hidden*2,\n",
        "                            return_sequences=True,\n",
        "                            stateful=False,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.GRU(hidden,\n",
        "                            return_sequences=True,\n",
        "                            stateful=False,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "         tf.keras.layers.GRU(hidden,\n",
        "                             return_sequences=True,\n",
        "                             stateful=False,\n",
        "                             recurrent_initializer='glorot_uniform'),                                  \n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "cR0_a_azNuWi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(vocab_size=len(vocab), embedding_dim=embedding_dim, hidden=hidden)\n",
        "\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "callbacks = [\n",
        "             tf.keras.callbacks.ReduceLROnPlateau(patience=2, monitor=\"loss\"),\n",
        "             tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, restore_best_weights=True),\n",
        "            ]\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=loss, metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "9SOtU-d3NuUZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(char_dataset, epochs=50, callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecRxTMdYNtSE",
        "outputId": "3275db2c-fcf8-43b8-856d-e428db938e9e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "88/88 [==============================] - 43s 342ms/step - loss: 2.0941 - accuracy: 0.5575 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "88/88 [==============================] - 33s 368ms/step - loss: 1.4414 - accuracy: 0.6067 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "88/88 [==============================] - 35s 383ms/step - loss: 1.2863 - accuracy: 0.6346 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "88/88 [==============================] - 34s 372ms/step - loss: 1.1939 - accuracy: 0.6565 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "88/88 [==============================] - 34s 375ms/step - loss: 1.1046 - accuracy: 0.6794 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "88/88 [==============================] - 35s 378ms/step - loss: 1.0180 - accuracy: 0.7033 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "88/88 [==============================] - 35s 375ms/step - loss: 0.9237 - accuracy: 0.7295 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "88/88 [==============================] - 33s 371ms/step - loss: 0.8404 - accuracy: 0.7552 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "88/88 [==============================] - 34s 380ms/step - loss: 0.7358 - accuracy: 0.7850 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "88/88 [==============================] - 34s 372ms/step - loss: 0.6376 - accuracy: 0.8159 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "88/88 [==============================] - 34s 371ms/step - loss: 0.5214 - accuracy: 0.8533 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "88/88 [==============================] - 34s 375ms/step - loss: 0.4608 - accuracy: 0.8797 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "88/88 [==============================] - 34s 377ms/step - loss: 0.4027 - accuracy: 0.9001 - lr: 0.0010\n",
            "Epoch 14/50\n",
            "88/88 [==============================] - 34s 376ms/step - loss: 0.3212 - accuracy: 0.9184 - lr: 0.0010\n",
            "Epoch 15/50\n",
            "88/88 [==============================] - 34s 378ms/step - loss: 0.2651 - accuracy: 0.9314 - lr: 0.0010\n",
            "Epoch 16/50\n",
            "88/88 [==============================] - 34s 374ms/step - loss: 0.2378 - accuracy: 0.9380 - lr: 0.0010\n",
            "Epoch 17/50\n",
            "88/88 [==============================] - 34s 372ms/step - loss: 0.2159 - accuracy: 0.9435 - lr: 0.0010\n",
            "Epoch 18/50\n",
            "88/88 [==============================] - 34s 379ms/step - loss: 0.1987 - accuracy: 0.9481 - lr: 0.0010\n",
            "Epoch 19/50\n",
            "88/88 [==============================] - 35s 374ms/step - loss: 0.1843 - accuracy: 0.9522 - lr: 0.0010\n",
            "Epoch 20/50\n",
            "88/88 [==============================] - 34s 375ms/step - loss: 0.1695 - accuracy: 0.9566 - lr: 0.0010\n",
            "Epoch 21/50\n",
            "88/88 [==============================] - 33s 369ms/step - loss: 0.2610 - accuracy: 0.9343 - lr: 0.0010\n",
            "Epoch 22/50\n",
            "88/88 [==============================] - 34s 371ms/step - loss: 0.1941 - accuracy: 0.9488 - lr: 0.0010\n",
            "Epoch 23/50\n",
            "88/88 [==============================] - 34s 374ms/step - loss: 0.1370 - accuracy: 0.9664 - lr: 1.0000e-04\n",
            "Epoch 24/50\n",
            "88/88 [==============================] - 34s 373ms/step - loss: 0.1235 - accuracy: 0.9702 - lr: 1.0000e-04\n",
            "Epoch 25/50\n",
            "88/88 [==============================] - 34s 372ms/step - loss: 0.1190 - accuracy: 0.9714 - lr: 1.0000e-04\n",
            "Epoch 26/50\n",
            "88/88 [==============================] - 35s 379ms/step - loss: 0.1163 - accuracy: 0.9719 - lr: 1.0000e-04\n",
            "Epoch 27/50\n",
            "88/88 [==============================] - 34s 372ms/step - loss: 0.1143 - accuracy: 0.9722 - lr: 1.0000e-04\n",
            "Epoch 28/50\n",
            "88/88 [==============================] - 34s 377ms/step - loss: 0.1125 - accuracy: 0.9726 - lr: 1.0000e-04\n",
            "Epoch 29/50\n",
            "88/88 [==============================] - 34s 375ms/step - loss: 0.1110 - accuracy: 0.9729 - lr: 1.0000e-04\n",
            "Epoch 30/50\n",
            "88/88 [==============================] - 33s 370ms/step - loss: 0.1094 - accuracy: 0.9731 - lr: 1.0000e-04\n",
            "Epoch 31/50\n",
            "88/88 [==============================] - 34s 372ms/step - loss: 0.1083 - accuracy: 0.9735 - lr: 1.0000e-04\n",
            "Epoch 32/50\n",
            "88/88 [==============================] - 34s 373ms/step - loss: 0.1070 - accuracy: 0.9736 - lr: 1.0000e-04\n",
            "Epoch 33/50\n",
            "88/88 [==============================] - 34s 374ms/step - loss: 0.1061 - accuracy: 0.9738 - lr: 1.0000e-04\n",
            "Epoch 34/50\n",
            "88/88 [==============================] - 34s 371ms/step - loss: 0.1050 - accuracy: 0.9740 - lr: 1.0000e-04\n",
            "Epoch 35/50\n",
            "88/88 [==============================] - 34s 377ms/step - loss: 0.1040 - accuracy: 0.9741 - lr: 1.0000e-04\n",
            "Epoch 36/50\n",
            "88/88 [==============================] - 34s 376ms/step - loss: 0.1029 - accuracy: 0.9745 - lr: 1.0000e-04\n",
            "Epoch 37/50\n",
            "88/88 [==============================] - 34s 375ms/step - loss: 0.1020 - accuracy: 0.9746 - lr: 1.0000e-04\n",
            "Epoch 38/50\n",
            "88/88 [==============================] - 35s 376ms/step - loss: 0.1013 - accuracy: 0.9746 - lr: 1.0000e-04\n",
            "Epoch 39/50\n",
            "88/88 [==============================] - 34s 373ms/step - loss: 0.1004 - accuracy: 0.9748 - lr: 1.0000e-04\n",
            "Epoch 40/50\n",
            "88/88 [==============================] - 34s 371ms/step - loss: 0.0997 - accuracy: 0.9750 - lr: 1.0000e-04\n",
            "Epoch 41/50\n",
            "88/88 [==============================] - 35s 380ms/step - loss: 0.0990 - accuracy: 0.9750 - lr: 1.0000e-04\n",
            "Epoch 42/50\n",
            "88/88 [==============================] - 34s 371ms/step - loss: 0.0981 - accuracy: 0.9752 - lr: 1.0000e-04\n",
            "Epoch 43/50\n",
            "88/88 [==============================] - 34s 376ms/step - loss: 0.0975 - accuracy: 0.9754 - lr: 1.0000e-04\n",
            "Epoch 44/50\n",
            "88/88 [==============================] - 34s 375ms/step - loss: 0.0969 - accuracy: 0.9754 - lr: 1.0000e-04\n",
            "Epoch 45/50\n",
            "88/88 [==============================] - 34s 376ms/step - loss: 0.0961 - accuracy: 0.9756 - lr: 1.0000e-04\n",
            "Epoch 46/50\n",
            "88/88 [==============================] - 34s 374ms/step - loss: 0.0954 - accuracy: 0.9757 - lr: 1.0000e-04\n",
            "Epoch 47/50\n",
            "88/88 [==============================] - 33s 370ms/step - loss: 0.0948 - accuracy: 0.9758 - lr: 1.0000e-04\n",
            "Epoch 48/50\n",
            "88/88 [==============================] - 34s 375ms/step - loss: 0.0941 - accuracy: 0.9759 - lr: 1.0000e-04\n",
            "Epoch 49/50\n",
            "88/88 [==============================] - 34s 370ms/step - loss: 0.0938 - accuracy: 0.9759 - lr: 1.0000e-04\n",
            "Epoch 50/50\n",
            "88/88 [==============================] - 34s 373ms/step - loss: 0.0933 - accuracy: 0.9762 - lr: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string, temperature=0.5, num_generate=500):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    # Number of characters to generate\n",
        "    # num_generate = 500\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperature results in more predictable text.\n",
        "    # Higher temperature results in more surprising text.\n",
        "    # Experiment to find the best setting.\n",
        "    # temperature = 0.5\n",
        "\n",
        "    # Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        # Pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "IKzD2oW8NtPV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=u\"И вот идет уже \", temperature=0.001))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQyxBi0PScUQ",
        "outputId": "f9c37842-201b-4ff1-83e0-1086fa26b80a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "И вот идет уже сто                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n"
          ]
        }
      ]
    }
  ]
}